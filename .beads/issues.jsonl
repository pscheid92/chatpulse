{"id":"twitch-tow-021","title":"Positive: Excellent security documentation in handlers_overlay.go","description":"The handlers_overlay.go file contains exemplary security documentation:\n\n**What makes it exceptional:**\n\n1. **38-line security comment** explaining CORS policy decision:\n   - States the security posture explicitly (CheckOrigin returns true)\n   - Explains WHY it's intentionally permissive (OBS browser sources)\n   - Lists 4 reasons supporting the decision\n   - Documents security properties (UUID randomness, rotation capability)\n   - Analyzes threat model (leakage, embedding, CSWSH)\n   - Shows awareness of trade-offs\n\n2. **Proactive threat modeling**:\n   - Overlay URL leakage → UUID rotation\n   - Third-party embedding → Acceptable by design\n   - Cross-site WebSocket hijacking → Not applicable (read-only)\n   - Analytics/tracking → Documented trade-off\n\n3. **Justification for security decisions**:\n   - Not just 'what' (CheckOrigin: true) but 'why' (OBS compatibility)\n   - Links technical choice to product requirements\n\n4. **Maintenance value**:\n   - Future developers won't accidentally 'fix' the CORS policy\n   - Security auditors can verify intent vs implementation\n   - Documents acceptable risk surface\n\n**This is a gold standard example** of security documentation. Other security-sensitive code could benefit from similar treatment.\n\n**Areas that could use similar documentation:**\n- Token encryption (why AES-GCM vs other ciphers)\n- Session secret usage (why 7-day max age)\n- CSRF protection strategy (why strict SameSite for some routes)\n- Webhook HMAC verification (threat model)\n\n**Recommendation**: Use handlers_overlay.go as a template for documenting other security decisions.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:57.093596+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:02.819837+01:00","closed_at":"2026-02-12T17:57:02.819837+01:00","close_reason":"Positive architectural notes - no action required"}
{"id":"twitch-tow-05i","title":"Summary: Maintainability strengths of this codebase","description":"After thorough review this codebase demonstrates strong maintainability practices:\n\nEXCELLENT:\n- Clean layered architecture with clear dependency flow\n- Domain-driven design with consumer-side interfaces\n- Comprehensive CLAUDE.md documentation (rare quality)\n- Security decisions documented with threat model (handlers_overlay.go)\n- SQL injection protection via sqlc\n- Well-configured linter (golangci-lint v2)\n- Consistent error handling with sentinel errors\n- Testcontainers for realistic integration tests\n- Actor pattern for concurrency (broadcaster)\n- Explicit dependency injection in main.go\n\nGOOD:\n- 141 tests across 8 packages\n- Package organization by responsibility\n- Interface granularity (though debatable if too fine)\n- Redis Lua functions for atomic operations\n- Structured logging with slog\n- CSRF protection on state-changing routes\n\nNEEDS IMPROVEMENT:\n- Missing package-level documentation (godoc)\n- No architecture decision records (ADRs)\n- Constants scattered across packages\n- Test file organization inconsistent\n- No code coverage requirements\n- Error handling patterns vary\n- Context propagation inconsistent\n- Template caching undocumented\n\nMINOR ISSUES:\n- Domain package split into 9 small files\n- Repository naming inconsistency (Repo vs Repository)\n- Manual mocks (consider mockery)\n- No dependency visualization\n\nCODEBASE METRICS:\n- 62 Go files (40 production, 22 test)\n- ~3400 lines production code\n- 11 packages\n- Zero circular dependencies\n- 9 domain interfaces\n\nOVERALL ASSESSMENT:\nThis is a well-architected, maintainable codebase that follows Go idioms and clean architecture principles. The documentation quality (especially CLAUDE.md) is exceptional. Most concerns are minor polish items not fundamental issues. Code is ready for production deployment and team scaling.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:09:59.106785+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:07.924171+01:00","closed_at":"2026-02-12T17:57:07.924171+01:00","close_reason":"Summary note - no action required"}
{"id":"twitch-tow-08s","title":"Discussion: No global WebSocket connection limits","description":"WebSocket connections have no per-session or per-instance limits configured. This could lead to resource exhaustion.\n\nCurrent limits:\n✓ maxClientsPerSession = 50 (prevents single session monopoly)\n✗ No limit on total connections per instance\n✗ No limit on connections per IP\n✗ No limit on connection rate\n\nResource concerns per connection:\n- 2 goroutines (clientWriter + read pump in handler)\n- 16-message send buffer per client\n- WebSocket upgrade buffers (1KB read + 1KB write)\n- TCP connection overhead\n\nAt 10K concurrent connections:\n- 20K goroutines\n- ~20 MB buffer memory\n- OS file descriptor limit (ulimit)\n\nAttack vectors:\n\n1. Connection exhaustion\n- Attacker opens thousands of WS connections\n- Bypasses maxClientsPerSession by targeting different sessions\n- Exhausts file descriptors, goroutines, memory\n\n2. Slowloris-style attack\n- Opens connections but never reads\n- clientWriter send buffer fills (cap 16)\n- Broadcaster drops slow client but connection remains\n\n3. Session enumeration\n- Attacker tries random UUIDs\n- 404 vs successful upgrade leaks valid sessions\n- No rate limiting on invalid attempts\n\nCurrent protections:\n✓ Slow client detection (non-blocking send, 16-message buffer)\n✓ Ping/pong heartbeat (30s/60s timeouts)\n✓ UUID is 128-bit random (not guessable)\n✗ No connection rate limiting\n✗ No total connection cap\n\nPotential solutions:\n\nA. Global connection limit per instance\n- Set max_websocket_connections (e.g., 10K)\n- Reject new connections when limit reached\n- Simple counter in Broadcaster\n\nB. Connection rate limiting\n- Track connections per IP using middleware\n- Allow burst, limit sustained rate\n- Requires reverse proxy for real IP\n\nC. Per-IP connection limit\n- Limit concurrent connections per IP (e.g., 100)\n- Prevents single-source exhaustion\n- Echo middleware with in-memory map\n\nD. File descriptor reservation\n- Check ulimit on startup\n- Log warning if too low\n- Fail fast instead of mysterious errors\n\nE. Adaptive limits\n- Start with low limit, increase based on system health\n- Monitor memory/goroutine count\n- Reduce limit under pressure\n\nRecommendation:\n1. Add global connection limit (10K default, configurable)\n2. Check ulimit on startup, warn if under 20K\n3. Add per-IP limit via Echo middleware (100 concurrent)\n4. Defer rate limiting to reverse proxy (nginx)\n\nPriority: P2 - connection exhaustion is a real DoS risk\n","notes":"RESOLVED: This connection limit concern is addressed by the consolidated resource limits solution.\n\n**Implemented in:**\n- **twitch-tow-mjm** (CONSOLIDATED: Resource Limits and Rate Limiting - Multi-Layer Defense)\n  - Layer 1: Global connection limits (week 1) - **directly addresses this discussion**\n  - Layer 2: Per-IP connection limits (week 1)\n  - Layer 3: Token bucket vote rate limiter (week 2)\n  - Layer 4: Connection rate limiting (week 2)\n\n**Also covered by epic:**\n- **twitch-tow-dmg** (Epic: Global WebSocket Connection Limits implementation details)\n\n**How the solution addresses this:**\n\n**A. Global connection limit per instance:** ✅\n```go\ntype GlobalConnectionLimiter struct {\n    current atomic.Int64\n    max     int64  // 10,000 default\n}\n```\n- Reject new connections with 503 when limit reached\n- Configurable via MAX_WEBSOCKET_CONNECTIONS\n- Log warning at 80% capacity\n- Metric: websocket_connections_current\n\n**B. Connection rate limiting:** ✅\n- 10 connections per second per IP\n- Uses golang.org/x/time/rate\n- Periodic cleanup to prevent memory leak\n\n**C. Per-IP connection limit:** ✅\n```go\ntype IPConnectionLimiter struct {\n    ips    map[string]int\n    maxPer int  // 100 default\n}\n```\n- Prevents single-source exhaustion\n- Accounts for NAT scenarios (configurable limit)\n- Metric: websocket_unique_ips\n\n**D. File descriptor reservation:** ✅\n```go\nfunc checkUlimit() {\n    // Warns if ulimit \u003c 20,000\n    // Logs recommended settings\n}\n```\n\nAll recommendations from this discussion (A-D) are implemented. Attack vectors (connection exhaustion, slowloris, session enumeration) are mitigated by the multi-layer defense.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:37.771962+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:36:34.360305+01:00","closed_at":"2026-02-12T17:36:34.360308+01:00"}
{"id":"twitch-tow-0bx","title":"Add timestamp freshness validation to webhook handler","description":"**Context**: Twitch EventSub best practices recommend validating message timestamp freshness (10 minute window) to prevent replay attacks using expired messages.\n\n**Current state**: Kappopher implements message ID deduplication for replay protection, but does not validate timestamp age.\n\n**Security impact**: LOW - Message ID deduplication already prevents replays, but timestamp validation adds defense-in-depth.\n\n**Implementation**:\n1. Parse Twitch-Eventsub-Message-Timestamp header\n2. Compare against current time (± 10 minutes per Twitch spec)\n3. Reject messages outside window with 403\n4. Add test coverage for expired messages\n\n**Location**: internal/twitch/webhook.go\n\n**Reference**: https://dev.twitch.tv/docs/eventsub/handling-webhook-events/","status":"closed","priority":3,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:42:41.011208+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:54:24.828388+01:00","closed_at":"2026-02-12T16:54:24.828388+01:00","close_reason":"Closed"}
{"id":"twitch-tow-0h2","title":"EPIC: Load Balancer Configuration Guide and Health Check Implementation","description":"## Epic Overview\nDocument load balancer configuration for horizontal scaling and implement health check endpoints to enable proper traffic routing and graceful instance management.\n\n## Parent Discussion\ntwitch-tow-2m8 (Load balancer configuration and WebSocket scaling)\n\n## User Story\nAs an operator deploying ChatPulse horizontally across multiple instances, I need comprehensive load balancer configuration guidance and health check endpoints so the load balancer can route traffic correctly and detect unhealthy instances.\n\n## Problem Analysis\n\n**Current state:**\n- ✅ Application is stateless (all state in Redis)\n- ✅ No sticky sessions required (WebSocket state in Redis)\n- ✅ Pure horizontal scaling works\n- ✅ Cookie-based sessions work across instances\n- ❌ No /health endpoint for load balancer probes\n- ❌ No graceful shutdown signaling\n- ❌ No documentation for load balancer setup\n\n**Why this is good:**\n- WebSocket connections can land on any instance\n- Client reconnection works seamlessly\n- Round-robin load balancing is sufficient\n- No complex session affinity needed\n\n**What's missing:**\n1. Health check endpoints\n2. Graceful shutdown coordination\n3. Load balancer configuration examples\n4. Deployment runbooks\n\n## Solution: Health Checks + Documentation\n\n### Part 1: Health Check Endpoints ✅\n\n**Already implemented in:** twitch-tow-682 (Phase 1 Health Checks)\n\nThis epic documents how to **USE** the health checks with load balancers, not implement them (already done).\n\n**Endpoints:**\n- `GET /health/live` → Liveness probe (process alive)\n- `GET /health/ready` → Readiness probe (dependencies healthy)\n- `GET /version` → Build info (git SHA, timestamp)\n\n### Part 2: Load Balancer Configuration Examples\n\nCreate comprehensive runbooks for popular load balancers.\n\n#### Nginx Configuration\n\n**File:** `docs/deployment/load-balancer-nginx.md` (NEW)\n\n```nginx\n# Upstream backend pool\nupstream chatpulse_backend {\n    # Use least_conn for better distribution of long-lived WebSocket connections\n    least_conn;\n    \n    # Backend instances\n    server 10.0.1.10:8080 max_fails=3 fail_timeout=30s;\n    server 10.0.1.11:8080 max_fails=3 fail_timeout=30s;\n    server 10.0.1.12:8080 max_fails=3 fail_timeout=30s;\n    \n    # Health check (requires nginx-plus or third-party module)\n    # For open-source nginx, use passive health checks via max_fails\n}\n\nserver {\n    listen 80;\n    server_name chatpulse.example.com;\n    \n    # Redirect HTTP to HTTPS\n    return 301 https://$host$request_uri;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name chatpulse.example.com;\n    \n    # SSL configuration\n    ssl_certificate /etc/nginx/ssl/cert.pem;\n    ssl_certificate_key /etc/nginx/ssl/key.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    \n    # WebSocket endpoints\n    location /ws/ {\n        proxy_pass http://chatpulse_backend;\n        \n        # WebSocket upgrade headers\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        \n        # Preserve client IP\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $host;\n        \n        # Timeouts for long-lived connections\n        proxy_read_timeout 3600s;  # 1 hour (OBS overlay stays open)\n        proxy_send_timeout 3600s;\n        proxy_connect_timeout 10s;\n        \n        # Buffer settings\n        proxy_buffering off;  # Disable for WebSocket\n    }\n    \n    # REST API endpoints\n    location / {\n        proxy_pass http://chatpulse_backend;\n        \n        # Standard proxy headers\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n        proxy_set_header Host $host;\n        \n        # Timeouts for HTTP requests\n        proxy_read_timeout 30s;\n        proxy_send_timeout 30s;\n        proxy_connect_timeout 5s;\n    }\n    \n    # Health check endpoint (internal only)\n    location /health/ready {\n        proxy_pass http://chatpulse_backend;\n        access_log off;  # Don't log health checks\n    }\n}\n\n# Health check monitoring (requires separate script or nginx-plus)\n# For open-source nginx, monitor via external tool (e.g., Prometheus blackbox_exporter)\n```\n\n#### AWS Application Load Balancer (ALB)\n\n**File:** `docs/deployment/load-balancer-aws-alb.md` (NEW)\n\n```yaml\n# Terraform configuration for ALB\nresource \"aws_lb_target_group\" \"chatpulse\" {\n  name     = \"chatpulse-tg\"\n  port     = 8080\n  protocol = \"HTTP\"\n  vpc_id   = var.vpc_id\n  \n  # Health check configuration\n  health_check {\n    enabled             = true\n    healthy_threshold   = 2\n    unhealthy_threshold = 2\n    timeout             = 5\n    interval            = 10\n    path                = \"/health/ready\"\n    matcher             = \"200\"\n  }\n  \n  # Deregistration delay (graceful shutdown)\n  deregistration_delay = 30\n  \n  # Sticky sessions NOT required (stateless)\n  stickiness {\n    enabled = false\n  }\n}\n\nresource \"aws_lb_listener\" \"chatpulse_https\" {\n  load_balancer_arn = aws_lb.main.arn\n  port              = \"443\"\n  protocol          = \"HTTPS\"\n  ssl_policy        = \"ELBSecurityPolicy-TLS-1-2-2017-01\"\n  certificate_arn   = var.certificate_arn\n  \n  default_action {\n    type             = \"forward\"\n    target_group_arn = aws_lb_target_group.chatpulse.arn\n  }\n}\n\n# Target group attributes for WebSocket\nresource \"aws_lb_target_group_attachment\" \"chatpulse\" {\n  count            = length(var.instance_ids)\n  target_group_arn = aws_lb_target_group.chatpulse.arn\n  target_id        = var.instance_ids[count.index]\n  port             = 8080\n}\n```\n\n**ALB automatically handles:**\n- WebSocket upgrade (HTTP/1.1 → WebSocket)\n- Connection draining (30s deregistration delay)\n- Health checks every 10 seconds\n- SSL/TLS termination\n\n#### Kubernetes Ingress + Service\n\n**File:** `docs/deployment/load-balancer-kubernetes.md` (NEW)\n\n```yaml\n# Service definition\napiVersion: v1\nkind: Service\nmetadata:\n  name: chatpulse\n  labels:\n    app: chatpulse\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 8080\n    name: http\n  selector:\n    app: chatpulse\n  # No session affinity needed (stateless)\n  sessionAffinity: None\n\n---\n# Deployment with probes\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: chatpulse\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: chatpulse\n  template:\n    metadata:\n      labels:\n        app: chatpulse\n    spec:\n      containers:\n      - name: chatpulse\n        image: chatpulse:latest\n        ports:\n        - containerPort: 8080\n          name: http\n        \n        # Liveness probe (restart if dead)\n        livenessProbe:\n          httpGet:\n            path: /health/live\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n          failureThreshold: 3\n        \n        # Readiness probe (remove from service if unhealthy)\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 2\n        \n        # Startup probe (for slow starts)\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 12  # 60s total\n        \n        # Graceful shutdown\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 15\"]  # Wait for deregistration\n\n---\n# Ingress definition\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: chatpulse\n  annotations:\n    kubernetes.io/ingress.class: nginx\n    nginx.ingress.kubernetes.io/websocket-services: \"chatpulse\"\n    nginx.ingress.kubernetes.io/proxy-read-timeout: \"3600\"\n    nginx.ingress.kubernetes.io/proxy-send-timeout: \"3600\"\nspec:\n  rules:\n  - host: chatpulse.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: chatpulse\n            port:\n              number: 80\n  tls:\n  - hosts:\n    - chatpulse.example.com\n    secretName: chatpulse-tls\n```\n\n### Part 3: Graceful Shutdown Best Practices\n\n**File:** `docs/deployment/graceful-shutdown.md` (NEW)\n\n**Current behavior:**\n- SIGINT/SIGTERM → `runGracefulShutdown` → 30s timeout\n- Echo server stops accepting new connections\n- Existing connections are NOT explicitly closed\n- Broadcaster.Stop() waits for goroutine exit\n\n**Best practices:**\n1. **Stop accepting new connections** ✅ (already done)\n2. **Notify load balancer** (via failing health checks)\n3. **Wait for connection drain** (30s)\n4. **Close remaining connections gracefully** (send WebSocket close frame)\n5. **Exit**\n\n**Enhancement (optional future work):**\n```go\nfunc (s *Server) Shutdown(ctx context.Context) error {\n    // 1. Start failing health checks\n    atomic.StoreInt32(\u0026s.shuttingDown, 1)\n    \n    // 2. Stop accepting new connections\n    if err := s.echo.Shutdown(ctx); err != nil {\n        return err\n    }\n    \n    // 3. Send close frames to all WebSocket clients\n    s.broadcaster.CloseAllConnections(\"server shutting down\")\n    \n    // 4. Wait for broadcaster to stop\n    s.broadcaster.Stop()\n    \n    return nil\n}\n```\n\n## Implementation Tasks\n\n### Task 1: Create Nginx runbook\n**File:** `docs/deployment/load-balancer-nginx.md` (NEW - 400 lines)\n\nComplete Nginx configuration with:\n- Upstream pool configuration\n- WebSocket proxy settings\n- Health check integration\n- SSL/TLS setup\n- Monitoring recommendations\n\n### Task 2: Create AWS ALB runbook\n**File:** `docs/deployment/load-balancer-aws-alb.md` (NEW - 300 lines)\n\nTerraform examples for:\n- Target group with health checks\n- HTTPS listener\n- Security groups\n- Auto-scaling integration\n\n### Task 3: Create Kubernetes runbook\n**File:** `docs/deployment/load-balancer-kubernetes.md` (NEW - 400 lines)\n\nYAML manifests for:\n- Service definition (ClusterIP)\n- Deployment with probes\n- Ingress for WebSocket\n- HorizontalPodAutoscaler example\n\n### Task 4: Document graceful shutdown\n**File:** `docs/deployment/graceful-shutdown.md` (NEW - 250 lines)\n\nDocument:\n- Current shutdown behavior\n- Load balancer deregistration timing\n- Connection draining best practices\n- Optional enhancements\n\n### Task 5: Create troubleshooting guide\n**File:** `docs/deployment/troubleshooting-load-balancer.md` (NEW - 300 lines)\n\nCommon issues:\n- WebSocket upgrade failures (missing headers)\n- Health check failures (timeout too short)\n- Sticky session misconfiguration (not needed)\n- SSL/TLS passthrough vs termination\n- Connection timeout tuning\n\n### Task 6: Update CLAUDE.md\n**File:** `CLAUDE.md`\n\nAdd deployment section linking to runbooks.\n\n## Acceptance Criteria\n\n- ✅ Nginx configuration documented with complete working example\n- ✅ AWS ALB Terraform configuration provided\n- ✅ Kubernetes manifests with proper probes\n- ✅ Graceful shutdown behavior documented\n- ✅ Troubleshooting guide covers common issues\n- ✅ All configurations tested in staging environment\n- ✅ WebSocket connections work through load balancer\n- ✅ Health checks properly remove unhealthy instances\n\n## Files Created/Modified\n\n**New files:**\n- `docs/deployment/load-balancer-nginx.md` (400 lines)\n- `docs/deployment/load-balancer-aws-alb.md` (300 lines)\n- `docs/deployment/load-balancer-kubernetes.md` (400 lines)\n- `docs/deployment/graceful-shutdown.md` (250 lines)\n- `docs/deployment/troubleshooting-load-balancer.md` (300 lines)\n\n**Modified files:**\n- `CLAUDE.md` (add deployment documentation section)\n\n## Testing Strategy\n\n**Manual testing:**\n- Deploy 3 instances behind Nginx\n- Verify WebSocket connections work\n- Kill one instance, verify health check removes it\n- Verify no connection drops for other instances\n- Test graceful shutdown (no dropped connections)\n\n**Load testing:**\n- 1,000 concurrent WebSocket connections\n- Verify even distribution across instances\n- Kill instance during load, verify reconnections\n- Measure connection distribution (should be balanced)\n\n## Dependencies\n- Requires health check endpoints (twitch-tow-682) - already implemented\n- Complements connection limits (twitch-tow-dmg)\n\n## Success Metrics\n- WebSocket connections distributed evenly across instances\n- Unhealthy instances removed from pool within 30 seconds\n- Zero connection drops during graceful shutdown\n- Clear documentation enables operators to deploy confidently\n\n## Effort Estimate\n**3 developer-days**\n\nBreakdown:\n- Nginx runbook: 1 day\n- AWS ALB runbook: 0.5 day\n- Kubernetes runbook: 1 day\n- Testing + troubleshooting guide: 0.5 day\n\n## Risk Mitigation\n- **Risk:** Load balancer misconfiguration causes connection failures\n  - **Mitigation:** Provide tested examples, troubleshooting guide\n- **Risk:** Graceful shutdown not working correctly\n  - **Mitigation:** Document expected behavior, test in staging\n- **Risk:** WebSocket timeout too short, connections dropped\n  - **Mitigation:** Document recommended timeouts (1 hour+)","status":"open","priority":3,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:42:16.59531+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:51.405159+01:00"}
{"id":"twitch-tow-0jo","title":"Idea: Package dependency visualization missing","description":"The codebase has a well-designed layered architecture but no visual documentation of dependencies:\n\n**Current state:**\n- CLAUDE.md describes architecture narratively (excellent documentation)\n- No dependency diagram showing package relationships\n- New contributors must read code to understand layering\n\n**Package layers (inferred from imports):**\n\n\n**Benefits of visualization:**\n1. **Onboarding**: New devs understand structure faster\n2. **Dependency validation**: Easy to spot circular deps or layer violations\n3. **Change impact**: Visual map shows what's affected by changes\n4. **Documentation maintenance**: Diagrams catch architectural drift\n\n**Options:**\nA. **Add static diagram**: Create dependency graph (Graphviz/Mermaid) in docs/\nB. **Use go-modgraph**: Generate on-demand with \nC. **Keep narrative only**: Argue that CLAUDE.md text is sufficient\nD. **Add godepgraph**: Generate PNG diagram in CI/docs\n\n**Tools to consider:**\n-  - generates GraphViz from Go packages\n-  - visualizes go.mod dependencies\n- Mermaid diagram in README.md (GitHub renders natively)\n\n**Low effort, high value**: A Mermaid diagram in CLAUDE.md could be added in ~30 minutes.","notes":"RESOLVED: Converted to Epic 12 (twitch-tow-e6s) - Add Package Dependency Visualization. Implements: Mermaid diagram in CLAUDE.md showing layered architecture, dependency validation script, CI enforcement of layering rules. 1 hour effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:43.206052+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:49:12.568555+01:00","closed_at":"2026-02-12T17:49:12.568559+01:00"}
{"id":"twitch-tow-0kl","title":"EPIC: Lua Function Versioning for Safe Rolling Deploys","description":"**User Story:** As a developer, I need versioned Lua function deployment so that I can safely make breaking changes to Redis Functions without causing data corruption during rolling deploys.\n\n**Problem Context:** Current Lua function loading uses `FUNCTION LOAD REPLACE` with no versioning:\n- Breaking changes cause incompatibility between old/new instances\n- No rollback mechanism (REPLACE overwrites immediately)\n- No version detection (can't detect mismatch)\n- Race during rolling deploy (mixed versions call wrong function)\n\n**Example failure:** Instance with v1 binary calls v2 function (different signature) → parse error or data corruption.\n\n**Solution Overview:** Add versioned function names (apply_vote_v1 → apply_vote_v2), version tracking in Redis, backward compatibility checks, and safe upgrade procedure.\n\n## Task Breakdown\n\n### 1. Add Function Version Constant\n\n**File:** `internal/redis/chatpulse.lua`\n\n**Add version header:**\n```lua\n#!lua name=chatpulse\n\n-- Library version (increment on breaking changes)\nlocal LIBRARY_VERSION = \"2\"\n\n-- Versioned function names (v2 added 2026-02-12)\nredis.register_function{\n  function_name = 'apply_vote_v2',\n  callback = function(keys, args)\n    -- ... existing apply_vote logic\n    -- Return version in response for validation\n    return {value, LIBRARY_VERSION}\n  end,\n  flags = { 'no-writes' }\n}\n\nredis.register_function{\n  function_name = 'get_decayed_value_v2',\n  callback = function(keys, args)\n    -- ... existing get_decayed_value logic\n    return {value, LIBRARY_VERSION}\n  end,\n  flags = { 'no-writes' }\n}\n\n-- Legacy v1 functions (keep for backward compat during rollout)\nredis.register_function{\n  function_name = 'apply_vote',\n  callback = function(keys, args)\n    -- Forward to v2\n    return redis.call('FCALL', 'apply_vote_v2', 1, keys[1], unpack(args))\n  end\n}\n\nredis.register_function{\n  function_name = 'get_decayed_value',\n  callback = function(keys, args)\n    -- Forward to v2\n    return redis.call('FCALL', 'get_decayed_value_v2', 1, keys[1], unpack(args))\n  end\n}\n```\n\n### 2. Add Version Tracking in Redis\n\n**File:** `internal/redis/client.go`\n\n**Track loaded version:**\n```go\nconst LibraryVersion = \"2\"  // Must match Lua LIBRARY_VERSION\n\nfunc NewClient(redisURL string) (*redis.Client, error) {\n    opts, err := redis.ParseURL(redisURL)\n    if err != nil {\n        return nil, fmt.Errorf(\"invalid redis URL: %w\", err)\n    }\n    \n    client := redis.NewClient(opts)\n    \n    // Ping to verify connection\n    if err := client.Ping(context.Background()).Err(); err != nil {\n        return nil, fmt.Errorf(\"redis ping failed: %w\", err)\n    }\n    \n    // Load Lua library\n    if err := loadLuaLibrary(client); err != nil {\n        return nil, fmt.Errorf(\"failed to load Lua library: %w\", err)\n    }\n    \n    // Store version in Redis for coordination\n    ctx := context.Background()\n    versionKey := \"chatpulse:function:version\"\n    \n    // Check existing version\n    existingVersion, err := client.Get(ctx, versionKey).Result()\n    if err != nil \u0026\u0026 err != redis.Nil {\n        return nil, fmt.Errorf(\"failed to check function version: %w\", err)\n    }\n    \n    if existingVersion != \"\" \u0026\u0026 existingVersion != LibraryVersion {\n        slog.Warn(\"Lua function version mismatch\",\n            \"current_version\", existingVersion,\n            \"new_version\", LibraryVersion,\n            \"action\", \"replacing\")\n        luaVersionMismatchesTotal.Inc()\n    }\n    \n    // Update version\n    err = client.Set(ctx, versionKey, LibraryVersion, 0).Err()\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to set function version: %w\", err)\n    }\n    \n    slog.Info(\"Lua functions loaded\",\n        \"version\", LibraryVersion,\n        \"library\", \"chatpulse\")\n    \n    return client, nil\n}\n```\n\n### 3. Update Function Callers to Use Versioned Names\n\n**File:** `internal/redis/sentiment_store.go`\n\n**Use versioned function name:**\n```go\nfunc (s *SentimentStore) ApplyVote(ctx context.Context, sessionUUID uuid.UUID, delta float64) (float64, error) {\n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    now := s.clock.Now().Unix()\n    \n    // Call versioned function (v2)\n    result, err := s.client.FCall(ctx, \"apply_vote_v2\", []string{key}, delta, now).Result()\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to apply vote: %w\", err)\n    }\n    \n    // Parse result: [value, version]\n    resultSlice, ok := result.([]interface{})\n    if !ok || len(resultSlice) != 2 {\n        return 0, fmt.Errorf(\"unexpected result format from apply_vote_v2\")\n    }\n    \n    valueStr, ok := resultSlice[0].(string)\n    if !ok {\n        return 0, fmt.Errorf(\"unexpected value type from apply_vote_v2\")\n    }\n    \n    version, ok := resultSlice[1].(string)\n    if !ok {\n        version = \"unknown\"\n    }\n    \n    // Validate version matches\n    if version != LibraryVersion {\n        luaVersionMismatchesTotal.Inc()\n        slog.Warn(\"Lua function version mismatch in response\",\n            \"expected\", LibraryVersion,\n            \"actual\", version,\n            \"function\", \"apply_vote_v2\")\n    }\n    \n    value, err := strconv.ParseFloat(valueStr, 64)\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to parse vote result: %w\", err)\n    }\n    \n    return value, nil\n}\n\nfunc (s *SentimentStore) GetSentiment(ctx context.Context, sessionUUID uuid.UUID) (float64, error) {\n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    now := s.clock.Now().Unix()\n    \n    // Call versioned function (v2, read-only)\n    result, err := s.client.FCallRO(ctx, \"get_decayed_value_v2\", []string{key}, now).Result()\n    if err != nil {\n        if errors.Is(err, redis.Nil) {\n            return 0, nil\n        }\n        return 0, fmt.Errorf(\"failed to get sentiment: %w\", err)\n    }\n    \n    // Parse result: [value, version]\n    resultSlice, ok := result.([]interface{})\n    if !ok || len(resultSlice) != 2 {\n        return 0, fmt.Errorf(\"unexpected result format from get_decayed_value_v2\")\n    }\n    \n    valueStr, ok := resultSlice[0].(string)\n    if !ok {\n        return 0, fmt.Errorf(\"unexpected value type\")\n    }\n    \n    value, err := strconv.ParseFloat(valueStr, 64)\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to parse sentiment value: %w\", err)\n    }\n    \n    return value, nil\n}\n```\n\n### 4. Add Version Mismatch Metrics\n\n**File:** `internal/redis/client.go`\n\n```go\nvar (\n    luaVersionMismatchesTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"lua_version_mismatches_total\",\n            Help: \"Total number of Lua function version mismatches detected\",\n        })\n    \n    luaFunctionLoadsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"lua_function_loads_total\",\n            Help: \"Total number of Lua function library loads\",\n        },\n        []string{\"version\"},\n    )\n)\n\nfunc init() {\n    prometheus.MustRegister(\n        luaVersionMismatchesTotal,\n        luaFunctionLoadsTotal,\n    )\n}\n\nfunc loadLuaLibrary(client *redis.Client) error {\n    // ... existing load logic\n    luaFunctionLoadsTotal.WithLabelValues(LibraryVersion).Inc()\n    return nil\n}\n```\n\n### 5. Add Health Check for Function Version\n\n**File:** `internal/server/handlers_health.go`\n\n**Check Lua function version:**\n```go\nfunc (s *Server) handleReadiness(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 2*time.Second)\n    defer cancel()\n    \n    checks := []healthCheck{\n        {name: \"redis\", fn: s.checkRedis},\n        {name: \"postgres\", fn: s.checkPostgres},\n        {name: \"lua_functions\", fn: s.checkLuaFunctions},  // NEW\n    }\n    \n    for _, check := range checks {\n        if err := check.fn(ctx); err != nil {\n            return c.JSON(503, map[string]any{\n                \"status\": \"unhealthy\",\n                \"failed_check\": check.name,\n                \"error\": err.Error(),\n            })\n        }\n    }\n    \n    return c.JSON(200, map[string]string{\"status\": \"ready\"})\n}\n\nfunc (s *Server) checkLuaFunctions(ctx context.Context) error {\n    // Check version matches\n    versionKey := \"chatpulse:function:version\"\n    version, err := s.redisClient.Get(ctx, versionKey).Result()\n    if err != nil {\n        return fmt.Errorf(\"lua function version not found: %w\", err)\n    }\n    \n    if version != redis.LibraryVersion {\n        return fmt.Errorf(\"lua function version mismatch: expected %s, got %s\",\n            redis.LibraryVersion, version)\n    }\n    \n    return nil\n}\n```\n\n### 6. Document Upgrade Procedure\n\n**File:** `CLAUDE.md`\n\n**Add section under \"Redis Architecture\":**\n```markdown\n### Lua Function Versioning\n\nRedis Functions (chatpulse library) are versioned to enable safe rolling deploys with breaking changes.\n\n**Version strategy:**\n- Function names include version suffix: `apply_vote_v2`, `get_decayed_value_v2`\n- Legacy function names forward to latest version: `apply_vote` → `apply_vote_v2`\n- Version constant in Lua (`LIBRARY_VERSION`) and Go (`LibraryVersion`) must match\n- Functions return version in response for validation\n\n**Version tracking:**\n- Key: `chatpulse:function:version` stores currently loaded version\n- Health check validates version matches binary expectation\n- Mismatch logged as warning, metric incremented\n\n**Metrics:**\n- lua_version_mismatches_total (counter)\n- lua_function_loads_total{version} (counter)\n\n**Zero-downtime upgrade procedure (breaking changes):**\n\n1. **Prepare:** Add new versioned functions to chatpulse.lua (e.g., v3)\n2. **Keep legacy:** Keep v2 functions for backward compat\n3. **Deploy:** Rolling deploy new binary\n   - Each instance loads v3 functions via FUNCTION LOAD REPLACE\n   - Old instances still call v2 functions (work via forwarding)\n4. **Verify:** Check lua_version_mismatches_total metric (should be 0 after rollout)\n5. **Cleanup:** Remove v2 functions in next release (after all instances upgraded)\n\n**Backward compatibility:**\n- Always provide legacy function names that forward to latest\n- Keep forwarding for N+1 releases (one release grace period)\n\n**Rollback:**\n- Rollback binary deployment (old instances reload old functions)\n- Version mismatch warnings expected during rollback (safe)\n```\n\n### 7. Testing\n\n**File:** `internal/redis/client_versioning_test.go` (NEW)\n\n```go\n// TestLuaFunctionVersioning verifies version tracking\nfunc TestLuaFunctionVersioning(t *testing.T) {\n    // Setup: Load functions with version \"2\"\n    // Assert: chatpulse:function:version key set to \"2\"\n    // Act: Load again with same version\n    // Assert: No mismatch warning\n}\n\n// TestLuaFunctionVersionMismatch verifies detection\nfunc TestLuaFunctionVersionMismatch(t *testing.T) {\n    // Setup: Set chatpulse:function:version to \"1\"\n    // Act: Load functions with version \"2\"\n    // Assert: Mismatch warning logged, metric incremented\n}\n\n// TestVersionedFunctionCall verifies v2 functions work\nfunc TestVersionedFunctionCall(t *testing.T) {\n    // Setup: Load v2 functions\n    // Act: Call apply_vote_v2\n    // Assert: Returns [value, \"2\"]\n}\n\n// TestLegacyFunctionForwarding verifies v1 compatibility\nfunc TestLegacyFunctionForwarding(t *testing.T) {\n    // Setup: Load v2 functions\n    // Act: Call apply_vote (legacy name)\n    // Assert: Forwards to apply_vote_v2, returns correct value\n}\n```\n\n## Acceptance Criteria\n\n✅ Lua functions have versioned names (apply_vote_v2, get_decayed_value_v2)\n✅ Legacy function names forward to latest version\n✅ Version constant in Lua and Go match\n✅ Version stored in Redis key (chatpulse:function:version)\n✅ Health check validates version matches binary expectation\n✅ Metrics track version mismatches and function loads\n✅ Functions return version in response for validation\n✅ CLAUDE.md documents zero-downtime upgrade procedure\n✅ Tests verify version tracking, mismatch detection, forwarding\n\n## Dependencies\n\n- None (self-contained improvement)\n\n## Files Modified\n\n**Modified:**\n- internal/redis/chatpulse.lua (add versioning, legacy forwarding)\n- internal/redis/client.go (version tracking, metrics)\n- internal/redis/sentiment_store.go (call versioned functions)\n- internal/server/handlers_health.go (version health check)\n- CLAUDE.md (document upgrade procedure)\n\n**New:**\n- internal/redis/client_versioning_test.go (versioning tests)\n\n## Estimated Effort\n\n**Implementation:** 2 developer-days\n- Lua versioning: 3 hours\n- Version tracking: 2 hours\n- Function caller updates: 3 hours\n- Health check: 1 hour\n- Metrics: 2 hours\n- Testing: 1 day\n- Documentation: 2 hours\n\n**Total:** 2 developer-days\n\n## Rollout Strategy\n\n1. Deploy v2 functions with legacy forwarding (backward compatible)\n2. Monitor lua_version_mismatches_total (should be 0)\n3. Verify health checks pass (/ready returns 200)\n4. Document upgrade procedure for future breaking changes\n5. Keep v1 forwarding for 1 release cycle, then remove","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:43:14.373935+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:44.147778+01:00"}
{"id":"twitch-tow-0lk","title":"Discussion: Cleanup timer thundering herd on multiple instances","description":"Session cleanup runs every 30 seconds with ListOrphans scanning Redis, followed by background Twitch unsubscribe calls. This can cause thundering herd issues.\n\nCurrent implementation:\n- Cleanup timer: 30s interval\n- ListOrphans: SCAN all session:* keys\n- Check each for disconnect timestamp over 30s\n- Delete from Redis (synchronous)\n- Twitch unsubscribe in background goroutine (async)\n\nIssues with current approach:\n\n1. All instances run cleanup simultaneously\n- Every instance has its own 30s ticker\n- All start at boot time (roughly synchronized)\n- All instances SCAN Redis at same time\n- All instances call Twitch API at same time\n\n2. Duplicate Twitch API calls\n- Instance A finds orphan, calls unsubscribe\n- Instance B finds same orphan, calls unsubscribe\n- Twitch API rate limits apply\n- Wastes API quota\n\n3. Redis delete race\n- Instance A: finds orphan, deletes\n- Instance B: finds orphan (before A deleted), deletes again\n- Not harmful but wasteful\n\n4. No backoff on failure\n- Twitch API call fails → orphan remains\n- Next tick (30s later) retries immediately\n- No exponential backoff\n- Could hit rate limits\n\n5. Unbounded background goroutines\n- Each cleanup spawns goroutine for Twitch calls\n- If Twitch API is slow, goroutines accumulate\n- cleanupWg.Go has no limit\n- Could exhaust goroutine budget\n\nScalability impact:\n- 10 instances × 100 orphans = 1000 Twitch API calls in burst\n- Twitch rate limits: unclear but likely strict\n- Redis SCAN contention minimal (safe operation)\n\nPotential solutions:\n\nA. Leader election for cleanup\n- Use Redis SETNX for leader lock\n- Only leader runs cleanup\n- Other instances skip cleanup\n- Leader lease renewal (30s)\n\nB. Distributed task queue\n- Put orphan cleanup tasks in Redis queue\n- Instances pull tasks from queue\n- Guarantees single processing per orphan\n- Requires queue infrastructure\n\nC. Randomize cleanup interval\n- Each instance: 30s ± random jitter (0-10s)\n- Spreads load over time\n- Reduces simultaneous SCAN\n- Doesn't prevent duplicates\n\nD. Dedupe via Redis flag\n- Mark orphan with cleanup:{sessionUUID} key before processing\n- Other instances see flag, skip\n- Delete flag after unsubscribe\n- Race still possible but reduced\n\nE. Partition by hash\n- Each instance responsible for UUID range\n- Hash(sessionUUID) % instance_count\n- Requires instance discovery\n- Complex for dynamic scaling\n\nRecommendation:\nPhase 1: Add jitter (option C) - simple, reduces burst\nPhase 2: Leader election (option A) - eliminates duplicates\nPhase 3: Proper task queue (option B) - best long-term solution\n\nJitter implementation:\n```go\njitter := time.Duration(rand.Intn(10)) * time.Second\nticker := clock.NewTicker(cleanupInterval + jitter)\n```\n\nLeader election with Redis:\n```go\nacquired := rdb.SetNX(ctx, \"cleanup:leader\", instanceID, 30*time.Second)\nif acquired {\n    runCleanup()\n}\n```\n\nPriority: P2 - duplicates waste resources but not breaking\n","notes":"RESOLVED: Converted to Epic 9 (twitch-tow-y9l) - Cleanup Thundering Herd Prevention. Implements Redis leader election with jitter to prevent duplicate cleanup and API calls. 5 hours effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:56.190196+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:43:07.66662+01:00","closed_at":"2026-02-12T17:43:07.666623+01:00"}
{"id":"twitch-tow-0qy","title":"Positive: Excellent linter configuration","description":"The golangci-lint configuration shows mature practices:\n\n**Strengths:**\n\n1. **Modern schema**: Uses golangci-lint v2 config format\n2. **Reasonable linter set**: Enables 18 additional linters beyond standard set\n   - Performance: gocritic (diagnostic + performance tags), prealloc\n   - Correctness: errorlint, nilerr, noctx, durationcheck, fatcontext\n   - Code quality: goconst, unparam, wastedassign, unconvert\n   - Readability: misspell, bodyclose\n\n3. **Thoughtful exclusions**:\n   - exitAfterDefer: Correctly notes it's standard pattern in main()/TestMain()\n   - hugeParam: Acknowledges struct sizes are acceptable\n   - errcheck: Excludes io.Close, Rollback, WebSocket cleanup (intentional)\n   - SA1019 middleware.Logger: Documents tech debt with reason\n\n4. **Appropriate scoping**:\n   - Excludes sqlcgen (generated code)\n   - unparam.check-exported: false (avoids forcing API changes)\n   - goconst: min-len=3, min-occurrences=3 (balanced threshold)\n\n5. **Test-friendly**:\n   - bodyclose exclusion for _test.go files (WebSocket false positives)\n\n**Minor notes:**\n- No cyclomatic complexity check (gocyclo) - could add if complexity becomes an issue\n- No import grouping enforcement (gci/goimports) - current imports are manually organized well\n- No deprecated linter warnings - config is up-to-date\n\n**Verdict**: This is a well-maintained, production-ready linter config. No changes needed.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:25.475529+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:02.894267+01:00","closed_at":"2026-02-12T17:57:02.894267+01:00","close_reason":"Positive architectural notes - no action required"}
{"id":"twitch-tow-1hb","title":"Idea: Consider adding code coverage badge and requirements","description":"The codebase has make test-coverage target but no coverage requirements or visibility:\n\nCurrent state:\n- make test-coverage generates coverage.out and coverage.html\n- 141 tests across 8 packages\n- No coverage requirement (no CI check for minimum coverage)\n- No coverage badge in README\n- No per-package coverage visibility\n\nBenefits of coverage tracking:\n1. Shows test health at a glance\n2. Catches untested code paths in PRs\n3. Provides baseline for improvement\n4. Prevents regressions in test coverage\n\nRecommendation:\nA. Add coverage badge to README (via Codecov, Coveralls, or GitHub Actions)\nB. Set minimum coverage threshold (suggest 70% for this codebase)\nC. Add per-package coverage report in CI\nD. Document coverage targets in CLAUDE.md\n\nNote: 100% coverage is not the goal - focus on critical paths (domain logic, Redis operations, HTTP handlers). Template rendering and simple getters can be lower priority.\n\nExample thresholds by package:\n- domain/: N/A (interfaces only)\n- sentiment/: 90%+ (core business logic)\n- redis/: 80%+ (integration tests cover most)\n- server/: 75%+ (handler logic)\n- broadcast/: 85%+ (concurrency patterns need thorough testing)\n\nLow effort high value addition.","notes":"RESOLVED: Merged into Epic 6 (twitch-tow-8dx) - Code Quality Improvements. Coverage badge + CI enforcement implemented in Task 1 of epic.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:09:26.584305+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:43.595978+01:00","closed_at":"2026-02-12T17:37:43.595991+01:00"}
{"id":"twitch-tow-1iq","title":"Discussion: Repository naming inconsistency","description":"Repository naming shows minor inconsistency across the codebase:\n\n**Naming patterns:**\n\nDatabase repositories (internal/database/):\n- UserRepo (type) + NewUserRepo (constructor) + UserRepository (interface)\n- ConfigRepo (type) + NewConfigRepo (constructor) + ConfigRepository (interface)\n- EventSubRepo (type) + NewEventSubRepo (constructor) + EventSubRepository (interface)\nPattern: XxxRepo (impl) + XxxRepository (interface)\n\nRedis repositories (internal/redis/):\n- SessionRepo (type) + NewSessionRepo (constructor) + SessionRepository (interface)\n- SentimentStore (type) + NewSentimentStore (constructor) + SentimentStore (interface) ← interface matches impl name\\!\n- Debouncer (type) + NewDebouncer (constructor) + Debouncer (interface) ← interface matches impl name\\!\n\n**Inconsistency:**\n- Database repos: Impl = 'Repo', Interface = 'Repository'\n- Redis repos: Impl = Interface name (SessionRepo, SentimentStore, Debouncer)\n\n**Why it matters:**\n1. Grep for 'SessionRepository' finds both interface definition and implementation\n2. IDE 'Find Usages' mixes interface and impl references\n3. New contributors may be confused by the pattern shift\n\n**Options:**\nA. **Standardize on 'Repo' vs 'Repository'**: Rename all impls to XxxRepo, interfaces to XxxRepository\nB. **Match names**: Rename interfaces to match impl (SessionRepo interface + SessionRepo impl)\nC. **Keep current**: Argue that context makes it clear (domain/ = interface, database|redis/ = impl)\nD. **Add suffixes**: SessionRepoImpl vs SessionRepoInterface (Go antipattern, avoid)\n\n**Go idiom**: Interfaces live where they're used (consumer-side), impls in producer packages. Names can match if no ambiguity.\n\n**Recommendation**: Document the naming convention or standardize on Repo (impl) + Repository (interface) everywhere.","notes":"RESOLVED: Merged into Epic 6 (twitch-tow-8dx) - Code Quality Improvements. Repository vs Service naming pattern documented in Task 3 (no code changes needed, intentional pattern).","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:12.795432+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:45.783969+01:00","closed_at":"2026-02-12T17:37:45.783973+01:00"}
{"id":"twitch-tow-1n1","title":"Solution: Observability implementation plan","description":"SOLUTION PROPOSAL for consensus observability concerns (twitch-tow-6hx, twitch-tow-bdb, twitch-tow-c8q)\n\n## Problem Summary\nAll three architects identified missing observability as a production blocker. Current state has no metrics, health checks, or operational visibility.\n\n## Proposed Solution: Three-Phase Implementation\n\n### Phase 1 - Health Checks (Day 1 - Required for production)\nAdd health check endpoints:\n- GET /health - basic liveness (returns 200 OK)\n- GET /health/ready - readiness check (validates Redis + PostgreSQL connections)\n- GET /health/live - liveness check (service is running, may not be ready)\n\nImplementation:\n- Add to internal/server/routes.go\n- No auth required (used by load balancers)\n- Timeout: 5s per dependency check\n- Returns JSON with component status\n\n### Phase 2 - Core Metrics (Week 1 - Operational visibility)\nAdd Prometheus metrics via prometheus/client_golang:\n- HTTP metrics (request count, duration, status codes by route)\n- WebSocket metrics (active connections, broadcasts/sec, message queue depth)\n- Redis metrics (operation count, latency, error rate, pool stats)\n- PostgreSQL metrics (query count, latency, pool stats)\n- Business metrics (active sessions, votes/sec, sentiment updates/sec)\n\nEndpoint: GET /metrics (Prometheus scrape target)\n\n### Phase 3 - Tracing (Week 2 - Optional, debugging)\nAdd OpenTelemetry tracing for request flows:\n- HTTP request → database query → Redis operation → WebSocket broadcast\n- Useful for debugging performance issues\n- Optional: Can defer to future iteration\n\n## Implementation Details\n\nAdd new internal/metrics package:\n\n\nMetrics to track:\n- http_requests_total (counter, labels: method, path, status)\n- http_request_duration_seconds (histogram, labels: method, path)\n- websocket_connections_active (gauge, labels: session_uuid)\n- broadcaster_tick_duration_seconds (histogram)\n- redis_operations_total (counter, labels: operation, status)\n- redis_operation_duration_seconds (histogram, labels: operation)\n- postgres_queries_total (counter, labels: query, status)\n- sessions_active (gauge)\n- votes_processed_total (counter, labels: trigger_matched)\n\n## Trade-offs\n\nPros:\n- Standard Prometheus format (works with Grafana, Datadog, etc.)\n- Low overhead (~1-2% CPU for metrics collection)\n- Essential for production operations\n- Enables alerting on SLOs\n\nCons:\n- Adds dependency (prometheus/client_golang)\n- Metrics endpoint needs scrape authentication in production\n- Cardinality explosion risk (avoid session_uuid labels on high-cardinality metrics)\n\n## Estimated Effort\n- Phase 1: 4 hours (health checks)\n- Phase 2: 16 hours (metrics instrumentation)\n- Phase 3: 8 hours (optional tracing)\n\n## Vote\n+1 from Maintainability architect. This is essential for production readiness and aligns with industry best practices.","notes":"Vote: +1 from architect-scalability\n\nRATIONALE: Solid proposal with good package structure. internal/metrics package is clean organization.\n\nPROPOSAL: Merge with twitch-tow-eyl. Use my timeline + your metrics package structure.\n\nOpenTelemetry deferral is right call - YAGNI for monolith.","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:11:32.110011+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.293617+01:00","closed_at":"2026-02-12T17:56:57.293617+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-1vt","title":"Fix race condition in cleanupWg usage","description":"**High Priority (Concurrency)**\n\nLocation: internal/app/service.go lines 192-209, 232\n\nIssue: sync.WaitGroup is not thread-safe for concurrent Add() calls. If multiple cleanup iterations happen before previous ones complete, there's a race.\n\nImpact: Can cause panic or incorrect goroutine counting.\n\nFix:\n- Use errgroup.Group which handles synchronization properly\n- Or: Add mutex around WaitGroup.Add()\n- Or: Ensure cleanup completes before next iteration\n- Test with race detector: make test-race","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:25:52.267219+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:35:49.138938+01:00","closed_at":"2026-02-12T16:35:49.138938+01:00","close_reason":"VERIFIED SAFE: Go 1.26's sync.WaitGroup.Go() handles concurrent calls safely. Added comprehensive concurrent cleanup test (TestCleanupOrphans_ConcurrentCleanupCalls) that passes with race detector. The WaitGroup.Go() method internally manages Add()/Done() synchronization, eliminating the race condition described in the original issue. No code changes needed - existing implementation is correct."}
{"id":"twitch-tow-21n","title":"Discussion: Ref counting has race conditions across instances","description":"Ref counting ensures cleanup only happens when no instances serve a session, but the implementation has race conditions and edge cases.\n\nCurrent implementation:\n1. First WS connection: app.IncrRefCount (Redis INCR)\n2. Last WS disconnection: app.DecrRefCount (Redis DECR)\n3. If count hits 0: MarkDisconnected (set timestamp)\n4. Cleanup: ListOrphans finds sessions with timestamp over 30s\n\nRace conditions:\n\n1. Connect/disconnect race across instances\n```\nInstance A: last client disconnects → DecrRefCount → 0\nInstance B: first client connects → IncrRefCount → 1\nBUT: Instance A already called MarkDisconnected\nResult: Active session marked as disconnected\n```\n\n2. Cleanup during activation\n```\nCleanup: finds orphan, deletes session\nConcurrent: EnsureSessionActive creates session\nResult: Race on Redis keys, undefined state\n```\n\n3. Ref count decrement below zero\n```\nInstance crashes after WS connect but before IncrRefCount\nSession remains in Redis with count never incremented\nCleanup timer decrements on disconnect\nCount goes negative\n```\n\n4. Double decrement\n```\nClient disconnects ungracefully\nRead pump returns error → Unregister → DecrRefCount\nBut connection was already counted down (how?)\nResult: Ref count decremented twice\n```\n\nWhy these races are partially mitigated:\n1. 30-second grace period before cleanup\n   - Most races resolve within 30s\n   - Unlikely new connection and cleanup overlap\n   \n2. MarkDisconnected is idempotent\n   - Multiple calls just update timestamp\n   - Not harmful\n   \n3. DeleteSession removes both session and ref_count\n   - Atomic-ish (uses pipeline)\n   - Session resurrection creates new ref count at 0\n\n4. IncrRefCount after EnsureSessionActive\n   - Session created, then ref count incremented\n   - If increment fails, session stays at 0\n   - Next cleanup removes it (self-healing)\n\nFailure modes that are NOT handled:\n\nA. Ref count leak (never decremented)\n- Instance crashes after IncrRefCount\n- Ref count stays at 1+ forever\n- Session never marked disconnected\n- Never cleaned up\n- Memory leak in Redis\n\nB. Ref count underflow\n- More decrements than increments\n- Count goes negative\n- DecrRefCount returns -1\n- MarkDisconnected not called (count \u003c= 0 check)\n- Session marked disconnected too late\n\nC. Concurrent activation + cleanup\n- No locking between EnsureSessionActive and DeleteSession\n- DeleteSession could run between SessionExists and ActivateSession\n- Session appears not to exist, gets recreated\n- EventSub subscribe called again (duplicate)\n\nPotential solutions:\n\n1. Distributed lock for session lifecycle\n- Acquire lock before EnsureSessionActive or DeleteSession\n- Redis SETNX with timeout\n- Prevents concurrent activation/cleanup\n- Adds latency\n\n2. Idempotent ref counting with SET instead of INCR\n- Store instance_id:timestamp in set\n- SADD on connect, SREM on disconnect\n- Count = SCARD\n- Requires instance ID, clock sync\n\n3. Lease-based ref counting\n- Each ref count has TTL (e.g., 5 minutes)\n- Instances renew lease periodically\n- Crash = lease expires, count decrements automatically\n- Complex: requires heartbeat goroutine\n\n4. Accept eventual consistency\n- Current design works \"most of the time\"\n- Rare races self-heal via cleanup\n- Ref count leak is worst case (fixed by Redis restart)\n\n5. Circuit breaker for ref count anomalies\n- Detect negative ref counts → log warning, reset to 0\n- Detect very high ref counts → investigate\n- Self-healing heuristics\n\nRecommendation:\n1. Add defensive checks (option 5)\n   - Log warning if ref count goes negative\n   - Reset to 0 if negative detected\n   - Add observability metric: ref_count_anomalies_total\n\n2. Document known races, acceptable trade-offs\n   - Ref count is best-effort tracking\n   - 30s grace period absorbs most races\n   - Memory leak is bounded (cleanup on restart)\n\n3. Defer distributed locks (option 1) unless leaks observed\n\nPriority: P2 - current implementation is pragmatic but fragile\n","notes":"Comment from Maintainability: Brilliant deep dive on race conditions. The ref counting is indeed best-effort. Your option 5 (defensive checks + observability) is pragmatic - aligns with the codebase philosophy of eventual consistency. Ref count leak is the main concern but as you note cleanup-on-restart bounds it. Suggest adding ref_count_distribution metric to detect anomalies (e.g., sessions stuck at high counts). Distributed locks (option 1) are overkill given 30s grace period. The cleanup race you identified (CASE 2 in twitch-tow-h1k) could be fixed with simple ref count check in DeleteSession before deletion. Vote: +1 for option 5.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:08:46.031707+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:42:02.5504+01:00","closed_at":"2026-02-12T17:42:02.5504+01:00","close_reason":"Superseded by implementation epic twitch-tow-pye (Defensive Ref Counting). Epic provides self-healing for negative counts, anomaly detection metrics, debug endpoint, background health check. Accepts eventual consistency as design philosophy."}
{"id":"twitch-tow-23o","title":"Idea: Constants scattered across packages","description":"Configuration constants are scattered across multiple packages without centralization:\n\n**Scattered constants:**\n\nBroadcast package:\n- maxClientsPerSession = 50\n- tickInterval = 50ms\n- redisTimeout = 2s\n- commandTimeout = 5s\n\nWriter package:\n- writeDeadline = 5s\n- pingInterval = 30s\n- pongDeadline = 60s\n- messageBufferSize = 16\n\nApp package:\n- orphanMaxAge = 30s\n- cleanupInterval = 30s\n- cleanupScanTimeout = 30s\n\nServer package:\n- sessionMaxAgeDays = 7\n\nTwitch package:\n- appTokenTimeout = 15s\n\n**Concerns:**\n1. **Hard to tune**: Need to grep codebase to find all timeout values\n2. **Inconsistent units**: Some use seconds, others milliseconds, others days\n3. **No environment override**: All hardcoded, can't change without recompile\n4. **Testing friction**: Can't speed up timeouts for tests (clockwork helps but not for all)\n5. **Production tuning**: Can't adjust client caps or timeouts based on load\n\n**Options:**\nA. **Central config**: Add to Config struct with env vars (BROADCASTER_TICK_MS, MAX_CLIENTS_PER_SESSION)\nB. **Constants package**: internal/constants/timeouts.go, limits.go\nC. **Keep local**: Argue constants belong near their usage\nD. **Partial centralization**: Only expose production-tunable values (client caps, timeouts)\n\n**Trade-off**: More config = more complexity. Current approach keeps code simple but inflexible.\n\n**Recommendation**: Document why these are hardcoded. If production tuning becomes needed, promote to config.","notes":"RESOLVED: Merged into Epic 7 (twitch-tow-749) - Configuration Tuning. Externalizes 5 production-tunable constants (maxClients, tickInterval, cleanupInterval, orphanMaxAge, sessionMaxAge). Documents hardcoded constants rationale.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:28.784631+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:39:36.058422+01:00","closed_at":"2026-02-12T17:39:36.058427+01:00"}
{"id":"twitch-tow-2cq","title":"Discussion: Redis key distribution and memory efficiency","description":"Redis key schema uses multiple key types with different access patterns.\n\nCurrent key schema:\n1. session:{uuid} - HASH (5 fields)\n2. ref_count:{uuid} - STRING (integer)\n3. broadcaster:{twitchUserID} - STRING (uuid mapping)\n4. debounce:{uuid}:{twitchUserID} - STRING with 1s TTL\n\nMemory per active session: ~400-700 bytes + debounce overhead\nFor 10K sessions: under 10 MB total (excellent!)\n\nIssues:\n1. Config JSON duplicated across sessions (could normalize)\n2. Debounce key explosion on high-engagement streams\n3. No TTL on session keys (relies on orphan cleanup)\n\nRecommendations:\n1. Current memory usage is excellent\n2. Add defensive TTL to session keys (24h)\n3. Monitor debounce key creation rate\n","notes":"CONVERTED TO EPIC: twitch-tow-hpr (Epic: Redis Memory Monitoring and Defensive TTLs). Implements 24h defensive TTLs on session keys, Redis memory metrics (5 metrics), memory breakdown debug endpoint, and comprehensive capacity planning documentation. Prevents indefinite memory leaks. 2 developer-days effort.","status":"closed","priority":3,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:59.285342+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:51:01.655445+01:00","closed_at":"2026-02-12T17:51:01.655448+01:00"}
{"id":"twitch-tow-2m8","title":"Discussion: Load balancer configuration and WebSocket scaling","description":"Horizontal scaling requires load balancer configuration but the application has WebSocket sticky session requirements.\n\nArchitecture for multi-instance deployment:\n```\nInternet → Load Balancer → [Instance 1, Instance 2, ..., Instance N]\n                ↓\n              Redis (shared state)\n                ↓  \n           PostgreSQL (user data)\n```\n\nWebSocket requirements:\n1. Long-lived connections (minutes to hours)\n2. No state in HTTP layer (all state in Redis)\n3. Bidirectional communication (broadcast from server)\n4. No sticky session needed for REST endpoints\n5. No sticky session needed for WebSockets (state in Redis)\n\nLoad balancer considerations:\n\nREST endpoints (stateless):\n✓ GET /auth/login - stateless\n✓ GET /auth/callback - uses session cookie but single request\n✓ POST /auth/logout - uses session cookie but single request  \n✓ GET /dashboard - uses session cookie\n✓ POST /dashboard/config - uses session cookie\n✓ POST /api/reset/:uuid - requires auth (session)\n✓ POST /api/rotate-overlay-uuid - requires auth\n\nWebSocket endpoints (long-lived):\n✓ GET /overlay/:uuid - single HTTP request, renders page\n✓ GET /ws/overlay/:uuid - upgrades to WebSocket, persistent\n\nSession storage:\n- gorilla/sessions with cookie store\n- Session data stored client-side in signed cookie\n- No server-side session storage\n- No sticky session required for auth\n\nWebSocket state:\n- All sentiment state in Redis\n- Broadcaster holds only local client connections\n- No persistent state in memory that must be preserved\n- Client reconnects work on any instance\n\nScaling implications:\n\n✓ Pure horizontal scaling works\n✓ No sticky sessions needed\n✓ Round-robin load balancing is fine\n✓ WebSocket connections can land on any instance\n✓ Client reconnection after instance failure works\n\nLoad balancer configuration (nginx example):\n```\nupstream backend {\n    least_conn;  # or round_robin\n    server instance1:8080;\n    server instance2:8080;\n}\n\nserver {\n    location /ws/ {\n        proxy_pass http://backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_read_timeout 3600s;  # long timeout for WS\n    }\n    location / {\n        proxy_pass http://backend;\n    }\n}\n```\n\nPotential issues:\n\n1. No health check endpoint\n- Load balancer can't detect unhealthy instances\n- Dead instances remain in pool\n- Connections fail, clients must retry\n\n2. No graceful connection draining\n- Instance shutdown → active WebSockets dropped immediately\n- No notification to load balancer\n- Clients see abrupt disconnect\n\n3. No instance discovery\n- Manual load balancer configuration\n- Must update config when scaling\n- No auto-scaling support\n\n4. Cookie-based sessions vulnerable to replay\n- Signed cookies prevent tampering\n- But cookies can be replayed across instances\n- No server-side invalidation on logout\n- Fixed via SESSION_SECRET rotation\n\nRecommendations:\n\n1. Add /health endpoint (option A)\n```go\nGET /health → 200 OK if:\n- Redis connection OK (PING)\n- DB connection OK (SELECT 1)\n- App not shutting down\n```\n\n2. Implement graceful shutdown (option B)\n- Stop accepting new WebSocket connections\n- Wait for existing connections to drain (timeout 30s)\n- Or send close frame to all clients\n- Then exit\n\n3. Document load balancer requirements\n- WebSocket proxy settings\n- Timeout configuration\n- Health check endpoint\n\n4. Consider Kubernetes readiness/liveness probes\n- Readiness: /health (stop receiving traffic if unhealthy)\n- Liveness: process alive (restart if dead)\n\nPriority: P2 - works without sticky sessions (good!), needs health checks\n","notes":"CONVERTED TO EPIC: twitch-tow-0h2 (Epic: Load Balancer Configuration Guide and Health Check Implementation). This epic provides comprehensive runbooks for Nginx, AWS ALB, and Kubernetes deployments. Includes WebSocket proxy configuration, health check integration, graceful shutdown best practices, and troubleshooting guides. Complements the health check endpoints already implemented in twitch-tow-682.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:09:12.252182+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:42:23.242563+01:00","closed_at":"2026-02-12T17:42:23.242565+01:00"}
{"id":"twitch-tow-2pg","title":"Discussion: Redis SCAN operation in orphan cleanup","description":"The ListOrphans() method in SessionRepo uses Redis SCAN with pattern 'session:*' and count=100. SCAN is cursor-based and safe for production, but scanning the entire keyspace every 30s could become expensive at scale.\n\nCurrent implementation:\n- Runs every 30 seconds (cleanupInterval)\n- Uses SCAN with count=100 (not a hard limit, just a hint)\n- Has 30s timeout (cleanupScanTimeout)\n- Checks context cancellation between iterations\n\n**Scalability concerns:**\n1. SCAN is O(N) where N = total keys in Redis\n2. At 10,000 active sessions, each cleanup scans 10K keys\n3. Pattern matching 'session:*' happens on every key\n4. The count=100 hint may result in many iterations\n\n**Impact:**\n- Memory: SCAN is cursor-based, low memory footprint ✓\n- CPU: Pattern matching on every key in keyspace\n- Latency: Could slow down with large keyspace (millions of keys)\n- Blocking: SCAN doesn't block other operations ✓\n\n**Potential solutions:**\n1. Use Redis Sorted Set with disconnect timestamps for O(log N) range queries\n2. Increase SCAN count hint to reduce roundtrips (e.g., 1000)\n3. Add Lua script to filter during scan (server-side filtering)\n4. Implement incremental cleanup (scan subset of keyspace per tick)\n5. Consider separate Redis database for session keys only\n\n**Trade-offs to consider:**\n- Sorted set adds write overhead on every disconnect\n- Higher count values increase per-iteration latency\n- Lua filtering still iterates all keys, just reduces network transfer\n- Multiple Redis DBs adds operational complexity\n\nPriority: P2 (medium) - becomes critical above 100K keys","notes":"CONVERTED TO EPIC: twitch-tow-8z6 (Epic: Optimize Orphan Cleanup with Redis Sorted Set). Replaces O(N) SCAN with O(log N) ZRANGEBYSCORE for 10-100x performance improvement. Includes migration script, benchmarks, and comprehensive testing. 4 developer-days effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:03:33.595189+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:47:38.379965+01:00","closed_at":"2026-02-12T17:47:38.379969+01:00"}
{"id":"twitch-tow-2r6","title":"Discussion: PostgreSQL connection pool configuration","description":"The database connection pool is created using pgxpool.ParseConfig() with no explicit pool size configuration. This relies on pgxpool defaults which may not be optimal for production.\n\n**Current state:**\n- Uses pgxpool.NewWithConfig() with parsed URL\n- No MaxConns, MinConns, MaxConnLifetime configured\n- No explicit pool size tunables exposed via config\n\n**pgxpool defaults (as of pgx v5):**\n- MaxConns: max(4, num_cpus)\n- MinConns: 0 (lazy connection creation)\n- MaxConnLifetime: 1 hour\n- MaxConnIdleTime: 30 minutes\n\n**Scalability concerns:**\n1. Default MaxConns may be too small for high-traffic instances\n2. All DB queries are blocking (no read replicas for read scaling)\n3. Database is only used for:\n   - User/config reads (session activation)\n   - Token updates (OAuth refresh)\n   - EventSub subscription CRUD\n4. Hot path (vote processing) doesn't touch DB at all ✓\n\n**Read patterns:**\n- High frequency: GetUserByOverlayUUID (every WS connection)\n- Low frequency: GetConfig, UpsertUser, EventSub CRUD\n- No session activation = cold start penalty\n\n**Potential improvements:**\n1. Make pool size configurable (MAX_DB_CONNECTIONS env var)\n2. Add connection pool metrics (idle/active/waiting)\n3. Consider read replicas for GetUserByOverlayUUID if DB becomes bottleneck\n4. Add DB query latency metrics\n5. Implement prepared statement caching (pgx does this by default)\n\n**Priority assessment:**\n- Current architecture keeps DB off hot path ✓\n- Becomes critical only if session activation rate is very high\n- Pool exhaustion would cause WS connection failures\n\nPriority: P2 (medium) - good current design, room for optimization","notes":"CONVERTED TO EPIC: twitch-tow-abv (Epic: PostgreSQL Connection Pool Configuration and Monitoring). Implements configurable pool sizing, comprehensive pool metrics (6 metrics), query tracing, pool exhaustion detection in health checks, and sizing guidelines. 3 developer-days effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:03:49.071708+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:49:09.06734+01:00","closed_at":"2026-02-12T17:49:09.067343+01:00"}
{"id":"twitch-tow-3bx","title":"Solution: Phased observability implementation","description":"## Proposal: Three-Phase Observability Rollout\n\nThis addresses consensus issues:\n- twitch-tow-6hx (Scalability)\n- twitch-tow-c8q (Resilience)\n- twitch-tow-bdb (Resilience - Prometheus idea)\n\n## Phase 1: Health Checks (Week 1) - MUST HAVE\n\n**Endpoints:**\n```\nGET /health         → 200 OK (always, liveness probe)\nGET /ready          → 200 OK if DB pingable + Redis pingable + migrations complete\nGET /version        → JSON with git SHA, build time\n```\n\n**Implementation:**\n```go\nfunc (s *Server) handleHealth(c echo.Context) error {\n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n\nfunc (s *Server) handleReady(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 2*time.Second)\n    defer cancel()\n    \n    // Check DB\n    if err := s.db.Ping(ctx); err != nil {\n        return c.JSON(503, map[string]string{\"status\": \"not ready\", \"reason\": \"database\"})\n    }\n    \n    // Check Redis\n    if err := s.redis.Ping(ctx).Err(); err != nil {\n        return c.JSON(503, map[string]string{\"status\": \"not ready\", \"reason\": \"redis\"})\n    }\n    \n    return c.JSON(200, map[string]string{\"status\": \"ready\"})\n}\n```\n\n**Why P0:** Load balancers need /ready for traffic routing\n\n## Phase 2: Core Metrics (Week 2-3) - HIGH PRIORITY\n\n**Metrics to instrument:**\n1. **HTTP**: request_duration_seconds{method, path, status}\n2. **Redis**: operations_total{operation, status}, operation_duration_seconds\n3. **Broadcaster**: active_sessions, tick_duration_seconds\n4. **WebSocket**: connections_total, connection_duration_seconds\n5. **Votes**: processed_total{result=applied|debounced|no_match}\n6. **Go runtime**: goroutines, memory, GC pauses (auto via prometheus/client_golang)\n\n**Library:** github.com/prometheus/client_golang v1.19+\n\n**Endpoint:** GET /metrics (Prometheus scrape format)\n\n**Example instrumentation:**\n```go\nvar (\n    redisOps = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"redis_operations_total\",\n            Help: \"Total Redis operations\",\n        },\n        []string{\"operation\", \"status\"},\n    )\n    redisLatency = prometheus.NewHistogramVec(\n        prometheus.HistogramOpts{\n            Name: \"redis_operation_duration_seconds\",\n            Help: \"Redis operation latency\",\n            Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5, 1, 2.5},\n        },\n        []string{\"operation\"},\n    )\n)\n```\n\n## Phase 3: Dashboards + Alerts (Week 4+) - NICE TO HAVE\n\n**Grafana dashboards:**\n1. Overview: Health, active sessions, vote rate, error rate\n2. Performance: Latency percentiles (p50/p95/p99), throughput\n3. Resources: Memory, goroutines, connection pools\n\n**Alerts:**\n- Redis error rate \u003e 1% for 5 min\n- Broadcaster tick \u003e 100ms (p99) for 5 min\n- WebSocket disconnects \u003e 10/sec\n- Orphan cleanup failures \u003e 3 consecutive\n\n## Trade-offs\n\n**Pros:**\n- Phased reduces risk (incremental value)\n- Health checks unblock K8s/ALB integration\n- Metrics enable capacity planning\n- Low overhead (\u003c1% CPU for metrics)\n\n**Cons:**\n- Adds dependency (prometheus/client_golang)\n- Requires Prometheus deployment\n- More code surface area to maintain\n\n## Decision Criteria\n- Phase 1: No dependencies, minimal code → APPROVE\n- Phase 2: One dependency, moderate instrumentation → VOTE NEEDED\n- Phase 3: Requires Grafana deployment → DEFER\n\n## Vote\n- Architect-Resilience: +1 (all phases)\n- Architect-Scalability: ?\n- Architect-Maintainability: ?\n\n## Files to Modify\n- internal/server/handlers.go (health endpoints)\n- internal/metrics/metrics.go (new package)\n- internal/redis/client.go (instrument operations)\n- internal/broadcast/broadcaster.go (instrument tick)","notes":"Vote: +1 from Maintainability architect. /version endpoint (Git SHA, build timestamp) is valuable addition for debugging. 3-phase structure aligns well with other proposals. Recommend merge with eyl + 1n1 into single consolidated observability plan. All three proposals are 95% aligned.\nVote: +1 from Maintainability Architect\n\nRATIONALE: Phased approach is excellent. Phase 1 (health checks) is minimal and essential. Phase 2 (Prometheus metrics) aligns with my ADR-016. Phase 3 (dashboards) is operational tooling.\n\nANSWERS TO QUESTIONS:\n- Q1 (/version endpoint): YES - Include git commit hash + build timestamp for deployment verification\n- Q2 (critical metrics): Add chatpulse_errors_total{type} and chatpulse_session_activation_duration_seconds\n\nINTEGRATION: Merge with my Epic 5 ADR-016. Your proposal = implementation plan, my ADR = rationale/alternatives/consequences.\n\nADDITIONAL SUGGESTIONS:\n- Document metric naming conventions in ERROR_HANDLING.md (align with structured logging)\n- Document SLOs in ADR-016 (99.5% uptime, P95 vote \u003c500ms, P95 broadcast \u003c100ms)","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:11:46.800712+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.199454+01:00","closed_at":"2026-02-12T17:56:57.199454+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-3dd","title":"EPIC: Write Tier 1 Foundation ADRs (3 ADRs)","description":"Write the 3 most critical ADRs that explain the core architectural decisions: Redis-only architecture, pull-based broadcaster, and EventSub webhooks + conduits. These are foundation decisions that all other architecture depends on.\n\n## User Story\nAs a developer joining the project, I need to understand why the core architectural decisions were made so I don't inadvertently break them or propose changes that re-open settled debates.\n\n## Value Proposition\n- Prevents cargo-cult refactoring (\"why don't we use pub/sub?\")\n- Documents rejected alternatives (\"we already tried sticky sessions\")\n- Captures trade-offs explicitly (latency vs simplicity)\n- Enables informed decision-making for future changes\n\n## Tasks\n\n### 1. Create ADR directory structure\n- Create `docs/adr/` directory in repo root\n- Create `docs/adr/README.md` index page with table of contents\n- Add frontmatter explaining ADR purpose and template\n\n**Files to create:**\n- `docs/adr/README.md`\n\n**Time estimate:** 15 minutes\n\n### 2. Write ADR-001: Redis-only architecture for session state\n\n**Context:**\n- Need shared state across multiple horizontally-scaled instances\n- Traditional sticky sessions don't meet multi-tenant scaling requirements\n- Database too slow for 50ms tick loop reads\n\n**Decision:**\n- All session state lives in Redis (zero in-memory state per instance)\n- Redis is source of truth for sessions + sentiment + ref counts\n- PostgreSQL only for durable config + user data\n- Instances are completely stateless - any instance can serve any session\n\n**Alternatives considered:**\n1. **Sticky sessions** - Load balancer pins user to instance\n   - Rejected: Doesn't solve cleanup when instance crashes\n   - Rejected: Uneven load distribution\n   - Rejected: Complex routing rules\n\n2. **In-memory with sync** - Each instance keeps local state, syncs via Redis pub/sub\n   - Rejected: Complex consistency model\n   - Rejected: Memory usage grows with total sessions, not just active\n   - Rejected: Warm-up time after restarts\n\n3. **Distributed cache (Hazelcast, etc.)** - JVM-style distributed data structures\n   - Rejected: Go ecosystem not mature for this\n   - Rejected: More moving parts than Redis\n   - Rejected: Overkill for current scale\n\n**Consequences:**\n\n✅ **Positive:**\n- Enables true stateless instances (any instance can serve any request)\n- Horizontal scaling is trivial (add more instances)\n- Instance crashes don't lose session state\n- Simple deployment (no warm-up period)\n- Redis expertise is common\n\n❌ **Negative:**\n- 2-5ms latency per Redis operation (network roundtrip)\n- Requires Redis HA setup in production\n- Redis becomes single point of failure\n- More complex than in-memory for single-instance deploys\n\n🔄 **Trade-offs:**\n- Chose availability + scalability over latency\n- Accept 50ms broadcast staleness (pull-based) vs real-time (pub/sub)\n- Eventual consistency for ref counting (30s cleanup window)\n\n**Related decisions:**\n- ADR-002: Pull-based broadcaster (consequence: N Redis calls per tick)\n- ADR-005: No sticky sessions\n- ADR-006: Ref counting strategy\n- ADR-007: Database vs Redis data separation\n\n**Files to create:**\n- `docs/adr/001-redis-only-architecture.md`\n\n**Time estimate:** 60 minutes (research, write, review)\n\n### 3. Write ADR-002: Pull-based broadcaster vs Redis pub/sub\n\n**Context:**\n- Need to broadcast sentiment updates to WebSocket clients every 50ms\n- Updates must reflect current time-decayed value (not stale)\n- Multiple instances may serve same session (no sticky sessions)\n\n**Decision:**\n- Broadcaster uses 50ms ticker to **pull** current values from Redis\n- Single `FCALL_RO get_decayed_value` per session per tick\n- Fan-out to local WebSocket clients in-memory (no Redis writes)\n- No Redis pub/sub, no push-based events\n\n**Alternatives considered:**\n1. **Redis pub/sub** - Vote events published, instances subscribe\n   - Rejected: Still need to compute time-decay on read (requires Redis call anyway)\n   - Rejected: Pub/sub doesn't scale well (all instances get all messages)\n   - Rejected: More complex (tick loop + pub/sub handler)\n\n2. **Push-based with in-memory cache** - Cache last value, invalidate on vote\n   - Rejected: Staleness for time-decay (need to recalc every 50ms anyway)\n   - Rejected: Cache invalidation across instances is complex\n   - Rejected: Doesn't eliminate Redis calls (need decay calculation)\n\n3. **Server-Sent Events (SSE)** - HTTP streaming instead of WebSockets\n   - Rejected: WebSocket already working, no strong reason to change\n   - Rejected: Doesn't affect Redis architecture\n\n**Consequences:**\n\n✅ **Positive:**\n- Simple code (single tick loop, no event handlers)\n- Always shows current time-decayed value (never stale from cache)\n- Easy to reason about (no event ordering concerns)\n- No pub/sub scaling issues\n\n❌ **Negative:**\n- N Redis calls per tick (one per active session)\n- 50ms staleness for vote updates (vs instant with pub/sub)\n- Constant Redis traffic even when no votes\n\n🔄 **Trade-offs:**\n- Chose simplicity over real-time updates (50ms acceptable for sentiment bar)\n- Accept higher Redis call frequency for simpler code\n- Prioritize correctness (always fresh decay) over optimization (cache + invalidate)\n\n**Related decisions:**\n- ADR-001: Redis-only architecture (consequence: all reads hit Redis)\n- ADR-011: Actor pattern for broadcaster (consequence: serial tick processing)\n\n**Files to create:**\n- `docs/adr/002-pull-based-broadcaster.md`\n\n**Time estimate:** 60 minutes\n\n### 4. Write ADR-003: Twitch EventSub webhooks + conduits vs WebSocket\n\n**Context:**\n- Need to receive chat messages from Twitch for sentiment analysis\n- Twitch offers two EventSub transports: WebSocket (client connects) or Webhook (server receives POST)\n- Previous architecture used EventSub WebSocket with complex 5-state FSM\n\n**Decision:**\n- Use **webhooks** via **conduit** for EventSub transport\n- Twitch POSTs chat messages to `/webhooks/eventsub` endpoint\n- HMAC signature verification via Kappopher library\n- Single conduit shared across all streamers\n- Requires public HTTPS endpoint (`WEBHOOK_CALLBACK_URL` env var)\n\n**Alternatives considered:**\n1. **EventSub WebSocket** - Long-lived client connection to Twitch\n   - Rejected: 660 lines of complex FSM code (5 states, reconnect logic)\n   - Rejected: Connection management complexity (keepalive, reconnect, failures)\n   - Rejected: Harder to horizontally scale (session affinity for WebSocket)\n   - Rejected: Previous implementation deleted (too complex)\n\n2. **Twitch IRC (chat.twitch.tv)** - Legacy chat protocol\n   - Rejected: Deprecated by Twitch, EventSub is preferred\n   - Rejected: Less reliable, fewer guarantees\n   - Rejected: No official Go SDK\n\n3. **Polling Twitch API** - Fetch recent messages periodically\n   - Rejected: High latency (seconds vs milliseconds)\n   - Rejected: Rate limit concerns\n   - Rejected: Inefficient (most polls return nothing)\n\n**Consequences:**\n\n✅ **Positive:**\n- Simpler deployment (no long-lived connection management)\n- Standard HTTP request handling (familiar patterns)\n- Kappopher handles HMAC verification (less code)\n- Horizontal scaling easier (stateless HTTP endpoints)\n- Conduit allows webhook URL changes without re-subscribing\n\n❌ **Negative:**\n- Requires public HTTPS endpoint (can't run purely localhost)\n- Need to configure `WEBHOOK_CALLBACK_URL` + `WEBHOOK_SECRET`\n- Twitch must be able to reach the app (firewall/NAT considerations)\n\n🔄 **Trade-offs:**\n- Chose operational simplicity over deployment flexibility\n- Accept public endpoint requirement for much simpler code\n- Reduced from 660 lines (WebSocket FSM) to ~150 lines (webhook handler)\n\n**Related decisions:**\n- ADR-004: Single bot account architecture (consequence: one bot reads all channels)\n\n**Files to create:**\n- `docs/adr/003-eventsub-webhooks-conduits.md`\n\n**Time estimate:** 60 minutes\n\n### 5. Create ADR index page\n\n**Content:**\n- Link to all 3 ADRs with one-sentence summaries\n- Explain tier system (Tier 1 = foundation, must read first)\n- Note that more ADRs will be added in future phases\n\n**Files to update:**\n- `docs/adr/README.md` (add table)\n\n**Time estimate:** 15 minutes\n\n### 6. Link ADRs from CLAUDE.md\n\nAdd reference to ADR directory in CLAUDE.md architecture section:\n\n```markdown\n## Architecture Decision Records\n\nSee [`docs/adr/`](docs/adr/) for detailed architectural decision records (ADRs). These documents explain why key decisions were made and what alternatives were considered.\n\n**Start here:** Read the [Tier 1 Foundation ADRs](docs/adr/README.md#tier-1-foundation) first - they explain the core architecture that all other decisions depend on.\n```\n\n**Files to update:**\n- `CLAUDE.md` (add ADR reference after Project Structure section)\n\n**Time estimate:** 10 minutes\n\n### 7. Review and polish\n\n- Read all 3 ADRs end-to-end for consistency\n- Check that template is followed uniformly\n- Verify alternatives section explains rejections clearly\n- Verify consequences capture trade-offs (not just pros/cons)\n- Check markdown formatting renders correctly on GitHub\n\n**Time estimate:** 20 minutes\n\n## Acceptance Criteria\n\n- ✅ All 3 ADRs written and committed\n- ✅ All ADRs follow template consistently (Context/Decision/Alternatives/Consequences)\n- ✅ Alternatives section documents **why** each option was rejected (not just listed)\n- ✅ Consequences section captures trade-offs clearly (✅ positive, ❌ negative, 🔄 trade-offs)\n- ✅ Index page (`README.md`) links to all ADRs\n- ✅ CLAUDE.md links to ADR directory\n- ✅ Markdown renders correctly on GitHub preview\n- ✅ Related decisions cross-referenced with ADR numbers\n\n## Files Changed\n\n**Created:**\n- `docs/adr/README.md` (index)\n- `docs/adr/001-redis-only-architecture.md`\n- `docs/adr/002-pull-based-broadcaster.md`\n- `docs/adr/003-eventsub-webhooks-conduits.md`\n\n**Modified:**\n- `CLAUDE.md` (add ADR reference link)\n\n## Dependencies\n- None (greenfield documentation)\n\n## Effort Estimate\n**Total: 3 hours** (180 minutes)\n- Setup (30 min): Directory structure + index\n- ADR-001 (60 min): Redis-only architecture\n- ADR-002 (60 min): Pull-based broadcaster  \n- ADR-003 (60 min): EventSub webhooks\n- Polish (30 min): Review + link from CLAUDE.md\n\n## Success Metrics\n- Team references ADRs when proposing architectural changes\n- New developers read ADRs during onboarding\n- Prevents re-opening settled debates (\"why not pub/sub?\")\n- Future PRs cite ADR numbers when relevant","status":"closed","priority":1,"issue_type":"epic","assignee":"dev-quality","owner":"patrick.scheid@deepl.com","estimated_minutes":180,"created_at":"2026-02-12T17:25:42.658949+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:04:37.990803+01:00","closed_at":"2026-02-12T18:04:37.990803+01:00","close_reason":"Implemented all Tier 1 Foundation ADRs. Created docs/adr/ directory with README.md index, wrote ADR-001 (Redis-only architecture), ADR-002 (Pull-based broadcaster), and ADR-003 (EventSub webhooks + conduits). All ADRs follow consistent template with Context/Decision/Alternatives/Consequences sections. Added reference link in CLAUDE.md. All acceptance criteria met. Committed in e82cc42."}
{"id":"twitch-tow-3p9","title":"Discussion: Redis Lua script testing gap","description":"The Lua scripts (chatpulse.lua) implement core business logic (time-decay calculation, vote clamping) but have limited test coverage:\n\n**Current state:**\n- Lua scripts: 30 lines of critical logic (exponential decay, clamping)\n- Integration tests cover the Go wrapper (SentimentStore) but not Lua logic in isolation\n- No unit tests for Lua functions themselves\n\n**Risks:**\n1. **Math errors hidden**: Exponential decay formula () could have bugs that only surface under specific timing conditions\n2. **Clamping edge cases**: [-100, 100] boundary logic not explicitly tested\n3. **Refactoring risk**: Changing Lua code requires running full integration tests to verify\n\n**Current testing approach:**\n-  exercises Lua via Redis, but tests Go interface, not Lua directly\n- Good coverage of happy path, but edge cases (negative time deltas, extreme decay rates) may not be covered\n\n**Options:**\nA. **Keep current** - argue that integration tests provide sufficient coverage\nB. **Add Lua unit tests** - use Redis test framework or lua testing library\nC. **Extract to Go** - move decay logic to Go, sacrifice atomic guarantees for testability\nD. **Document edge cases** - add comprehensive comments in Lua explaining behavior\n\n**Recommendation**: At minimum, add comprehensive comments documenting the decay formula and clamping behavior. Consider option B if decay logic gets more complex.","notes":"RESOLVED: Documentation-only recommendation. Lua scripts are adequately covered by integration tests (internal/redis/*_integration_test.go). Math logic is simple (exponential decay, clamping). Adding comprehensive inline comments to chatpulse.lua explaining decay formula and edge cases is sufficient. If logic becomes complex, revisit with Lua unit testing framework. No epic needed - current coverage adequate for 30 lines of Lua.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:13.032993+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:50:15.589284+01:00","closed_at":"2026-02-12T17:50:15.589287+01:00"}
{"id":"twitch-tow-3ua","title":"EPIC: Redis/PostgreSQL Consistency Monitoring and Reconciliation","description":"Add consistency monitoring between Redis (session cache) and PostgreSQL (source of truth) to detect and alert on data drift.\n\n## User Story\nAs an operator, I need visibility into Redis/PostgreSQL data consistency so I can detect config drift and prevent users from seeing stale configuration.\n\n## Value Proposition\n- Detect config drift (stale Redis data) before users report issues\n- Alert on Redis/DB inconsistencies (drift_detected metric)\n- Document consistency model and failure modes\n- Graceful degradation when Redis unavailable (reject SaveConfig vs serve stale)\n\n## Background\n\n**Current issue (twitch-tow-wdy):**\n- SaveConfig writes to PostgreSQL (source of truth)\n- Best-effort UpdateConfig writes to Redis (cache)\n- If UpdateConfig fails → error logged but **ignored** → Redis has stale config\n- No reconciliation job to detect drift\n\n**Failure scenarios:**\n1. SaveConfig succeeds, UpdateConfig fails → stale Redis config\n2. Redis flush → all sessions reset, no DB record\n3. Manual DB updates → no cache invalidation\n\n## Tasks\n\n### 1. Make UpdateConfig failures non-silent\n\n**Current code (app/service.go:167-169):**\n```go\nif err := s.sessions.UpdateConfig(ctx, user.OverlayUUID, \u0026configSnapshot); err != nil {\n    s.logger.Error(\"failed to update session config\", \"error\", err)\n    // ERROR IS IGNORED - SaveConfig returns nil\\!\n}\nreturn nil\n```\n\n**Problem:** SaveConfig succeeds even if Redis update fails → user sees success but config is stale\n\n**Solution: Return error to caller**\n```go\nif err := s.sessions.UpdateConfig(ctx, user.OverlayUUID, \u0026configSnapshot); err != nil {\n    s.logger.Error(\"failed to update session config in Redis\",\n        \"user_id\", userID,\n        \"overlay_uuid\", user.OverlayUUID,\n        \"error\", err)\n    \n    // Return error to handler - user should see error, retry\n    return fmt.Errorf(\"config saved to database but failed to update active session: %w\", err)\n}\nreturn nil\n```\n\n**Impact:**\n- ✅ User sees error if Redis update fails (can retry)\n- ✅ Prevents silent drift\n- ❌ Degrades UX if Redis is temporarily unavailable\n\n**Alternative:** Circuit breaker pattern\n- If Redis unavailable, reject SaveConfig immediately (fail-fast)\n- Better UX than \"saved but not applied\"\n\n**Files to modify:**\n- `internal/app/service.go` (SaveConfig method)\n\n**Time estimate:** 30 minutes\n\n---\n\n### 2. Add config version tracking\n\n**Add version field to detect drift:**\n\n```go\n// internal/domain/config.go\ntype Config struct {\n    UserID           uuid.UUID `db:\"user_id\"`\n    ForTrigger       string    `db:\"for_trigger\"`\n    AgainstTrigger   string    `db:\"against_trigger\"`\n    // ... other fields\n    \n    Version          int       `db:\"version\"` // NEW: Incremented on each update\n    UpdatedAt        time.Time `db:\"updated_at\"`\n}\n```\n\n**Migration:**\n```sql\n-- internal/database/sqlc/schemas/003_add_config_version.sql\nALTER TABLE configs ADD COLUMN version INTEGER NOT NULL DEFAULT 1;\nALTER TABLE configs ADD COLUMN updated_at TIMESTAMP NOT NULL DEFAULT NOW();\n\n-- Trigger to auto-increment version\nCREATE OR REPLACE FUNCTION increment_config_version()\nRETURNS TRIGGER AS 30692\nBEGIN\n    NEW.version = OLD.version + 1;\n    NEW.updated_at = NOW();\n    RETURN NEW;\nEND;\n30692 LANGUAGE plpgsql;\n\nCREATE TRIGGER configs_version_trigger\n    BEFORE UPDATE ON configs\n    FOR EACH ROW\n    EXECUTE FUNCTION increment_config_version();\n```\n\n**Store version in Redis:**\n```go\n// internal/redis/session_repository.go\nfunc (r *SessionRepo) ActivateSession(...) error {\n    configJSON, _ := json.Marshal(config)\n    \n    r.rdb.HSet(ctx, sessionKey,\n        \"config_json\", configJSON,\n        \"config_version\", config.Version, // NEW: Store version\n        // ... other fields\n    )\n}\n```\n\n**Check version on reads:**\n```go\nfunc (r *SessionRepo) GetSessionConfig(ctx context.Context, sessionUUID uuid.UUID) (*domain.ConfigSnapshot, error) {\n    sessionKey := fmt.Sprintf(\"session:%s\", sessionUUID)\n    \n    configJSON := r.rdb.HGet(ctx, sessionKey, \"config_json\").Val()\n    versionStr := r.rdb.HGet(ctx, sessionKey, \"config_version\").Val()\n    redisVersion, _ := strconv.Atoi(versionStr)\n    \n    var config domain.ConfigSnapshot\n    json.Unmarshal([]byte(configJSON), \u0026config)\n    \n    // Version mismatch detection\n    if config.Version != redisVersion {\n        r.logger.Warn(\"config version mismatch\",\n            \"session_uuid\", sessionUUID,\n            \"redis_version\", redisVersion,\n            \"config_version\", config.Version)\n    }\n    \n    return \u0026config, nil\n}\n```\n\n**Files to modify:**\n- `internal/domain/config.go` (add Version + UpdatedAt fields)\n- `internal/database/sqlc/schemas/003_add_config_version.sql` (new migration)\n- `internal/database/sqlc/queries/config.sql` (regenerate sqlc)\n- `internal/redis/session_repository.go` (store + check version)\n\n**Time estimate:** 2 hours (migration + code changes + sqlc regen)\n\n---\n\n### 3. Add reconciliation background job\n\n**Periodic consistency check:**\n\n```go\n// internal/app/reconciler.go (new file)\n\ntype ConfigReconciler struct {\n    configRepo   domain.ConfigRepository\n    sessionRepo  domain.SessionRepository\n    rdb          *redis.Client\n    interval     time.Duration\n    logger       *slog.Logger\n}\n\nfunc NewConfigReconciler(configRepo domain.ConfigRepository, sessionRepo domain.SessionRepository, rdb *redis.Client, logger *slog.Logger) *ConfigReconciler {\n    return \u0026ConfigReconciler{\n        configRepo:  configRepo,\n        sessionRepo: sessionRepo,\n        rdb:         rdb,\n        interval:    5 * time.Minute, // Check every 5 minutes\n        logger:      logger,\n    }\n}\n\nfunc (r *ConfigReconciler) Start(ctx context.Context) {\n    ticker := time.NewTicker(r.interval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ticker.C:\n            if err := r.reconcile(ctx); err != nil {\n                r.logger.Error(\"config reconciliation failed\", \"error\", err)\n            }\n        case \u003c-ctx.Done():\n            return\n        }\n    }\n}\n\nfunc (r *ConfigReconciler) reconcile(ctx context.Context) error {\n    // Get all active sessions from Redis\n    sessions, err := r.sessionRepo.ListActiveSessions(ctx)\n    if err != nil {\n        return fmt.Errorf(\"failed to list active sessions: %w\", err)\n    }\n    \n    for _, session := range sessions {\n        // Compare Redis config version vs PostgreSQL version\n        redisConfig, err := r.sessionRepo.GetSessionConfig(ctx, session.OverlayUUID)\n        if err != nil {\n            r.logger.Warn(\"failed to get Redis config\", \"session_uuid\", session.OverlayUUID, \"error\", err)\n            continue\n        }\n        \n        dbConfig, err := r.configRepo.GetByUserID(ctx, session.UserID)\n        if err != nil {\n            r.logger.Warn(\"failed to get DB config\", \"user_id\", session.UserID, \"error\", err)\n            continue\n        }\n        \n        if redisConfig.Version != dbConfig.Version {\n            r.logger.Warn(\"config drift detected\",\n                \"user_id\", session.UserID,\n                \"session_uuid\", session.OverlayUUID,\n                \"redis_version\", redisConfig.Version,\n                \"db_version\", dbConfig.Version)\n            \n            // Metric for alerting\n            metrics.ConfigDriftDetected.WithLabelValues(session.UserID.String()).Inc()\n            \n            // Auto-fix: Update Redis with latest DB config\n            configSnapshot := domain.ToConfigSnapshot(dbConfig)\n            if err := r.sessionRepo.UpdateConfig(ctx, session.OverlayUUID, configSnapshot); err != nil {\n                r.logger.Error(\"failed to fix config drift\", \"error\", err)\n            } else {\n                r.logger.Info(\"config drift auto-fixed\",\n                    \"session_uuid\", session.OverlayUUID,\n                    \"new_version\", dbConfig.Version)\n            }\n        }\n    }\n    \n    return nil\n}\n```\n\n**Wire into app:**\n```go\n// cmd/server/main.go\n\nreconciler := app.NewConfigReconciler(configRepo, sessionRepo, rdb, logger)\ngo reconciler.Start(ctx)\n```\n\n**Files to create:**\n- `internal/app/reconciler.go` (new file)\n\n**Files to modify:**\n- `internal/domain/session.go` (add ListActiveSessions method to SessionRepository)\n- `internal/redis/session_repository.go` (implement ListActiveSessions)\n- `cmd/server/main.go` (start reconciler)\n\n**Time estimate:** 2 hours\n\n---\n\n### 4. Add consistency metrics\n\n**Track drift events:**\n\n```go\n// internal/app/metrics.go\n\nvar (\n    configDriftDetected = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"chatpulse_config_drift_detected_total\",\n            Help: \"Total config drift events detected (Redis vs PostgreSQL)\",\n        },\n        []string{\"user_id\"},\n    )\n    \n    configDriftFixed = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"chatpulse_config_drift_fixed_total\",\n            Help: \"Total config drift events auto-fixed\",\n        },\n        []string{\"user_id\"},\n    )\n    \n    redisUpdateFailures = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"chatpulse_redis_update_failures_total\",\n            Help: \"Total Redis update failures (SaveConfig)\",\n        },\n        []string{\"reason\"}, // \"timeout\", \"connection_error\", etc.\n    )\n)\n```\n\n**Alert rule:**\n```yaml\n# Prometheus alert\n- alert: ConfigDriftDetected\n  expr: increase(chatpulse_config_drift_detected_total[5m]) \u003e 0\n  annotations:\n    summary: \"Config drift detected between Redis and PostgreSQL\"\n    description: \"User {{ .user_id }} has stale config in Redis\"\n```\n\n**Files to modify:**\n- `internal/app/metrics.go` (add drift metrics)\n- `internal/app/reconciler.go` (increment metrics)\n- `internal/app/service.go` (track Redis update failures)\n\n**Time estimate:** 1 hour\n\n---\n\n### 5. Document consistency model\n\n**Add to CLAUDE.md:**\n\n```markdown\n## Data Consistency Model\n\n### PostgreSQL = Source of Truth\n- User config stored in `configs` table\n- Config version incremented on every update (trigger)\n- Authoritative for all config reads\n\n### Redis = Session Cache\n- Active session state cached for performance\n- Config snapshot stored in `session:{uuid}` hash\n- Config version stored alongside (`config_version` field)\n\n### Consistency Guarantees\n\n**Strong consistency:** NOT provided\n- Redis update can fail after PostgreSQL succeeds\n- Eventual consistency model (5-minute reconciliation)\n\n**Eventual consistency:** Provided\n- Background reconciler detects drift every 5 minutes\n- Auto-fixes stale Redis config\n- Alert fires on drift detection\n\n**Failure modes:**\n\n1. **SaveConfig + Redis failure**\n   - PostgreSQL: Updated ✅\n   - Redis: Stale ❌\n   - User sees error → can retry\n   - Reconciler fixes within 5 minutes\n\n2. **Redis flush**\n   - All sessions reset\n   - Next activation fetches from PostgreSQL\n   - Reconciler not needed (sessions recreated fresh)\n\n3. **Manual DB update**\n   - PostgreSQL: Updated ✅\n   - Redis: Stale ❌ (no cache invalidation)\n   - Reconciler detects + fixes within 5 minutes\n\n### Operations\n\n**Check for drift manually:**\n```bash\n# Query Prometheus\nchatpulse_config_drift_detected_total\n```\n\n**Force reconciliation:**\n```bash\n# Restart reconciler (kills + recreates)\nkubectl rollout restart deployment chatpulse\n```\n\n**Manual cache invalidation:**\n```bash\n# Delete session (forces re-activation with fresh config)\nredis-cli DEL \"session:{overlay_uuid}\"\n```\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Data Consistency Model section)\n\n**Time estimate:** 30 minutes\n\n---\n\n## Acceptance Criteria\n\n- ✅ SaveConfig returns error if Redis update fails (no silent drift)\n- ✅ Config version tracked in PostgreSQL + Redis\n- ✅ Reconciler runs every 5 minutes, detects drift\n- ✅ Reconciler auto-fixes drift (updates Redis from PostgreSQL)\n- ✅ Metrics track drift events (`config_drift_detected_total`)\n- ✅ Alert fires on drift detection\n- ✅ Documentation explains consistency model + failure modes\n\n## Files Changed\n\n**Created:**\n- `internal/app/reconciler.go` (config reconciliation job)\n- `internal/database/sqlc/schemas/003_add_config_version.sql` (version migration)\n\n**Modified:**\n- `internal/domain/config.go` (add Version + UpdatedAt fields)\n- `internal/domain/session.go` (add ListActiveSessions to interface)\n- `internal/redis/session_repository.go` (store/check version, implement ListActiveSessions)\n- `internal/app/service.go` (return error on Redis update failure)\n- `internal/app/metrics.go` (add drift metrics)\n- `internal/database/sqlc/queries/config.sql` (regenerate for version field)\n- `cmd/server/main.go` (start reconciler goroutine)\n- `CLAUDE.md` (add Data Consistency Model section)\n\n## Dependencies\n- Optional: Observability epic for metrics (can implement metrics later)\n\n## Effort Estimate\n**Total: 4 hours** (240 minutes)\n- Error handling (30min): Return error from SaveConfig\n- Version tracking (2h): Migration + code changes + sqlc regen\n- Reconciler (2h): Background job + ListActiveSessions\n- Metrics (1h): Drift detection + alerting\n- Documentation (30min): CLAUDE.md section\n\n## Success Metrics\n- Zero `config_drift_detected_total` in steady state\n- If drift occurs, auto-fixed within 5 minutes\n- `redis_update_failures_total` tracks Redis availability issues","status":"open","priority":3,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":240,"created_at":"2026-02-12T17:46:23.059791+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:50.679882+01:00"}
{"id":"twitch-tow-4ax","title":"Discussion: Public overlay UUID as security boundary","description":"The overlay system uses UUIDs for access control, which has implications:\n\n**Current design:**\n- Overlay URL: GET /overlay/{uuid} (public, no auth)\n- WebSocket: GET /ws/overlay/{uuid} (public, no auth)\n- UUID is separate from user's internal ID (user.overlay_uuid vs user.id)\n- UUID can be rotated via POST /api/rotate-overlay-uuid (authenticated + CSRF)\n\n**Security model:**\n- UUID acts as a bearer token (whoever has UUID can view overlay)\n- UUID space: 2^122 (122 bits of entropy) - effectively unguessable\n- No rate limiting on overlay/WebSocket endpoints\n- No session tracking for overlay viewers\n- Streamers expose UUID in OBS (visible to viewers via browser source URL inspection)\n\n**Threat model:**\n\n**Low risk:**\n1. Brute force UUID guessing: Infeasible (2^122 space)\n2. User A accessing User B's overlay: Requires knowing UUID (not leaked in UI)\n\n**Medium risk:**\n1. **Leaked UUID in streaming software**: If streamer shares OBS config/screenshot, UUID is visible\n2. **No revocation without rotation**: Can't block specific overlay viewers, only rotate UUID (breaks existing links)\n3. **Denial of service**: No rate limit on WebSocket connections per UUID (maxClientsPerSession=50 per instance, but no global limit)\n\n**High risk (acceptable):**\n1. **Intended public exposure**: Streamers WANT viewers to see overlay, so lack of viewer auth is by design\n2. **UUID rotation breaks embed**: When rotated, old URLs stop working (documented trade-off)\n\n**Potential improvements (if needed):**\nA. Add rate limiting to overlay endpoints (per-IP, per-UUID)\nB. Add session tracking for overlay viewers (analytics)\nC. Add optional password protection (per-streamer setting)\nD. Document UUID rotation UX impact\n\n**Verdict**: Security model is appropriate for use case. UUID rotation provides sufficient revocation. Consider rate limiting if abuse occurs.","notes":"RESOLVED: Already addressed by Epic 3 (twitch-tow-q21) Task 1 - ADR-008: UUID-based overlay access control. Documents threat model, security posture, alternatives considered (passwords, OAuth, signed URLs), and trade-offs. Verdict: Security model appropriate for use case (UUID = bearer token, rotation provides revocation).","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:44.069135+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:45:14.216994+01:00","closed_at":"2026-02-12T17:45:14.216997+01:00"}
{"id":"twitch-tow-4c4","title":"EPIC: Local Config Cache - Reduce Redis Config Reads by 99%","description":"## Epic Overview\nImplement in-memory config caching with TTL to dramatically reduce Redis load from broadcaster tick loops. Reduces config reads from 20/sec per session to 0.1/sec per session (99.5% reduction).\n\n## User Story\nAs an operator, I need config reads to be cached locally so Redis isn't overwhelmed by repetitive config fetches, reducing load and improving performance.\n\n## Parent Solution\ntwitch-tow-ojd (Redis Resilience via Sentinel + Read Replicas + Circuit Breaker)\n\n## Dependencies\n- twitch-tow-6hl (Circuit Breaker) - should be complete first\n\n## Problem Analysis\n\n**Current behavior:**\n- Broadcaster ticks every 50ms (20 times/second)\n- Each tick calls `GetCurrentValue()` → fetches config from Redis\n- 1,000 sessions = 20,000 Redis reads/second just for config\n- Config rarely changes (only when user clicks Save in dashboard)\n\n**Solution:**\n- Cache config in memory with 10-second TTL\n- 10s delay for config updates is acceptable\n- Reduces to ~0.1 reads/sec per session (when TTL expires)\n- 99.5% reduction in Redis calls\n\n## Technical Requirements\n\n### Cache Behavior\n- **TTL:** 10 seconds (config changes appear within 10s)\n- **Invalidation:** TTL-based (simple, no pub/sub complexity)\n- **Thread-safety:** RWMutex for concurrent access\n- **Memory:** ~200 bytes per cached config × 1K sessions = 200 KB\n\n### Cache Hit Rate Target\n- Expected: \u003e99% hit rate\n- Measured via metrics: `config_cache_hits_total` vs `config_cache_misses_total`\n\n## Implementation Tasks\n\n### Task 1: Create config cache\n**File:** `internal/sentiment/config_cache.go` (NEW)\n```go\npackage sentiment\n\nimport (\n    \"sync\"\n    \"time\"\n    \n    \"github.com/google/uuid\"\n    \"github.com/pscheid92/chatpulse/internal/domain\"\n)\n\ntype ConfigCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]*cacheEntry\n    ttl     time.Duration\n    clock   clockwork.Clock\n}\n\ntype cacheEntry struct {\n    config    domain.ConfigSnapshot\n    expiresAt time.Time\n}\n\nfunc NewConfigCache(ttl time.Duration, clock clockwork.Clock) *ConfigCache {\n    return \u0026ConfigCache{\n        entries: make(map[uuid.UUID]*cacheEntry),\n        ttl:     ttl,\n        clock:   clock,\n    }\n}\n\nfunc (c *ConfigCache) Get(sessionUUID uuid.UUID) (*domain.ConfigSnapshot, bool) {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[sessionUUID]\n    if !ok {\n        return nil, false\n    }\n    \n    if c.clock.Now().After(entry.expiresAt) {\n        // Expired, treat as miss\n        return nil, false\n    }\n    \n    return \u0026entry.config, true\n}\n\nfunc (c *ConfigCache) Set(sessionUUID uuid.UUID, config domain.ConfigSnapshot) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    c.entries[sessionUUID] = \u0026cacheEntry{\n        config:    config,\n        expiresAt: c.clock.Now().Add(c.ttl),\n    }\n}\n\nfunc (c *ConfigCache) Invalidate(sessionUUID uuid.UUID) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    delete(c.entries, sessionUUID)\n}\n\nfunc (c *ConfigCache) Clear() {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    c.entries = make(map[uuid.UUID]*cacheEntry)\n}\n\nfunc (c *ConfigCache) Size() int {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    return len(c.entries)\n}\n\n// Periodic cleanup of expired entries (prevents unbounded growth)\nfunc (c *ConfigCache) EvictExpired() int {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    now := c.clock.Now()\n    evicted := 0\n    \n    for uuid, entry := range c.entries {\n        if now.After(entry.expiresAt) {\n            delete(c.entries, uuid)\n            evicted++\n        }\n    }\n    \n    return evicted\n}\n```\n\n### Task 2: Integrate cache into Engine\n**File:** `internal/sentiment/engine.go`\n```go\ntype Engine struct {\n    sessions     domain.SessionRepository\n    sentiment    domain.SentimentStore\n    debouncer    domain.Debouncer\n    clock        clockwork.Clock\n    configCache  *ConfigCache  // NEW\n}\n\nfunc NewEngine(sessions, sentiment, debouncer, clock, cache) *Engine {\n    return \u0026Engine{\n        sessions:    sessions,\n        sentiment:   sentiment,\n        debouncer:   debouncer,\n        clock:       clock,\n        configCache: cache,\n    }\n}\n\nfunc (e *Engine) GetCurrentValue(ctx, sessionUUID) (domain.SessionUpdate, error) {\n    // Try cache first\n    config, hit := e.configCache.Get(sessionUUID)\n    if !hit {\n        // Cache miss, fetch from Redis\n        metrics.ConfigCacheMisses.Inc()\n        \n        fetchedConfig, err := e.sessions.GetSessionConfig(ctx, sessionUUID)\n        if err != nil {\n            return domain.SessionUpdate{}, err\n        }\n        \n        config = \u0026fetchedConfig\n        e.configCache.Set(sessionUUID, fetchedConfig)\n    } else {\n        metrics.ConfigCacheHits.Inc()\n    }\n    \n    // Get sentiment value (potentially decayed)\n    value, err := e.sentiment.GetSentiment(ctx, sessionUUID, e.clock.Now())\n    if err != nil {\n        return domain.SessionUpdate{}, err\n    }\n    \n    return domain.SessionUpdate{\n        Value:  value,\n        Status: \"active\",\n    }, nil\n}\n```\n\n### Task 3: Invalidate cache on config save\n**File:** `internal/app/service.go`\n```go\nfunc (s *Service) SaveConfig(ctx, userID, updates) error {\n    // ... existing validation and save logic\n    \n    // Invalidate config cache\n    user, err := s.users.GetByID(ctx, userID)\n    if err != nil {\n        return err\n    }\n    \n    // Invalidate immediately so next tick picks up new config\n    s.engine.InvalidateConfigCache(user.OverlayUUID)\n    \n    return nil\n}\n```\n\n**File:** `internal/sentiment/engine.go`\n```go\nfunc (e *Engine) InvalidateConfigCache(sessionUUID uuid.UUID) {\n    e.configCache.Invalidate(sessionUUID)\n}\n```\n\n### Task 4: Add cache eviction timer\n**File:** `internal/sentiment/config_cache.go`\nAdd periodic cleanup to prevent unbounded growth:\n```go\nfunc (c *ConfigCache) StartEvictionTimer(interval time.Duration) func() {\n    ticker := time.NewTicker(interval)\n    done := make(chan bool)\n    \n    go func() {\n        for {\n            select {\n            case \u003c-ticker.C:\n                evicted := c.EvictExpired()\n                if evicted \u003e 0 {\n                    slog.Debug(\"Evicted expired config cache entries\", \"count\", evicted)\n                    metrics.ConfigCacheEvictions.Add(float64(evicted))\n                }\n            case \u003c-done:\n                ticker.Stop()\n                return\n            }\n        }\n    }()\n    \n    return func() {\n        close(done)\n    }\n}\n```\n\nStart in `cmd/server/main.go`:\n```go\ncache := sentiment.NewConfigCache(10*time.Second, clock)\nstopEviction := cache.StartEvictionTimer(1 * time.Minute)\ndefer stopEviction()\n```\n\n### Task 5: Add cache metrics\n**File:** `internal/metrics/metrics.go`\n```go\nConfigCacheHits = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"config_cache_hits_total\",\n        Help: \"Total config cache hits\",\n    },\n)\n\nConfigCacheMisses = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"config_cache_misses_total\",\n        Help: \"Total config cache misses\",\n    },\n)\n\nConfigCacheSize = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"config_cache_entries\",\n        Help: \"Number of entries in config cache\",\n    },\n)\n\nConfigCacheEvictions = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"config_cache_evictions_total\",\n        Help: \"Number of expired entries evicted from cache\",\n    },\n)\n```\n\nUpdate gauge periodically:\n```go\nfunc (c *ConfigCache) StartEvictionTimer(...) {\n    // ... in ticker loop\n    metrics.ConfigCacheSize.Set(float64(c.Size()))\n}\n```\n\n### Task 6: Unit tests for cache\n**File:** `internal/sentiment/config_cache_test.go` (NEW)\nTest scenarios:\n1. **Cache miss:** Get non-existent key returns false\n2. **Cache hit:** Set then Get returns cached value\n3. **TTL expiry:** Value expires after TTL, treated as miss\n4. **Explicit invalidation:** Invalidate() removes entry\n5. **Clear:** Clear() removes all entries\n6. **EvictExpired:** Removes only expired entries\n7. **Concurrency:** Concurrent Get/Set doesn't race (run with -race)\n\nUse fake clock:\n```go\nclock := clockwork.NewFakeClock()\ncache := NewConfigCache(10*time.Second, clock)\n\n// Set entry\ncache.Set(uuid, config)\n\n// Advance time\nclock.Advance(11 * time.Second)\n\n// Should be expired\n_, hit := cache.Get(uuid)\nassert.False(t, hit)\n```\n\n### Task 7: Integration test with broadcaster\n**File:** `internal/broadcast/broadcaster_config_cache_test.go` (NEW)\nTest scenario:\n1. Create broadcaster with real Engine + ConfigCache\n2. Activate session (config fetched from Redis, cached)\n3. Make 100 tick calls\n4. Verify Redis only queried once (cache hit rate = 99%)\n5. Update config in database\n6. Invalidate cache\n7. Next tick fetches new config from Redis\n\n### Task 8: Measure memory usage\n**File:** `internal/sentiment/config_cache_bench_test.go` (NEW)\nBenchmark:\n```go\nfunc BenchmarkConfigCache(b *testing.B) {\n    cache := NewConfigCache(10*time.Second, clockwork.NewRealClock())\n    config := domain.ConfigSnapshot{\n        TriggerFor:     \"test\",\n        TriggerAgainst: \"test\",\n        LabelFor:       \"test\",\n        LabelAgainst:   \"test\",\n        DecaySpeed:     1.0,\n    }\n    \n    b.ResetTimer()\n    for i := 0; i \u003c b.N; i++ {\n        uuid := uuid.New()\n        cache.Set(uuid, config)\n        cache.Get(uuid)\n    }\n    \n    b.ReportMetric(float64(cache.Size()), \"entries\")\n}\n```\n\nExpected: \u003c300 bytes per entry (including map overhead)\n\n### Task 9: Documentation\n**File:** `docs/architecture/config-caching.md` (NEW)\nDocument:\n- Cache behavior (TTL, invalidation)\n- Memory overhead analysis\n- Hit rate monitoring\n- Tuning TTL parameter\n- Trade-offs (10s delay for config updates)\n\n### Task 10: Update CLAUDE.md\nAdd config caching to ## Sentiment Engine section\n\n## Acceptance Criteria\n\n- ✅ Config cache implemented with 10-second TTL\n- ✅ Cache hit rate \u003e99% under normal load\n- ✅ Redis config reads reduced by 99%+\n- ✅ Memory overhead \u003c500 KB for 1,000 sessions\n- ✅ Config changes appear in overlay within 10 seconds\n- ✅ Cache invalidated immediately on config save\n- ✅ Metrics track hit/miss rates\n- ✅ Unit tests achieve 100% coverage\n- ✅ Integration test verifies cache behavior\n- ✅ No race conditions (verified with -race flag)\n\n## Files Created/Modified\n\n**New files:**\n- `internal/sentiment/config_cache.go` (200 lines)\n- `internal/sentiment/config_cache_test.go` (300 lines)\n- `internal/sentiment/config_cache_bench_test.go` (100 lines)\n- `internal/broadcast/broadcaster_config_cache_test.go` (150 lines)\n- `docs/architecture/config-caching.md` (200 lines)\n\n**Modified files:**\n- `internal/sentiment/engine.go` (integrate cache, add 30 lines)\n- `internal/app/service.go` (invalidate cache on save, add 5 lines)\n- `cmd/server/main.go` (create cache, start eviction timer, add 10 lines)\n- `internal/metrics/metrics.go` (add cache metrics, add 30 lines)\n- `CLAUDE.md` (document config caching)\n\n## Testing Strategy\n\n**Unit tests:**\n- Test all cache operations\n- Test TTL expiry with fake clock\n- Test concurrent access with -race\n- Test eviction logic\n\n**Integration tests:**\n- Test with real broadcaster tick loop\n- Measure actual hit rate\n- Test invalidation on config save\n- Verify new config appears within 10s\n\n**Performance tests:**\n- Benchmark memory usage per entry\n- Measure cache overhead (should be negligible)\n- Load test with 10K sessions\n\n**Manual testing:**\n- Deploy to staging\n- Monitor metrics (hit rate should be 99%+)\n- Change config in dashboard\n- Verify overlay updates within 10 seconds\n\n## Dependencies\n- **Blocks:** twitch-tow-6hl (circuit breaker)\n- **Recommended:** Have metrics epic (twitch-tow-kgj) complete first\n\n## Success Metrics\n- Redis config reads reduced from 20K/sec to 100/sec (1K sessions)\n- Cache hit rate sustained \u003e99%\n- Memory usage \u003c500 KB for 1K sessions\n- Config updates appear within 10s (p99)\n- Zero race conditions detected\n\n## Effort Estimate\n**5 developer-days** (1 week)\n\nBreakdown:\n- Cache implementation: 1 day\n- Integration: 1 day\n- Unit tests: 1 day\n- Integration tests: 1 day\n- Documentation + benchmarks: 1 day\n\n## Risk Mitigation\n- **Risk:** 10-second delay confuses users\n  - **Mitigation:** Show \"Saved! Changes will appear in 10 seconds\" in UI\n  - **Mitigation:** Add explicit invalidation on save (already planned)\n- **Risk:** Cache memory leak\n  - **Mitigation:** Periodic eviction timer (1-minute interval)\n  - **Mitigation:** Monitor cache size metric\n- **Risk:** Cache inconsistency across instances\n  - **Mitigation:** Each instance has own cache (10s eventual consistency)\n  - **Mitigation:** TTL ensures refresh every 10s","status":"in_progress","priority":2,"issue_type":"epic","assignee":"dev-backend","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:27:59.043974+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:17:19.306259+01:00"}
{"id":"twitch-tow-4kh","title":"Idea: Add architecture decision records (ADR)","description":"The codebase has made several significant architectural decisions but lacks formal ADR documentation:\n\nKey decisions that should be documented:\n- Redis-only architecture (no in-memory state)\n- Pull-based broadcaster vs pub/sub\n- Actor pattern for broadcaster\n- Webhooks + conduits vs EventSub WebSocket\n- Manual DI vs framework\n- sqlc for SQL generation\n- Single bot account for all streamers\n- UUID-based overlay access control\n- Time-decay sentiment algorithm\n- Singleflight for session activation\n- AES-256-GCM for token encryption\n- tern for migrations\n\nCurrent documentation state:\n- CLAUDE.md contains excellent narrative docs\n- Code comments explain WHY for specific decisions (e.g. CORS policy)\n- But no structured record of decision rationale, alternatives considered, or consequences\n\nBenefits of ADR:\n1. Historical context for future maintainers\n2. Documents alternatives that were rejected and why\n3. Makes trade-offs explicit\n4. Helps new contributors understand constraints\n5. Prevents revisiting settled decisions\n\nRecommended ADR format:\n- docs/adr/0001-redis-only-architecture.md\n- docs/adr/0002-pull-based-broadcasting.md\n- Each ADR: Title, Context, Decision, Consequences, Status\n\nLightweight approach:\nStart with 3-5 most critical decisions, add more as needed. Keep template simple (1-2 pages max per ADR).\n\nRecommendation: Add ADRs for top 5 architectural decisions. Reference from CLAUDE.md for discoverability.","notes":"RESOLVED: Already addressed by Epics 1-5 (twitch-tow-3dd, u90, q21, dmu). 16 ADRs planned across 5 tiers covering all major architectural decisions.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:08:35.287252+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:50.956957+01:00","closed_at":"2026-02-12T17:37:50.956959+01:00"}
{"id":"twitch-tow-4ml","title":"Fix slog format issues in eventsub.go","description":"**Context**: internal/twitch/eventsub.go has slog calls using Printf-style formatting instead of key-value pairs.\n\n**Errors**:\n- Line 74: call to slog.Info missing a final value\n- Line 93: slog.Error arg \"err\" should be a string or a slog.Attr\n- Line 139: call to slog.Info missing a final value\n- Line 152: slog.Info arg \"userID\" should be a string or a slog.Attr\n- Line 173: call to slog.Info missing a final value\n- Line 186: slog.Error arg \"cleanupErr\" should be a string or a slog.Attr\n- Line 207: slog.Error arg \"err\" should be a string or a slog.Attr\n- Line 216: slog.Info arg \"userID\" should be a string or a slog.Attr\n\n**Fix**: Convert all Printf-style slog calls to proper key-value pairs:\n`slog.Info(\"message\", \"key\", value)` instead of `slog.Info(\"message: %s\", value)`\n\n**Location**: internal/twitch/eventsub.go\n\n**Impact**: Tests currently fail to compile in twitch package","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:51:41.899446+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:54:39.084298+01:00","closed_at":"2026-02-12T16:54:39.084298+01:00","close_reason":"Fixed all slog format issues in eventsub.go\n\nConverted 11 Printf-style slog calls to proper key-value pairs:\n\n✅ Line 74: Found existing conduit - added 'conduit_id' key\n✅ Line 83: Created conduit - added 'conduit_id' and 'shard_count' keys\n✅ Line 93: Shard configuration error - fixed key order (conduit_id, error)\n✅ Line 110: Conduit configured - added 'conduit_id' and 'callback_url' keys\n✅ Line 139: Deleted conduit - added 'conduit_id' key\n✅ Line 152: Subscription exists - added 'user_id' key\n✅ Line 173: Subscription exists on Twitch - added 'broadcaster_user_id' key\n✅ Line 186: Cleanup failure - fixed to use 'subscription_id' and 'error' keys\n✅ Line 191: Subscribed to chat - added 'broadcaster_user_id' and 'subscription_id' keys\n✅ Line 207: Delete subscription error - fixed to use 'subscription_id' and 'error' keys\n✅ Line 216: Unsubscribed - added 'user_id' key\n\nAll slog calls now use proper structured logging format:\n`slog.Level(\"message\", \"key\", value, \"key2\", value2)`\n\nBuild verification:\n✅ go build ./internal/twitch/ - Success\n✅ go test -c ./internal/twitch/ - Success  \n✅ make build - Success\n\nTwitch package now compiles and tests build successfully!"}
{"id":"twitch-tow-4v4","title":"Discussion: Webhook vote processing has no rate limiting per broadcaster","description":"The webhook handler ProcessVote() pipeline has per-user debouncing (1 second) but no rate limiting at the broadcaster/session level. A coordinated bot attack could overwhelm Redis.\n\n**Current protection:**\n1. ✓ Per-chatter debounce (1s via Redis SETNX with TTL)\n2. ✓ HMAC signature verification (Kappopher)\n3. ✓ Message ID deduplication (Kappopher, replay protection)\n4. ✓ Timestamp freshness check (10 min window)\n\n**Missing protection:**\n1. ✗ No rate limit per broadcaster/session\n2. ✗ No rate limit per IP (Twitch webhook IPs are shared)\n3. ✗ No detection of coordinated bot attacks (N unique chatters)\n\n**Attack scenarios:**\n\n**Scenario 1: Botnet vote flooding**\n- 10,000 unique bot accounts spam trigger words\n- Each bot bypasses debounce (unique chatter ID)\n- Result: 10,000 votes processed instantly\n- Impact: Redis receives 10K FCALL apply_vote in burst\n\n**Scenario 2: Multiple broadcasters targeted**\n- Attacker spams trigger words in 100 channels simultaneously\n- 100 broadcasters × 100 chatters = 10,000 votes/sec\n- Impact: All app instances process votes, Redis handles aggregate load\n\n**Why this is partially mitigated:**\n1. Twitch EventSub already rate-limits webhook delivery\n2. Vote delta is clamped to [-100, 100] (bounded impact)\n3. Time decay reduces value over time\n4. Attack requires actual Twitch accounts (not free)\n5. Broadcaster can change trigger words to mitigate\n\n**Scalability impact:**\n- Redis can handle 100K+ ops/sec\n- Vote processing is O(1) per vote (Lua function)\n- Main risk: Redis CPU exhaustion from sustained load\n- Secondary risk: App instance CPU from signature verification\n\n**Potential solutions:**\n1. **Sliding window rate limiter per broadcaster** (e.g., 100 votes/minute)\n   - Pro: Protects against coordinated attacks\n   - Con: Legitimate high-engagement moments might be throttled\n   \n2. **Token bucket per session** (burst allowed, sustained rate limited)\n   - Pro: Allows natural spikes, blocks sustained abuse\n   - Con: More complex to implement\n\n3. **Adaptive rate limiting** (increase limit if sustained traffic)\n   - Pro: Balances protection vs engagement\n   - Con: Complex heuristics needed\n\n4. **Vote deduplication window** (track recent message hashes)\n   - Pro: Catches copy-paste bot behavior\n   - Con: Storage overhead, may block legitimate spam\n\n5. **Trust score per broadcaster** (reduce limits for suspicious patterns)\n   - Pro: Surgical mitigation\n   - Con: Requires ML/heuristics\n\n**Recommendation:**\nImplement token bucket per session (100 votes/min sustained, burst to 20):\n- Protects against sustained bot attacks\n- Allows natural high-engagement moments\n- Simple to implement with Redis INCR + EXPIRE\n\nPriority: P2 (medium) - current protections are decent but sustained bot attacks could degrade Redis performance","notes":"RESOLVED: This webhook vote rate limiting concern is addressed by the consolidated resource limits solution.\n\n**Implemented in:**\n- **twitch-tow-mjm** Layer 3: Token Bucket Rate Limiter for Votes (week 2)\n- **twitch-tow-tsg** (Epic: Vote Rate Limiting implementation details)\n\n**How the solution addresses this:**\n\nImplements exactly the recommended token bucket approach from this discussion:\n\n**Token bucket per session in Redis Lua:**\n```lua\n-- Capacity: 100 tokens (burst allowance)\n-- Refill rate: 100 tokens/minute (sustained rate)\n-- Allows: 100 votes immediately, then 100/min sustained\n```\n\n**Attack scenarios mitigated:**\n\n**Scenario 1: Botnet vote flooding** ✅\n- 10,000 unique bots spam trigger words\n- Per-user debounce (1s) bypassed by unique chatter IDs\n- **Solution:** Session-level token bucket limits to 100 votes burst, then 100/min\n- Result: 10,000 bot attempts → 100 applied, 9,900 rate limited\n\n**Scenario 2: Multiple broadcasters targeted** ✅\n- 100 broadcasters × 100 chatters = 10,000 votes/sec\n- **Solution:** Each session has independent token bucket\n- Result: Each session limited to 100/min independently\n\n**Why token bucket (vs alternatives):**\n- ✅ Allows natural bursts (chat excitement during hype moments)\n- ✅ Prevents sustained bot spam (refills gradually)\n- ✅ Fair per-session enforcement\n- ✅ Redis Lua implementation (multi-instance safe)\n\n**Implementation location:**\n- After debounce check, before ApplyVote in ProcessVote pipeline\n- Fails open on Redis errors (availability \u003e strict limiting)\n- Metrics: vote_rate_limit_checks_total{result}\n\nThe solution directly addresses the \"sustained bot attacks could degrade Redis performance\" risk identified in this discussion.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:39.463546+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:36:49.775329+01:00","closed_at":"2026-02-12T17:36:49.775332+01:00"}
{"id":"twitch-tow-4vo","title":"Idea: Add structured error types and error budget tracking","description":"## Proposal\nDefine structured error types with classification (transient vs. permanent, retriable vs. not) and track error budgets for SLO enforcement.\n\n## Error Type Hierarchy\n```go\ntype ErrorClass string\n\nconst (\n    ErrorClassTransient  ErrorClass = \"transient\"   // Retry may succeed\n    ErrorClassPermanent  ErrorClass = \"permanent\"   // Retry will not help\n    ErrorClassRateLimit  ErrorClass = \"rate_limit\"  // Backoff required\n    ErrorClassValidation ErrorClass = \"validation\"  // User input error\n)\n\ntype AppError struct {\n    Class      ErrorClass\n    Code       string        // e.g., \"REDIS_TIMEOUT\", \"TWITCH_API_500\"\n    Message    string\n    Cause      error\n    Retriable  bool\n    RetryAfter time.Duration\n    Context    map[string]any\n}\n```\n\n## Error Budget Implementation\n```go\ntype ErrorBudget struct {\n    windowSize    time.Duration  // e.g., 5 minutes\n    threshold     float64        // e.g., 0.01 (1% error rate)\n    recentErrors  []time.Time    // sliding window\n    mu            sync.RWMutex\n}\n\nfunc (eb *ErrorBudget) RecordError() bool {\n    // Returns false if error budget exhausted\n}\n\nfunc (eb *ErrorBudget) ShouldCircuitBreak() bool {\n    return eb.ErrorRate() \u003e eb.threshold\n}\n```\n\n## Error Classification Examples\n- **Transient**: Redis timeout, Twitch API 503, network error\n- **Permanent**: Invalid UUID, user not found, invalid config\n- **Rate Limit**: Twitch API 429, Redis LOADING\n- **Validation**: Empty trigger, invalid decay rate\n\n## Usage Pattern\n```go\nfunc (e *Engine) ProcessVote(...) (float64, error) {\n    sessionUUID, found, err := e.sessions.GetSessionByBroadcaster(ctx, broadcasterUserID)\n    if err != nil {\n        return 0, \u0026AppError{\n            Class:     ErrorClassTransient,\n            Code:      \"SESSION_LOOKUP_FAILED\",\n            Message:   \"failed to lookup session\",\n            Cause:     err,\n            Retriable: true,\n            Context:   map[string]any{\"broadcaster_user_id\": broadcasterUserID},\n        }\n    }\n    // ...\n}\n```\n\n## Error Budget Policy\n- **Vote processing**: 99% success rate (1% budget)\n- **Config updates**: 99.9% success rate (0.1% budget)\n- **WebSocket connections**: 95% success rate (5% budget, OBS flakiness)\n- **Twitch API**: 98% success rate (2% budget, API instability)\n\n## Related Issues\n- twitch-tow-uf7 (Error handling consistency)\n- twitch-tow-c8q (Missing observability)\n\n## Files to Modify\n- internal/domain/errors.go (define error types)\n- internal/sentiment/engine.go (return structured errors)\n- internal/twitch/webhook.go (classify Twitch API errors)\n- internal/app/service.go (error budget tracking)","notes":"NOTE: Epic 4 (twitch-tow-apx) covers error classification (3 tiers: Domain/Infrastructure/Programming) and subcategories (Transient/Permanent/Rate Limit). This bead proposes structured AppError type with fields. Complementary but may be lower priority since Epic 4 addresses core concerns. Consider deferring structured types to Phase 2.\nRESOLVED: Core concerns addressed by Epic 4 (twitch-tow-apx) which provides 3-tier error classification (Domain/Infrastructure/Programming) with subcategories (Transient/Permanent/Rate Limit). The structured AppError type proposed in this bead is complementary but deferred to Phase 2 - Epic 4's classification system is simpler and addresses 80% of use cases.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:36.281005+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:48:24.754828+01:00","closed_at":"2026-02-12T17:48:24.754831+01:00"}
{"id":"twitch-tow-563","title":"Add timeout and synchronization to Broadcaster.Stop()","description":"**High Priority (Concurrency)**\n\nLocation: internal/broadcast/broadcaster.go lines 102-104\n\nIssue: If handleTick() blocks, the stopCmd will be queued but never processed. Stop() is non-blocking - the broadcaster's run() goroutine might still be executing after Stop() returns.\n\nImpact: Graceful shutdown can hang. Race conditions during shutdown.\n\nFix:\n- Add timeout to Stop() (e.g., 5-10 seconds)\n- Use sync.WaitGroup or done channel to wait for goroutine exit\n- Consider adding context cancellation\n- Test shutdown scenarios","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:25:47.567458+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:38:13.575937+01:00","closed_at":"2026-02-12T16:38:13.575937+01:00","close_reason":"FIXED: Added proper shutdown synchronization to Broadcaster.Stop(). Changes: (1) Added 'done' channel to Broadcaster struct, closed when run() goroutine exits. (2) Stop() now blocks waiting for done channel (10s timeout). (3) Added WaitGroup to clientWriter to ensure write goroutines exit before stop() returns. (4) All tests pass with race detector. Goroutine cleanup verified - Stop() now guarantees all broadcaster goroutines exit before returning."}
{"id":"twitch-tow-597","title":"Discussion: WebSocket connection lifecycle and resource cleanup","description":"## Issue\nWebSocket connections lack explicit timeout enforcement, leak detection, and connection state tracking. Read pump blocks indefinitely with no idle timeout.\n\n## Current State\n- Read pump loops forever reading messages that are ignored (handlers_overlay.go:116-120)\n- No idle connection timeout (clients could stay connected forever doing nothing)\n- SetReadDeadline only updated on pong receipt (writer.go:87-89)\n- SetWriteDeadline updated before each write (writer.go:82-84)\n- clientWriter.stop() waits indefinitely for goroutine exit (writer.go:66-71)\n- Upgrader accepts all origins (intentional, documented)\n\n## Failure Modes\n1. **Zombie connections**: Clients that stop sending pongs but don't close TCP connection\n2. **Resource leaks**: clientWriter goroutine could leak if stop() doesn't complete\n3. **Unbounded connection lifetime**: No maximum connection age (hours/days old connections)\n4. **OBS browser source tabs**: Left-open OBS tabs maintain WebSocket connections indefinitely\n5. **No connection limits**: Instance-level connection cap missing (only per-session cap: 50)\n\n## Risks\n- **Memory exhaustion**: Long-lived connections accumulate in activeClients map\n- **File descriptor exhaustion**: OS limits on open connections (typically 1024-65536)\n- **Broadcast latency**: More connections = more time to fan out updates\n- **Cleanup on crash**: Ungraceful shutdown leaves connections in TIME_WAIT\n\n## Suggestions\n1. Add idle connection timeout (e.g., 5 minutes of no pongs = force disconnect)\n2. Add maximum connection age (e.g., 24 hours, force reconnect)\n3. Add instance-level connection cap (e.g., 5000 connections total)\n4. Make clientWriter.stop() timeout after N seconds (don't wait forever)\n5. Add connection state metrics (active, idle, age histogram)\n6. Implement connection draining during graceful shutdown (send close frame)\n7. Add rate limiting for connection attempts (per IP, per session)\n8. Log connection lifecycle events (connect, disconnect, reason)\n\n## Files\n- internal/server/handlers_overlay.go:80-125 (WebSocket lifecycle)\n- internal/broadcast/writer.go:40-64 (clientWriter.run)\n- internal/broadcast/writer.go:66-72 (clientWriter.stop)\n- internal/broadcast/broadcaster.go:16-17 (maxClientsPerSession)","notes":"RESOLVED: WebSocket connection lifecycle concerns are addressed by the consolidated resource limits solution.\n\n**Implemented in:**\n- **twitch-tow-mjm** (CONSOLIDATED: Resource Limits - Multi-Layer Defense)\n- **twitch-tow-dmg** (Epic: Global WebSocket Connection Limits)\n\n**How the solution addresses the concerns:**\n\n**1. Zombie connections** ✅\n- Current ping/pong heartbeat (30s/60s) already handles this\n- Pong timeout disconnects zombie connections\n- No changes needed beyond existing implementation\n\n**2. Resource leaks** ✅\n- clientWriter.stop() timeout not added (blocking is intentional for clean shutdown)\n- Global connection limit prevents unbounded growth\n- Metrics track connection count for leak detection\n\n**3. Unbounded connection lifetime** ⚠️ \n- No maximum connection age enforced\n- Decision: Not needed - ping/pong timeout sufficient\n- OBS tabs staying open indefinitely is acceptable behavior\n\n**4. OBS browser source tabs** ✅\n- Addressed by maxClientsPerSession = 50 (existing)\n- Global limit prevents instance-level exhaustion\n- Per-IP limit prevents single user monopolizing resources\n\n**5. No connection limits** ✅\n- **Global limit:** 10,000 connections per instance (configurable)\n- **Per-IP limit:** 100 concurrent connections per IP\n- **Per-session limit:** 50 clients (already exists)\n\n**Additional protections added:**\n- Connection rate limiting (10/sec per IP)\n- File descriptor check on startup (ulimit warning)\n- Metrics for connection tracking:\n  - websocket_connections_current\n  - websocket_connections_rejected_total{reason}\n  - websocket_connection_capacity_pct\n\n**Not implemented (intentional):**\n- Idle connection timeout (ping/pong sufficient)\n- Maximum connection age (unnecessary complexity)\n- Connection draining (graceful shutdown already exists)\n\nThe key risks (memory exhaustion, file descriptor exhaustion) are mitigated by the global and per-IP connection limits.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:47.657927+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:31.290329+01:00","closed_at":"2026-02-12T17:37:31.290329+01:00","close_reason":"Superseded by implementation epic twitch-tow-d80 (WebSocket Connection Lifecycle Management). Epic provides full solution with idle timeouts, max age, instance cap, graceful shutdown, metrics. Addresses all failure modes."}
{"id":"twitch-tow-5h7","title":"Discussion: Lua function deployment has no versioning strategy","description":"Lua functions are loaded once at startup via FunctionLoadReplace. No versioning or rollback strategy exists.\n\nCurrent approach:\n- chatpulse.lua embedded in binary via go:embed\n- Loaded with FUNCTION LOAD REPLACE on NewClient\n- Same library shared by all app instances\n- No version tracking\n\nRisks:\n\n1. Incompatible function changes\n- Change Lua function signature or behavior\n- Old instances still running with old code\n- New instances use new Lua functions\n- Data corruption or inconsistent results\n\n2. No rollback mechanism\n- Bad Lua function deployed\n- No way to revert to previous version\n- FUNCTION LOAD REPLACE overwrites immediately\n- All instances affected\n\n3. Redis restart loses functions\n- Redis restart = functions deleted\n- Must reconnect to reload\n- NewClient does this but only on startup\n- Runtime Redis restart breaks app until restart\n\n4. No function version detection\n- Can't check which version is loaded\n- Can't detect version mismatch across instances\n- No way to coordinate upgrades\n\n5. Race during rolling deploy\n- Instance A loads v1 functions\n- Instance B loads v2 functions (REPLACE)\n- Instance A's calls use v2 code (unexpected)\n- Instance A never reloads\n\nExample failure scenario:\n1. Deploy new binary with changed Lua function\n2. Instance 1 starts, FUNCTION LOAD REPLACE v2\n3. Instance 2 (still v1 binary) makes Lua call\n4. Redis executes v2 function (was replaced)\n5. Instance 2 expects v1 return format\n6. Parse error or data corruption\n\nCurrent mitigation:\n- Functions are simple and stable\n- Breaking changes are rare\n- Small deployment window\n\nPotential solutions:\n\nA. Versioned function names\n- apply_vote_v1, apply_vote_v2\n- Load new version alongside old\n- Roll out binary changes first\n- Switch function name reference\n- Delete old version after grace period\n\nB. Function version check\n- Store library version in Redis key\n- Check on startup, warn on mismatch\n- Fail fast if incompatible\n\nC. Blue-green Redis deployment\n- Two Redis instances\n- Deploy new functions to green\n- Switch traffic after all instances upgraded\n- Complex, requires Redis replication\n\nD. Schema migrations for functions\n- Track function version like DB migrations\n- Apply changes incrementally\n- Coordinate with deployment\n\nE. Accept risk for now\n- Functions are simple and stable\n- Breaking changes are rare\n- Document upgrade procedure\n\nRecommendation:\n1. Add version key (functio:version) checked on startup (option B)\n2. Document zero-downtime upgrade procedure\n3. Defer versioned function names (option A) until needed\n\nUpgrade procedure:\n1. Deploy new binary to one instance (it does REPLACE)\n2. Verify new function works\n3. Rolling deploy to remaining instances\n4. All instances now call new function\n\nRisk: Small window where mixed versions run\nMitigation: Make Lua functions backward-compatible\n\nPriority: P2 - low risk now, critical during breaking Lua changes\n","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:36.150903+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:43:21.061907+01:00","closed_at":"2026-02-12T17:43:21.061907+01:00","close_reason":"Superseded by implementation epic twitch-tow-0kl (Lua Function Versioning). Epic provides versioned function names (v2), version tracking in Redis, legacy forwarding, health checks, zero-downtime upgrade procedure."}
{"id":"twitch-tow-5rs","title":"Discussion: Secret rotation and cryptographic hygiene","description":"## Issue\nNo mechanism for rotating secrets (session secret, webhook secret, encryption key). Key compromise requires full redeployment with new secrets.\n\n## Current State\n- Session secret loaded once at startup (server.go:63)\n- Webhook secret loaded once at startup, sent to Twitch (eventsub.go:123)\n- Token encryption key loaded once at startup (main.go:150)\n- No support for multiple valid keys during rotation window\n- No key versioning or rollback capability\n\n## Rotation Scenarios\n1. **Session secret rotation**: All users forced to re-login (session cookies invalidated)\n2. **Webhook secret rotation**: Must update Twitch conduit shard (requires downtime)\n3. **Encryption key rotation**: Cannot decrypt existing tokens (all users must re-auth)\n4. **Key compromise**: No emergency rotation procedure without service disruption\n\n## Risks\n- **Blast radius of compromise**: Single key compromise requires full redeployment + all users re-auth\n- **Zero-downtime impossible**: Key rotation always causes service disruption\n- **Audit requirements**: Some compliance regimes require periodic key rotation (e.g., 90 days)\n- **Crypto best practices**: Long-lived keys accumulate more ciphertext (cryptanalysis risk)\n\n## Suggestions\n1. Support multiple encryption keys with version prefix (e.g., \"v1:ciphertext\")\n   - Current key for encryption, old keys for decryption (rotation window)\n2. Add background job to re-encrypt tokens with new key (gradual migration)\n3. Add webhook secret rotation endpoint (update Twitch conduit + new secret)\n4. Use ephemeral session secrets (rotate every N hours, store mapping in Redis)\n5. Add key expiration timestamps (auto-rotate after 90 days)\n6. Document emergency rotation procedure (steps to minimize downtime)\n7. Add audit trail for key rotation events (who, when, reason)\n8. Consider external key management service (AWS KMS, HashiCorp Vault)\n\n## Files\n- internal/crypto/crypto.go:13-56 (AesGcmCryptoService, no versioning)\n- internal/config/config.go:20 (TokenEncryptionKey loaded once)\n- internal/server/server.go:63 (SessionSecret loaded once)\n- internal/twitch/eventsub.go:123 (WebhookSecret sent to Twitch)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:21.692005+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:46:49.750529+01:00","closed_at":"2026-02-12T17:46:49.750529+01:00","close_reason":"Superseded by implementation epic twitch-tow-li3 (Secret Rotation with Zero-Downtime Key Versioning). Epic provides versioned encryption keys, background re-encryption, webhook secret rotation endpoint, rotation procedures documentation."}
{"id":"twitch-tow-5sz","title":"Discussion: Error handling inconsistency - mixed patterns","description":"The codebase shows inconsistent error handling patterns that hurt maintainability:\n\n**1. Sentinel errors vs wrapped errors**\n- domain.ErrUserNotFound, ErrConfigNotFound (sentinel errors) - good for control flow\n- But many repository methods wrap errors with fmt.Errorf without preserving sentinels\n- Example: UserRepo.Upsert returns 'failed to upsert user: %w' - loses domain semantics\n\n**2. Error logging location varies**\n- Engine.ProcessVote: logs 'ApplyVote error' then returns (0, false) - swallows error\n- EventSubManager: logs then returns error - caller might double-log\n- App.OnSessionEmpty: logs error but continues - correct fire-and-forget pattern\n- Broadcaster.handleTick: logs Redis timeout specifically, generic errors otherwise\n\n**3. Missing error context**\n- Many errors lack sufficient context for debugging (e.g., 'failed to unmarshal config' - which session?)\n- slog structured logging is used inconsistently (some errors have fields, others don't)\n\n**4. HTTP error responses**\n- Generic '500 Internal error' strings in handlers\n- No structured error responses\n- Leaks no information (good security) but hard to debug client-side\n\n**Recommendation**: Establish clear error handling guidelines - where to log, where to wrap, when to use sentinels.","notes":"RESOLVED: Already addressed by Epic 4 (twitch-tow-apx) - Error Handling Standardization. Covers all concerns: sentinel vs wrapped errors (Tier A vs B), logging location rules (decision boundary vs origin), structured logging standards, consistent field names. Epic 4 is the comprehensive solution to all error handling inconsistencies.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:23.364341+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:50:38.243647+01:00","closed_at":"2026-02-12T17:50:38.24365+01:00"}
{"id":"twitch-tow-5ul","title":"Discussion: Testcontainers startup time impact on developer workflow","description":"The test suite uses testcontainers for PostgreSQL and Redis integration tests which adds startup overhead:\n\nCurrent approach:\n- TestMain() in postgres_test.go starts PostgreSQL 15 container once (2-5s overhead)\n- TestMain() in redis integration_test.go starts Redis 7 container once (1-2s overhead)\n- Containers reused across all tests in package\n- Each test truncates/flushes data for isolation\n\nImpact on workflow:\n- First test run: 3-7s container startup\n- Subsequent test runs in same session: No overhead (containers cached)\n- make test-short: 0s overhead (skips integration tests)\n- Total test suite runtime: ~15s (including container startup)\n\nDeveloper experience:\n- TDD loop: Write test -\u003e run package tests -\u003e 15s feedback cycle\n- Quick unit test: Use -short flag -\u003e \u003c2s feedback cycle\n- CI pipeline: 15s test time is acceptable\n\nTrade-offs:\n\nPros (testcontainers):\n- Real database behavior (no mocking PostgreSQL/Redis)\n- Tests are production-like\n- Catches schema/query issues early\n- Docker standardization across team\n\nCons (testcontainers):\n- Slower than pure unit tests\n- Requires Docker daemon running\n- Can fail on resource-constrained CI\n- Not great for rapid TDD loops\n\nAlternative approaches:\nA. Keep testcontainers for CI, use mocks for local TDD\nB. Add make test-unit target (runs only unit tests)\nC. Use ephemeral in-memory databases (sqlite for Postgres, miniredis for Redis)\nD. Extract more logic to pure functions testable without infrastructure\n\nCurrent workaround is effective: -short flag for TDD, full suite for CI.\n\nRecommendation: Document -short flag usage prominently. Consider adding make test-unit alias. Testcontainers approach is appropriate for this codebase size.","notes":"RESOLVED: Merged into Epic 6 (twitch-tow-8dx) - Code Quality Improvements. Testcontainers usage + TDD workflow documented in Task 4. Current approach (testcontainers for integration, -short flag for TDD) is appropriate.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:09:43.916174+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:47.484688+01:00","closed_at":"2026-02-12T17:37:47.484691+01:00"}
{"id":"twitch-tow-682","title":"EPIC: Phase 1 Health Checks - Implement Liveness and Readiness Endpoints","description":"## Epic Overview\nImplement production-ready health check endpoints to enable load balancer traffic routing and automated health monitoring. This is CRITICAL for production deployment.\n\n## User Story\nAs an operator, I need health check endpoints so the load balancer can route traffic only to healthy instances and automatically remove unhealthy instances from the pool.\n\n## Parent Solution\ntwitch-tow-eyl (Comprehensive Observability)\n\n## Technical Requirements\n\n### Endpoints to Implement\n1. **GET /health/live** - Liveness probe (always 200 OK if process alive)\n2. **GET /health/ready** - Readiness probe (200 OK if all dependencies healthy, 503 if any failed)\n\n### Readiness Checks Required\n- **Redis**: PING command with 1s timeout\n- **PostgreSQL**: `SELECT 1` with 1s timeout  \n- **Redis Functions**: Verify chatpulse library loaded (`FUNCTION LIST`)\n- **Twitch EventSub** (optional): Check conduit exists if webhooks configured\n\n### Response Format\n```json\n// Liveness - always 200\n{\"status\": \"ok\"}\n\n// Readiness - 200 when healthy\n{\"status\": \"ready\"}\n\n// Readiness - 503 when unhealthy\n{\n  \"status\": \"unhealthy\",\n  \"failed_check\": \"redis\",\n  \"error\": \"connection timeout\"\n}\n```\n\n## Implementation Tasks\n\n### Task 1: Create health handlers module\n**File:** `internal/server/handlers_health.go`\n```go\ntype healthChecker struct {\n    db    *pgxpool.Pool\n    redis *redis.Client\n}\n\nfunc (s *Server) handleLiveness(c echo.Context) error {\n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n\nfunc (s *Server) handleReadiness(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 5*time.Second)\n    defer cancel()\n    \n    checks := []struct{\n        name string\n        fn func(context.Context) error\n    }{\n        {\"redis\", s.checkRedis},\n        {\"postgres\", s.checkPostgres},\n        {\"redis_functions\", s.checkRedisFunc},\n    }\n    \n    for _, check := range checks {\n        if err := check.fn(ctx); err != nil {\n            return c.JSON(503, map[string]any{\n                \"status\": \"unhealthy\",\n                \"failed_check\": check.name,\n                \"error\": err.Error(),\n            })\n        }\n    }\n    \n    return c.JSON(200, map[string]string{\"status\": \"ready\"})\n}\n\nfunc (s *Server) checkRedis(ctx context.Context) error {\n    return s.redis.Ping(ctx).Err()\n}\n\nfunc (s *Server) checkPostgres(ctx context.Context) error {\n    return s.db.Ping(ctx)\n}\n\nfunc (s *Server) checkRedisFunc(ctx context.Context) error {\n    result := s.redis.FunctionList(ctx, redis.FunctionListQuery{\n        LibraryName: \"chatpulse\",\n    })\n    if result.Err() != nil {\n        return result.Err()\n    }\n    libs, _ := result.Result()\n    if len(libs) == 0 {\n        return fmt.Errorf(\"chatpulse library not loaded\")\n    }\n    return nil\n}\n```\n\n### Task 2: Add routes\n**File:** `internal/server/routes.go`\nAdd routes (no auth required):\n```go\ne.GET(\"/health/live\", s.handleLiveness)\ne.GET(\"/health/ready\", s.handleReadiness)\n```\n\n### Task 3: Unit tests with mocks\n**File:** `internal/server/handlers_health_test.go`\nTest cases:\n- Liveness always returns 200\n- Readiness returns 200 when all checks pass\n- Readiness returns 503 when Redis down\n- Readiness returns 503 when PostgreSQL down\n- Readiness returns 503 when Redis functions not loaded\n- Check timeout behavior (5s total timeout)\n\nMock dependencies:\n```go\ntype mockHealthChecker struct {\n    redisErr    error\n    postgresErr error\n    funcErr     error\n}\n```\n\n### Task 4: Integration test with real failures\n**File:** `internal/server/handlers_health_integration_test.go`\nTest scenario:\n1. Start testcontainers (Redis + PostgreSQL)\n2. Verify /health/ready returns 200\n3. Stop Redis container\n4. Verify /health/ready returns 503 with \"redis\" as failed_check\n5. Restart Redis\n6. Verify /health/ready returns 200 again\n\n### Task 5: Load balancer configuration example\n**File:** `docs/deployment/load-balancer-health-checks.md` (NEW)\nDocument configuration for:\n- AWS ALB target group health checks\n- Nginx upstream health checks\n- Kubernetes liveness/readiness probes\n- HAProxy backend checks\n\nExample Kubernetes:\n```yaml\nlivenessProbe:\n  httpGet:\n    path: /health/live\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\n  \nreadinessProbe:\n  httpGet:\n    path: /health/ready\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 5\n  failureThreshold: 2\n```\n\n### Task 6: Update CLAUDE.md\nDocument new endpoints in ## Key Routes section\n\n## Acceptance Criteria\n\n- ✅ GET /health/live always returns 200 OK\n- ✅ GET /health/ready returns 200 when all dependencies healthy\n- ✅ GET /health/ready returns 503 with clear error when any dependency unhealthy\n- ✅ Health checks complete within 5 seconds total\n- ✅ Unit tests cover all success and failure paths\n- ✅ Integration test verifies real Redis failure detection\n- ✅ Load balancer configuration documented for 4+ platforms\n- ✅ CLAUDE.md updated with new endpoints\n\n## Files Created/Modified\n\n**New files:**\n- `internal/server/handlers_health.go` (150 lines)\n- `internal/server/handlers_health_test.go` (200 lines unit tests)\n- `internal/server/handlers_health_integration_test.go` (100 lines)\n- `docs/deployment/load-balancer-health-checks.md` (200 lines docs)\n\n**Modified files:**\n- `internal/server/routes.go` (add 2 routes)\n- `CLAUDE.md` (document endpoints in ## Key Routes)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock all dependencies (Redis, PostgreSQL, clock)\n- Test timeout behavior\n- Test error message format\n- 100% coverage of handlers_health.go\n\n**Integration tests:**\n- Use testcontainers for real dependencies\n- Simulate Redis failure (stop container)\n- Simulate PostgreSQL failure\n- Verify 503 response format\n- Test recovery after restart\n\n**Manual testing:**\n- Deploy to staging\n- Configure load balancer with new endpoints\n- Verify traffic routing works\n- Kill Redis, verify instance removed from pool\n- Restart Redis, verify instance added back\n\n## Dependencies\n- None (first epic in observability track)\n\n## Success Metrics\n- Health checks complete in \u003c1 second p99\n- Zero false negatives (never returns 200 when actually unhealthy)\n- Zero false positives (never returns 503 when actually healthy)\n- Load balancer successfully removes unhealthy instances\n\n## Effort Estimate\n**5 developer-days** (1 week)\n\nBreakdown:\n- Handlers implementation: 1 day\n- Unit tests: 1 day\n- Integration tests: 1 day\n- Documentation: 1 day\n- Testing/refinement: 1 day\n\n## Risk Mitigation\n- **Risk:** Health check timeout too short, causes false positives\n  - **Mitigation:** 5s total timeout is conservative (1s per check + margin)\n- **Risk:** Redis/PostgreSQL libraries don't support context timeout\n  - **Mitigation:** Both libraries support context properly, verified in existing code\n- **Risk:** Load balancer polling interval conflicts with check timeout\n  - **Mitigation:** Document recommended polling intervals (5-10s)","status":"closed","priority":1,"issue_type":"epic","assignee":"dev-infrastructure","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:24:56.280136+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:07:39.175105+01:00","closed_at":"2026-02-12T18:07:39.175105+01:00","close_reason":"Implemented health check endpoints with comprehensive testing and documentation. All acceptance criteria met: /health/live (liveness), /health/ready (readiness with Redis/PostgreSQL/Redis Functions checks), 5s timeout, unit tests (11), integration tests (3), load balancer config docs for 4+ platforms, CLAUDE.md updated. Ready for production deployment."}
{"id":"twitch-tow-6gu","title":"Add context timeout to Redis operations in Broadcaster","description":"**High Priority (Resource Management)**\n\nLocation: internal/broadcast/broadcaster.go line 184\n\nIssue: GetCurrentValue calls Redis functions with a context that has no timeout. If Redis is slow or hung, handleTick() blocks forever, stalling the broadcaster.\n\nImpact: Can cause cascading timeouts and goroutine hangs. The entire broadcaster stops updating all overlays.\n\nFix:\n- Replace context.Background() with context.WithTimeout (e.g., 2-5 seconds)\n- Handle context.DeadlineExceeded errors gracefully\n- Log timeouts at warning level\n- Consider circuit breaker pattern for repeated failures","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:25:55.88565+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:42:33.732114+01:00","closed_at":"2026-02-12T16:42:33.732114+01:00","close_reason":"FIXED: Added per-session 2-second timeout to Redis operations in Broadcaster.handleTick(). Each session's GetCurrentValue call gets its own context.WithTimeout, preventing slow/hung Redis from blocking the broadcaster's tick loop. Timeout errors are logged as WARNING. Added test TestBroadcaster_RedisTimeoutHandling to verify broadcaster remains responsive during Redis delays. All tests pass with race detector clean."}
{"id":"twitch-tow-6hl","title":"EPIC: Redis Circuit Breaker - Implement Graceful Degradation for Redis Failures","description":"## Epic Overview\nImplement circuit breaker pattern around Redis client to prevent Redis failures from cascading and crashing the application. Enables graceful degradation with stale data or controlled errors.\n\n## User Story\nAs an operator, when Redis becomes unavailable or slow, I need the application to gracefully degrade rather than crash, serving stale data where acceptable and logging errors for investigation.\n\n## Parent Solution\ntwitch-tow-ojd (Redis Resilience via Sentinel + Read Replicas + Circuit Breaker)\n\n## Technical Requirements\n\n### Circuit Breaker Pattern\nUse **github.com/sony/gobreaker** library with these settings:\n- **Max failures:** 5 consecutive errors before opening\n- **Timeout:** 10 seconds in open state before half-open\n- **Success threshold:** 3 successful requests to close from half-open\n\n### Graceful Degradation Strategy\nDifferent components handle open circuit differently:\n\n1. **Broadcaster (tick loop):** Return last known value (stale but acceptable)\n2. **Vote processing:** Log error, return 0 (votes are not critical)\n3. **Session activation:** Return error to client (trigger retry logic)\n4. **Config reads:** Return cached config (implement local cache in future epic)\n\n### Circuit Breaker States\n- **Closed:** Normal operation, requests pass through\n- **Open:** Too many failures, all requests fail fast (no Redis calls)\n- **Half-open:** Testing if Redis recovered, limited requests allowed\n\n## Implementation Tasks\n\n### Task 1: Add gobreaker dependency\n```bash\ngo get github.com/sony/gobreaker@v1.0.0\n```\n\nUpdate `go.mod` and `go.sum`\n\n### Task 2: Create circuit breaker wrapper\n**File:** `internal/redis/circuit_breaker.go` (NEW)\n```go\npackage redis\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n    \n    \"github.com/redis/go-redis/v9\"\n    \"github.com/sony/gobreaker\"\n)\n\ntype CircuitBreakerClient struct {\n    client *redis.Client\n    cb     *gobreaker.CircuitBreaker\n    \n    // For fallback behavior\n    lastKnownValues map[string]cachedValue\n    mu              sync.RWMutex\n}\n\ntype cachedValue struct {\n    value     string\n    timestamp time.Time\n}\n\nfunc NewCircuitBreakerClient(client *redis.Client) *CircuitBreakerClient {\n    settings := gobreaker.Settings{\n        Name:        \"redis\",\n        MaxRequests: 3,  // half-open: allow 3 requests\n        Interval:    60 * time.Second,  // rolling window\n        Timeout:     10 * time.Second,  // open duration\n        ReadyToTrip: func(counts gobreaker.Counts) bool {\n            failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)\n            return counts.Requests \u003e= 5 \u0026\u0026 failureRatio \u003e= 0.6\n        },\n        OnStateChange: func(name string, from, to gobreaker.State) {\n            slog.Warn(\"Circuit breaker state changed\",\n                \"component\", name,\n                \"from\", from,\n                \"to\", to,\n            )\n            \n            // Emit metric\n            metrics.CircuitBreakerStateChanges.WithLabelValues(name, to.String()).Inc()\n        },\n    }\n    \n    return \u0026CircuitBreakerClient{\n        client:          client,\n        cb:              gobreaker.NewCircuitBreaker(settings),\n        lastKnownValues: make(map[string]cachedValue),\n    }\n}\n\n// Wrap critical Redis methods\nfunc (c *CircuitBreakerClient) Get(ctx context.Context, key string) (string, error) {\n    result, err := c.cb.Execute(func() (interface{}, error) {\n        return c.client.Get(ctx, key).Result()\n    })\n    \n    if err == gobreaker.ErrOpenState {\n        // Circuit open, try fallback\n        if cached := c.getCached(key); cached != \"\" {\n            slog.Debug(\"Circuit open, returning cached value\", \"key\", key)\n            return cached, nil\n        }\n        return \"\", fmt.Errorf(\"redis unavailable and no cached value: %w\", err)\n    }\n    \n    if err == nil {\n        // Success, cache the value\n        c.setCached(key, result.(string))\n    }\n    \n    return result.(string), err\n}\n\nfunc (c *CircuitBreakerClient) FCallRO(ctx context.Context, function string, keys []string, args ...interface{}) *redis.Cmd {\n    // For read-only functions, allow fallback\n    result := c.cb.Execute(func() (interface{}, error) {\n        cmd := c.client.FCallRO(ctx, function, keys, args...)\n        return cmd, cmd.Err()\n    })\n    \n    if err := result.(error); err == gobreaker.ErrOpenState {\n        // Return last known value for sentiment reads\n        if function == \"get_decayed_value\" \u0026\u0026 len(keys) \u003e 0 {\n            if cached := c.getCached(keys[0]); cached != \"\" {\n                // Return cached sentiment value\n                return redis.NewCmd(ctx, \"FCALL_RO\", cached)\n            }\n        }\n        return redis.NewCmd(ctx, \"FCALL_RO\", err)\n    }\n    \n    return result.(*redis.Cmd)\n}\n\nfunc (c *CircuitBreakerClient) setCached(key, value string) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    c.lastKnownValues[key] = cachedValue{\n        value:     value,\n        timestamp: time.Now(),\n    }\n}\n\nfunc (c *CircuitBreakerClient) getCached(key string) string {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    if cached, ok := c.lastKnownValues[key]; ok {\n        // Only return if less than 5 minutes old\n        if time.Since(cached.timestamp) \u003c 5*time.Minute {\n            return cached.value\n        }\n    }\n    return \"\"\n}\n\n// Wrap all critical methods: HGet, HSet, FCall, Incr, Decr, etc.\n```\n\n### Task 3: Add circuit breaker metrics\n**File:** `internal/metrics/metrics.go`\n```go\nCircuitBreakerStateChanges = promauto.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: \"circuit_breaker_state_changes_total\",\n        Help: \"Circuit breaker state transitions\",\n    },\n    []string{\"component\", \"state\"},\n)\n\nCircuitBreakerState = promauto.NewGaugeVec(\n    prometheus.GaugeOpts{\n        Name: \"circuit_breaker_state\",\n        Help: \"Current circuit breaker state (0=closed, 1=half-open, 2=open)\",\n    },\n    []string{\"component\"},\n)\n```\n\n### Task 4: Integrate into Redis client creation\n**File:** `internal/redis/client.go`\n```go\nfunc NewClient(ctx context.Context, redisURL string, clock clockwork.Clock) (*redis.Client, error) {\n    opts, err := redis.ParseURL(redisURL)\n    if err != nil {\n        return nil, err\n    }\n    \n    baseClient := redis.NewClient(opts)\n    \n    // Test connection\n    if err := baseClient.Ping(ctx).Err(); err != nil {\n        return nil, fmt.Errorf(\"redis ping failed: %w\", err)\n    }\n    \n    // Load Lua functions\n    if err := loadChatPulseFunctions(ctx, baseClient); err != nil {\n        return nil, err\n    }\n    \n    // Wrap with circuit breaker\n    cbClient := NewCircuitBreakerClient(baseClient)\n    \n    return cbClient, nil\n}\n```\n\n### Task 5: Update sentiment store to handle circuit breaker errors\n**File:** `internal/redis/sentiment_store.go`\n```go\nfunc (s *SentimentStore) GetSentiment(ctx, sessionUUID, now) (float64, error) {\n    result := s.client.FCallRO(ctx, \"get_decayed_value\", \n        []string{fmt.Sprintf(\"session:%s\", sessionUUID)},\n        now.UnixMilli(),\n    )\n    \n    if err := result.Err(); err != nil {\n        if err == gobreaker.ErrOpenState {\n            // Circuit open, return neutral sentiment\n            slog.Warn(\"Circuit breaker open, returning neutral sentiment\",\n                \"session_uuid\", sessionUUID,\n            )\n            return 0.0, nil  // Neutral is safe fallback\n        }\n        return 0.0, err\n    }\n    \n    value, _ := result.Float64()\n    return value, nil\n}\n```\n\n### Task 6: Unit tests with circuit breaker simulation\n**File:** `internal/redis/circuit_breaker_test.go` (NEW)\nTest scenarios:\n1. **Normal operation:** Circuit stays closed\n2. **Transient failures:** Circuit stays closed (under threshold)\n3. **Sustained failures:** Circuit opens after 5 failures\n4. **Recovery:** Circuit half-opens after timeout, closes after 3 successes\n5. **Fallback behavior:** Cached values returned when circuit open\n6. **Cache expiry:** Old cached values (\u003e5 min) not returned\n\nMock Redis client that can fail on command:\n```go\ntype failingRedisClient struct {\n    failCount int\n    maxFails  int\n}\n\nfunc (c *failingRedisClient) Get(ctx, key) *redis.StringCmd {\n    c.failCount++\n    if c.failCount \u003c= c.maxFails {\n        return redis.NewStringResult(\"\", errors.New(\"connection refused\"))\n    }\n    return redis.NewStringResult(\"ok\", nil)\n}\n```\n\n### Task 7: Integration test with real Redis failures\n**File:** `internal/redis/circuit_breaker_integration_test.go` (NEW)\nTest scenario:\n1. Start testcontainers Redis\n2. Make 3 successful requests (circuit closed)\n3. Stop Redis container\n4. Make 5 requests (should fail, circuit opens)\n5. Make request (should fail fast, no Redis call)\n6. Verify cached value returned for reads\n7. Wait 10 seconds (circuit timeout)\n8. Restart Redis\n9. Make 3 requests (circuit half-open → closed)\n10. Verify normal operation resumed\n\n### Task 8: Documentation\n**File:** `docs/architecture/circuit-breaker.md` (NEW)\nDocument:\n- Circuit breaker pattern overview\n- State transitions diagram\n- Fallback behavior by component\n- Tuning parameters (failure threshold, timeout)\n- How to monitor circuit breaker state\n- Troubleshooting guide\n\n### Task 9: Update CLAUDE.md\nAdd circuit breaker pattern to ## Redis Architecture section\n\n## Acceptance Criteria\n\n- ✅ Circuit breaker wraps all Redis operations\n- ✅ Circuit opens after 5 consecutive failures within 60s\n- ✅ Circuit stays open for 10 seconds before half-open\n- ✅ Circuit closes after 3 successful requests in half-open\n- ✅ Cached values returned for reads when circuit open (\u003c5 min old)\n- ✅ Votes gracefully fail when circuit open (logged, not crash)\n- ✅ Metrics track circuit breaker state changes\n- ✅ Unit tests cover all state transitions\n- ✅ Integration test verifies real Redis failure handling\n- ✅ Application doesn't crash when Redis unavailable\n\n## Files Created/Modified\n\n**New files:**\n- `internal/redis/circuit_breaker.go` (300 lines)\n- `internal/redis/circuit_breaker_test.go` (400 lines unit tests)\n- `internal/redis/circuit_breaker_integration_test.go` (200 lines)\n- `docs/architecture/circuit-breaker.md` (300 lines)\n\n**Modified files:**\n- `internal/redis/client.go` (wrap client with circuit breaker)\n- `internal/redis/sentiment_store.go` (handle ErrOpenState)\n- `internal/metrics/metrics.go` (add circuit breaker metrics)\n- `go.mod` (add github.com/sony/gobreaker v1.0.0)\n- `CLAUDE.md` (document circuit breaker in Redis Architecture)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock Redis client with controlled failures\n- Test all state transitions\n- Test fallback behavior\n- Test cache expiry logic\n- 100% coverage of circuit_breaker.go\n\n**Integration tests:**\n- Use testcontainers Redis\n- Stop/start container to simulate failures\n- Verify graceful degradation\n- Verify recovery after Redis restart\n- Test with concurrent requests\n\n**Chaos testing:**\n- Deploy to staging\n- Randomly kill Redis with tc (traffic control) or iptables\n- Verify application stays up\n- Verify overlays show stale data (acceptable)\n- Verify automatic recovery\n\n## Dependencies\n- **Package:** github.com/sony/gobreaker v1.0.0\n- **Blocks:** None (independent feature)\n\n## Success Metrics\n- Zero application crashes due to Redis failures\n- Circuit breaker opens within 10 seconds of Redis failure\n- Stale data served for \u003c5 minutes during outage\n- Automatic recovery within 30 seconds of Redis restart\n- No manual intervention required\n\n## Effort Estimate\n**5 developer-days** (1 week)\n\nBreakdown:\n- Circuit breaker wrapper: 1.5 days\n- Integration: 1 day\n- Unit tests: 1 day\n- Integration tests: 1 day\n- Documentation: 0.5 day\n\n## Risk Mitigation\n- **Risk:** Circuit breaker false positives (opens when Redis healthy)\n  - **Mitigation:** Failure ratio threshold (60% failures over 60s window)\n  - **Mitigation:** Tune based on production data\n- **Risk:** Stale data causes incorrect behavior\n  - **Mitigation:** 5-minute TTL on cached values\n  - **Mitigation:** Only cache read-only operations\n- **Risk:** Cache memory leak (unbounded growth)\n  - **Mitigation:** LRU eviction policy (future enhancement)\n  - **Mitigation:** 5-minute TTL prevents indefinite growth","status":"closed","priority":1,"issue_type":"epic","assignee":"dev-backend","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:26:56.772061+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:16:36.017005+01:00","closed_at":"2026-02-12T18:16:36.017005+01:00","close_reason":"Implemented Redis circuit breaker with hooks-based architecture. Provides graceful degradation when Redis fails: caches read values (5min TTL), returns neutral sentiment for reads, fails fast on writes. Comprehensive testing: 13 unit tests + 3 integration tests with real Redis failures. Metrics track circuit state. Documentation complete. All acceptance criteria met. Build successful."}
{"id":"twitch-tow-6hx","title":"Discussion: Missing observability and metrics for production","description":"Application has no built-in observability for monitoring performance and scalability issues at runtime.\n\nMissing metrics:\n\nRedis operations:\n- Call latency (p50, p95, p99)\n- Operation count by type (HGET, FCALL, INCR, etc)\n- Error rate\n- Connection pool stats\n\nBroadcaster:\n- Active sessions per instance\n- Clients per session distribution\n- Tick duration (should be under 50ms)\n- Slow client disconnect rate\n- Command channel depth\n\nWebSocket:\n- Total active connections\n- Connection rate (connects/disconnects per sec)\n- Send buffer full events\n- Upgrade failures\n- Per-session connection count\n\nDatabase:\n- Query latency by query type\n- Connection pool utilization (active/idle/waiting)\n- Transaction rate\n- Error rate by query\n\nVote processing:\n- Vote rate (votes/sec)\n- Debounce hit rate\n- Trigger match rate\n- Invalid broadcaster rate\n\nSession lifecycle:\n- Activation rate\n- Cleanup rate (orphans deleted)\n- Session duration histogram\n- Ref count distribution\n\nApplication:\n- Goroutine count\n- Memory usage (heap, stack, total)\n- GC pause time\n- CPU usage\n\nCurrent state: Only structured logging (slog)\nNo metrics, no dashboards, no alerting\n\nImpact:\n- Cannot detect performance degradation\n- No visibility into Redis bottleneck\n- Cannot measure horizontal scaling effectiveness\n- Hard to troubleshoot production issues\n\nRecommendation:\n\nPhase 1: Add Prometheus metrics\n1. Use prometheus/client_golang\n2. Add /metrics endpoint\n3. Instrument critical paths:\n   - redis_operations_total{operation, status}\n   - redis_operation_duration_seconds{operation}\n   - broadcaster_active_sessions\n   - websocket_connections_total\n   - vote_processing_total{result}\n\nPhase 2: Add tracing (OpenTelemetry)\n- Distributed tracing for vote pipeline\n- Track latency across components\n\nPhase 3: Dashboards + alerts\n- Grafana dashboard for key metrics\n- Alert on error rate spikes\n- Alert on Redis latency over SLO\n\nPriority: P1 - cannot operate at scale without observability\n","notes":"RESOLVED: This discussion is addressed by the consolidated observability solution.\n\n**Implemented in:**\n- **twitch-tow-php** (CONSOLIDATED: Comprehensive Observability - Health Checks + Metrics + Version Endpoint)\n  - Phase 1: Health checks + /version endpoint (week 1)\n  - Phase 2: Prometheus metrics (week 2-3) - **22 metrics covering all components mentioned in this discussion**\n  - Phase 3: Structured logging improvements (week 4)\n  - Phase 4: Distributed tracing (deferred)\n\n**How the solution addresses this:**\nAll requested metrics from this discussion are implemented:\n\n**Redis operations:** ✅\n- redis_operations_total{operation, status}\n- redis_operation_duration_seconds{operation}  \n- redis_connection_errors_total\n\n**Broadcaster:** ✅\n- broadcaster_active_sessions\n- broadcaster_connected_clients_total\n- broadcaster_tick_duration_seconds\n- broadcaster_slow_clients_evicted_total\n\n**WebSocket:** ✅\n- websocket_connections_current\n- websocket_connections_total{result}\n- websocket_message_send_duration_seconds\n- websocket_connection_duration_seconds\n\n**Database:** ✅\n- db_query_duration_seconds{query}\n- db_connections_current{state}\n- db_errors_total{query}\n\n**Vote processing:** ✅\n- vote_processing_total{result}\n- vote_processing_duration_seconds\n- vote_trigger_matches_total{trigger_type}\n\n**Also covered by epics:**\n- twitch-tow-682 (Phase 1: Health Checks implementation)\n- twitch-tow-kgj (Phase 2: Prometheus Metrics implementation)\n\nThe /metrics endpoint, Grafana dashboards, and alert rules are all included in the consolidated solution.","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:54.071453+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:35:56.746132+01:00","closed_at":"2026-02-12T17:35:56.746134+01:00"}
{"id":"twitch-tow-6oh","title":"Audit webhook HMAC verification implementation","description":"**Medium Priority (Security)**\n\nLocation: internal/twitch/webhook.go lines 23-26\n\nIssue: HMAC verification handled by Kappopher library. No defense-in-depth verification layer.\n\nImpact: Security vulnerability if library has bugs.\n\nFix:\n- Audit the Kappopher library's HMAC verification code\n- Consider adding secondary verification layer\n- Add integration tests that verify HMAC rejection\n- Verify actual implementation beyond existing tests","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:06.559451+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:40:07.521242+01:00","closed_at":"2026-02-12T16:40:07.521242+01:00","close_reason":"Closed"}
{"id":"twitch-tow-6vk","title":"Example: ADR-001 Redis-only architecture","description":"EXAMPLE ADR - This shows what a completed ADR would look like\n\n# ADR-001: Redis-only architecture for session state\n\nStatus: Accepted\nDate: 2026-02-12\nDeciders: Architecture team (Scalability, Resilience, Maintainability)\n\n## Context\n\nChatPulse needs to support horizontal scaling across multiple instances to handle thousands of concurrent streamers. Each instance runs an HTTP server, WebSocket broadcaster, and vote processing pipeline. The system must maintain shared state for:\n\n- Active sessions (which streamers are live)\n- Sentiment values (current bar position per session)\n- Session configuration (triggers, labels, decay rate)\n- Client connections (who is watching which overlay)\n\nKey constraints:\n- Instances can be added/removed dynamically (cloud auto-scaling)\n- No instance affinity (any instance can serve any session)\n- WebSocket connections are sticky to instance but session state is not\n- Sub-100ms latency requirement for vote processing\n- Tolerance for eventual consistency (sentiment is approximate, not transactional)\n\nTraditional approaches:\n- Sticky sessions: Ties clients to specific instances, limits scaling\n- In-memory with sync: Complex distributed state management, consistency challenges\n- Database for everything: Too slow for 50ms broadcast tick loop\n\n## Decision\n\n**All session state lives in Redis. Zero in-memory session state per instance.**\n\nSpecifically:\n- Session data stored in Redis hashes (value, config, timestamps)\n- Broadcaster reads from Redis on every 50ms tick\n- Vote processing writes directly to Redis (atomic Lua functions)\n- Instances coordinate via Redis primitives (INCR/DECR for ref counting)\n- PostgreSQL stores durable config and user data (source of truth)\n- Redis is ephemeral cache - losable on restart with cold-start from DB\n\nRedis key schema:\n- session:{uuid} - hash with value, config, timestamps\n- broadcaster:{twitch_id} - string mapping broadcaster to session UUID\n- ref_count:{uuid} - integer tracking how many instances serve this session\n\n## Alternatives Considered\n\n### A. Sticky sessions via load balancer\n- Pros: Simple, no distributed state\n- Cons: Uneven load distribution, reconnect disrupts session, limits horizontal scaling\n- Rejected: Violates stateless instance requirement\n\n### B. In-memory state with Redis Pub/Sub sync\n- Pros: Fast local reads, lower Redis load\n- Cons: Complex sync logic, message loss on network partition, memory per instance\n- Rejected: Adds complexity without significant performance benefit (Redis is fast enough)\n\n### C. All state in PostgreSQL\n- Pros: ACID guarantees, single source of truth\n- Cons: 10-50ms query latency, connection pool limits, 50ms tick loop infeasible\n- Rejected: Too slow for broadcast tick frequency\n\n### D. Hybrid: Hot state in memory, sync via CDC/event stream\n- Pros: Fast reads, eventual consistency\n- Cons: Very complex, dual-write consistency, CDC infrastructure\n- Rejected: Over-engineered for current scale\n\n## Consequences\n\n### Positive\n- **Horizontal scaling**: Add instances freely, state in Redis\n- **Stateless instances**: Any instance can serve any request, simple load balancing\n- **Fast operations**: Redis 1-5ms latency vs 10-50ms PostgreSQL\n- **Simple failure handling**: Instance crash has no state loss (Redis survives)\n- **Ref counting works**: Redis INCR/DECR enables cross-instance coordination\n- **Atomic operations**: Lua functions provide atomicity for vote + decay\n\n### Negative\n- **Redis is SPOF**: Single Redis instance failure = total outage (mitigated by Sentinel in production)\n- **Memory cost**: All sessions fit in RAM (at 10K sessions × 1KB = 10MB, acceptable)\n- **Eventual consistency**: Slight staleness acceptable (50ms tick loop)\n- **No strong consistency**: Ref counting has race conditions (30s grace period mitigates)\n- **Redis as critical path**: Every vote and broadcast reads/writes Redis\n\n### Neutral\n- **Cold start penalty**: First connection requires DB fetch (user + config), then cached in Redis\n- **Cleanup complexity**: Orphan sessions need periodic scanning and deletion (30s grace period)\n- **Redis HA required**: Must deploy Sentinel or Cluster for production (acceptable operational cost)\n- **Network dependency**: All instances depend on Redis network availability\n\n## Notes\n\nThis decision enables the stateless instance requirement (ADR-005) and makes ref counting (ADR-006) possible. It also drives the pull-based broadcaster design (ADR-002) since pulling from Redis every 50ms is feasible.\n\nRedis Sentinel (HA) will be added in Phase 2 (see twitch-tow-hdl). Read replicas may be added if broadcaster tick load exceeds master capacity.\n\nRelated decisions: ADR-002 (Pull-based broadcasting), ADR-005 (No sticky sessions), ADR-006 (Ref counting), ADR-007 (Database vs Redis separation)\n\n## References\n- Original discussion: CLAUDE.md Architecture section\n- Implementation: internal/redis/session_repository.go\n- Tests: internal/redis/session_repository_integration_test.go","notes":"Feedback from architect-resilience:\n\n**EXCELLENT QUALITY** - This sets the perfect bar for the remaining ADRs.\n\n## What Works Well\n\n✅ **Context is clear**: Explains the problem, constraints, and why traditional approaches don't fit\n✅ **Decision is concrete**: Specific key schema, not just high-level \"use Redis\"\n✅ **4 alternatives analyzed**: Each with pros/cons and rejection rationale\n✅ **Consequences framework**: 6 positive, 5 negative, 4 neutral - comprehensive trade-off analysis\n✅ **Cross-references**: Links to related ADRs (002, 005, 006, 007) show dependencies\n✅ **Implementation links**: Points to actual code (session_repository.go) for traceability\n✅ **Living document**: Status can evolve (Accepted → Deprecated)\n✅ **Right length**: ~900 words, ~1 page, readable in 5 minutes\n\n## Specific Strengths\n\n**Context section:**\n- Key constraints listed explicitly (no affinity, sub-100ms latency, eventual consistency OK)\n- Traditional approaches dismissed with reasoning\n- Sets up decision rationale perfectly\n\n**Alternatives section:**\n- 4 options (sticky sessions, pub/sub sync, all-Postgres, hybrid CDC)\n- Each has clear pros/cons\n- Rejection reason stated explicitly\n\n**Consequences split:**\n- Positive: Highlights scaling benefits\n- Negative: Honest about SPOF risk, eventual consistency\n- Neutral: Operational impacts (cold start, cleanup, HA requirement)\n\n**Notes section:**\n- Shows decision dependencies (enables ADR-005, makes ADR-006 possible)\n- References future work (Sentinel in Phase 2)\n- Links to related ADRs\n\n## Minor Suggestions\n\n**1. Add Monitoring \u0026 Validation section** (from my earlier feedback):\n```\n## Monitoring \u0026 Validation\n- Monitor: redis_operation_duration_seconds (detect degradation)\n- Alert: redis_connection_errors_total rate \u003e 5/min\n- Validate: Multi-instance deployment shares state correctly\n- Metric: session_cold_start_duration_seconds (track DB fetch penalty)\n```\n\nRationale: Helps operators validate that the decision is working in production.\n\n**2. Consider adding \"When to revisit\" criteria:**\n```\n## When to Revisit This Decision\n- Redis becomes throughput bottleneck (\u003e80% CPU)\n- Memory cost exceeds $500/month\n- Strong consistency becomes requirement (e.g., payment processing)\n- Network latency to Redis exceeds 10ms p99\n```\n\nRationale: Guides future architects on when to reconsider this decision.\n\n## Answers to Your Questions\n\n**Q: Does this meet expectations for ADR quality?**\n✅ YES - This is exactly the right level of detail. Comprehensive but not overwhelming.\n\n**Q: Too detailed? Too brief?**\n✅ PERFECT - 900 words is the sweet spot. More would be too long, less would miss key details.\n\n**Q: Would you change anything?**\n- Add \"Monitoring \u0026 Validation\" section (minor addition, 3-4 lines)\n- Optionally add \"When to revisit\" criteria (nice-to-have, not critical)\n- Otherwise: No changes needed - excellent as-is\n\n## Effort Validation\n\n**45 minutes actual** vs **1 hour estimate** = ✅ Estimate is accurate\n\nWith this example as reference, subsequent ADRs should be faster (copy template, fill in specifics). Estimate holds.\n\n## Parallelization Plan - APPROVED\n\nYour proposed ownership:\n- ✅ Maintainability: ADR-011, 012, 013\n- ✅ Scalability: ADR-004, 005, 007\n- ✅ Resilience: ADR-008, 009, 010\n\nI can additionally own:\n- ADR-016 (Observability) - I provided SLO targets\n- ADR-017 (Disaster recovery) - NEW, I proposed\n- ADR-018 (WebSocket limits) - NEW, I proposed\n- ADR-019 (No retries) - NEW, I proposed\n\nTotal my ownership: 7 ADRs (including 3 new ones from my additions)\n\n## Ready to Proceed\n\nConsensus: This example sets the perfect quality bar. Let's parallelize and write the remaining ADRs using this template.\n\nTimeline: With 3 architects writing in parallel, we can complete all 19 ADRs in 1 week (instead of 4 weeks sequential).","status":"open","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:17:34.307785+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:31:43.690616+01:00"}
{"id":"twitch-tow-6zq","title":"EPIC: Standardize Test File Organization with Clear Integration Test Naming","description":"Standardize test file naming to clearly distinguish unit tests from integration tests using consistent _integration_test.go suffix.\n\n## User Story\nAs a developer running tests, I want clear test file naming so I can easily run only unit tests (fast feedback) or only integration tests (full coverage) without hunting through code.\n\n## Value Proposition  \n- Consistent naming enables targeted test execution\n- Clear distinction between unit (fast) and integration (slow) tests\n- Easier CI optimization (run unit tests first, integration in parallel)\n- Better developer experience (know which tests need Docker/testcontainers)\n\n## Background\n\n**Current state (twitch-tow-op7):**\n- **Pattern 1:** redis/ uses `_integration_test.go` suffix ✅\n- **Pattern 2:** database/ uses plain `_test.go` (but all are integration tests) ❌\n- **Pattern 3:** app/ mixes unit + integration in `service_test.go` ❌\n- **Workaround:** `-short` flag skips integration tests via `testing.Short()` check\n\n**Inconsistency problems:**\n1. Hard to run ONLY integration tests (can't glob `*_integration_test.go`)\n2. Some packages have clear split, others don't\n3. Naming doesn't indicate which tests need testcontainers\n\n## Tasks\n\n### 1. Audit current test file organization\n\n**Scan all test files:**\n```bash\nfind . -name \"*_test.go\" -type f | grep -v vendor | sort\n```\n\n**Categorize by pattern:**\n- Unit tests only (use mocks, no external deps)\n- Integration tests only (use testcontainers, real Redis/PostgreSQL)\n- Mixed (both unit + integration in same file)\n\n**Document findings:**\n| Package | File | Type | Needs Rename? |\n|---------|------|------|---------------|\n| redis | client_integration_test.go | Integration | ✅ Already good |\n| redis | session_repository_integration_test.go | Integration | ✅ Already good |\n| database | postgres_test.go | Integration | ❌ Rename to postgres_integration_test.go |\n| database | user_repository_test.go | Integration | ❌ Rename to user_repository_integration_test.go |\n| app | service_test.go | Mixed | ❌ Split or add testing.Short() |\n| sentiment | engine_test.go | Unit | ✅ Already good |\n| ... | ... | ... | ... |\n\n**Files to document:**\n- Create `TEST_ORGANIZATION.md` (audit results)\n\n**Time estimate:** 30 minutes\n\n---\n\n### 2. Standardize integration test naming\n\n**Decision:** Use `_integration_test.go` suffix for ALL integration tests\n\n**Rationale:**\n- Already used in redis/ package (precedent)\n- Clear semantic meaning (not just \"test\")\n- Enables glob patterns: `go test ./... -run='^Test.*' ./internal/redis/sentiment_store_integration_test.go\n./internal/redis/debouncer_integration_test.go\n./internal/redis/client_integration_test.go\n./internal/redis/session_repository_integration_test.go`\n- Consistent with Go community practice\n\n**Rename files:**\n```bash\n# database package\ngit mv internal/database/postgres_test.go internal/database/postgres_integration_test.go\ngit mv internal/database/user_repository_test.go internal/database/user_repository_integration_test.go\ngit mv internal/database/config_repository_test.go internal/database/config_repository_integration_test.go\ngit mv internal/database/eventsub_repository_test.go internal/database/eventsub_repository_integration_test.go\n\n# Any others found in audit\n```\n\n**Files to rename:**\n- `internal/database/postgres_test.go` → `postgres_integration_test.go`\n- `internal/database/user_repository_test.go` → `user_repository_integration_test.go`\n- `internal/database/config_repository_test.go` → `config_repository_integration_test.go`\n- `internal/database/eventsub_repository_test.go` → `eventsub_repository_integration_test.go`\n\n**Time estimate:** 15 minutes\n\n---\n\n### 3. Update CLAUDE.md with test file conventions\n\n**Add to Testing section:**\n\n```markdown\n## Test File Organization\n\n### Naming Conventions\n\n**Unit tests:** `\u003cpackage\u003e_test.go` or `\u003cfeature\u003e_test.go`\n- Use mocks for all external dependencies\n- Fast execution (\u003c100ms per test)\n- No Docker, no testcontainers\n- Example: `internal/sentiment/engine_test.go`\n\n**Integration tests:** `\u003cfeature\u003e_integration_test.go`\n- Use real infrastructure (PostgreSQL, Redis via testcontainers)\n- Slower execution (100ms-1s per test)\n- Require Docker daemon running\n- Example: `internal/redis/session_repository_integration_test.go`\n\n### Running Tests\n\n**All tests (unit + integration):**\n```bash\nmake test\n# OR\ngo test ./...\n```\n\n**Unit tests only (fast feedback):**\n```bash\nmake test-short\n# OR\ngo test -short ./...\n```\n\n**Integration tests only:**\n```bash\n# Run all files matching *_integration_test.go\ngo test ./internal/redis\n```\n\n**Single package:**\n```bash\ngo test ./internal/redis\ngo test -short ./internal/app  # unit tests only\n```\n\n### Mixed Test Files\n\nSome packages (e.g., `app/service_test.go`) contain BOTH unit and integration tests:\n- Integration tests check `testing.Short()` and skip if `-short` flag set\n- This is acceptable but prefer separate files for clarity\n\n**Example pattern:**\n```go\nfunc TestIntegrationFeature(t *testing.T) {\n    if testing.Short() {\n        t.Skip(\"Skipping integration test\")\n    }\n    // Integration test code...\n}\n```\n\n### Test Infrastructure Files\n\n**Shared test helpers:** `\u003cpackage\u003e_test.go` (not `_integration_test.go`)\n- Example: `internal/server/handlers_test.go` contains mocks + setup helpers\n- Used by both unit and integration tests\n\n**TestMain setup:** Always in `\u003cpackage\u003e_integration_test.go` or dedicated file\n- Example: `internal/database/postgres_integration_test.go` has `TestMain`\n- Sets up testcontainers once per package\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Test File Organization section under Testing)\n\n**Time estimate:** 30 minutes\n\n---\n\n### 4. Add make target for integration-only tests\n\n**Update Makefile:**\n```makefile\n.PHONY: test-integration\ntest-integration:  ## Run integration tests only\n\\t@echo \"Running integration tests...\"\n\\t@go test -v \n\n.PHONY: test-unit\ntest-unit: test-short  ## Alias for test-short (unit tests only)\n```\n\n**Update help target to show new commands:**\n```makefile\nhelp:\n\\t@grep -E '^[a-zA-Z_-]+:.*?## .*31188'  | awk 'BEGIN {FS = \":.*?## \"}; {printf \"\\033[36m%-20s\\033[0m %s\\n\", 311881, 311882}'\n```\n\n**Files to modify:**\n- `Makefile` (add test-integration + test-unit targets)\n\n**Time estimate:** 15 minutes\n\n---\n\n### 5. Document convention in README.md\n\n**Add quick reference to README:**\n\n```markdown\n## Testing\n\n```bash\nmake test              # All tests (unit + integration, ~15s)\nmake test-short        # Unit tests only (\u003c2s)\nmake test-integration  # Integration tests only (~12s)\nmake test-coverage     # Generate coverage report\n```\n\n**Test types:**\n- **Unit tests:** Fast, use mocks, no Docker required\n- **Integration tests:** Slower, use testcontainers (PostgreSQL + Redis)\n\nSee [CLAUDE.md Testing section](./CLAUDE.md#testing) for details.\n```\n\n**Files to modify:**\n- `README.md` (add testing quick reference)\n\n**Time estimate:** 10 minutes\n\n---\n\n## Acceptance Criteria\n\n- ✅ All integration tests use `_integration_test.go` suffix\n- ✅ `make test-integration` runs only integration tests\n- ✅ `make test-short` runs only unit tests (existing)\n- ✅ CLAUDE.md documents naming conventions\n- ✅ README.md has quick testing reference\n- ✅ No mixed-pattern confusion (consistent across all packages)\n\n## Files Changed\n\n**Renamed:**\n- `internal/database/postgres_test.go` → `postgres_integration_test.go`\n- `internal/database/user_repository_test.go` → `user_repository_integration_test.go`\n- `internal/database/config_repository_test.go` → `config_repository_integration_test.go`\n- `internal/database/eventsub_repository_test.go` → `eventsub_repository_integration_test.go`\n\n**Modified:**\n- `Makefile` (add test-integration + test-unit targets)\n- `CLAUDE.md` (add Test File Organization section)\n- `README.md` (add testing quick reference)\n\n**Created:**\n- (Optional) `TEST_ORGANIZATION.md` (audit results, can be temporary)\n\n## Dependencies\n- None (organizational refactoring)\n\n## Effort Estimate\n**Total: 1.5 hours** (90 minutes)\n- Audit (30min): Document current test file patterns\n- Rename (15min): Move files to `_integration_test.go`\n- Makefile (15min): Add test-integration target\n- Documentation (30min): Update CLAUDE.md + README.md\n- Verification (10min): Run all 3 test modes, ensure no regressions\n\n## Success Metrics\n- Developers can easily run integration-only tests (`make test-integration`)\n- Clear distinction in file names (grep `_integration_test.go` = all integration tests)\n- No confusion about which tests need Docker","status":"open","priority":3,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":90,"created_at":"2026-02-12T17:47:23.956912+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:50.306656+01:00"}
{"id":"twitch-tow-72d","title":"Add session regeneration after OAuth login","description":"**Context**: Session fixation attack mitigation best practice.\n\n**Current state**: Session uses the same session ID before and after authentication. An attacker could fixate a user's session ID before login and hijack it after authentication.\n\n**Security impact**: LOW-MEDIUM - Requires attacker to already have access to set cookies on the user's browser (XSS or network interception).\n\n**Implementation**:\n1. After successful OAuth callback in handlers_auth.go\n2. Before storing userID in session\n3. Call session.Options.MaxAge = -1 to delete old session\n4. Create new session with fresh ID\n5. Store userID in new session\n\n**Location**: internal/server/handlers_auth.go (handleOAuthCallback)\n\n**Reference**: OWASP Session Management Cheat Sheet - Session ID regeneration on privilege level change\n\n**Note**: Already protected by:\n- HTTPS (prevents network interception)\n- HttpOnly cookies (prevents XSS cookie theft)\n- SameSite=Lax (limits CSRF cookie usage)\nBut regeneration adds defense-in-depth.","status":"closed","priority":3,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:42:50.762685+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:52:58.328759+01:00","closed_at":"2026-02-12T16:52:58.328759+01:00","close_reason":"Closed"}
{"id":"twitch-tow-749","title":"EPIC: Configuration Tuning - Externalize Hardcoded Constants","description":"Extract hardcoded constants into environment-configurable settings for production tuning. Enables operational flexibility without code changes.\n\n## User Story\nAs an operator running ChatPulse in production, I need to tune performance parameters (timeouts, limits, intervals) based on observed metrics without redeploying code.\n\n## Value Proposition\n- Production tuning without code changes or recompilation\n- A/B testing of performance parameters (compare 50ms vs 100ms tick intervals)\n- Environment-specific settings (dev = fast timeouts, prod = longer)\n- Easier load testing (crank up limits, measure breaking points)\n\n## Background\n\n**Problem identified (twitch-tow-23o):**\nConstants scattered across 5 packages, all hardcoded:\n- Broadcast: maxClientsPerSession=50, tickInterval=50ms, redisTimeout=2s\n- Writer: writeDeadline=5s, pingInterval=30s, pongDeadline=60s\n- App: orphanMaxAge=30s, cleanupInterval=30s\n- Server: sessionMaxAgeDays=7\n- Twitch: appTokenTimeout=15s\n\n**Current issues:**\n1. Can't tune without recompile\n2. Inconsistent units (ms, s, days)\n3. Can't A/B test parameter changes\n4. Test code can't speed up timeouts (relies on clockwork for some)\n\n## Tasks\n\n### 1. Categorize constants by tunability\n\n**Three categories:**\n\n**Category A: Production-tunable (externalize to env vars)**\n- `maxClientsPerSession` = 50 → Can vary by instance size\n- `tickInterval` = 50ms → May need tuning based on Redis latency\n- `cleanupInterval` = 30s → May need tuning based on session churn\n- `sessionMaxAgeDays` = 7 → Business decision, may change\n- `orphanMaxAge` = 30s → May need tuning based on ref count races\n\n**Category B: Performance-critical (keep hardcoded, document in ADR)**\n- `writeDeadline` = 5s → WebSocket write timeout (well-tuned)\n- `pingInterval` = 30s → WebSocket heartbeat (RFC standard)\n- `pongDeadline` = 60s → WebSocket timeout (2× ping interval)\n- `redisTimeout` = 2s → Circuit breaker threshold (coordinated with CB)\n\n**Category C: Protocol constants (keep hardcoded)**\n- `messageBufferSize` = 16 → Goroutine channel buffer (premature optimization)\n- `appTokenTimeout` = 15s → Twitch API timeout (their SLA)\n\n**Files to analyze:**\n- `internal/broadcast/broadcaster.go`\n- `internal/broadcast/writer.go`\n- `internal/app/service.go`\n- `internal/server/server.go`\n- `internal/twitch/eventsub.go`\n\n**Time estimate:** 1 hour (categorization + rationale)\n\n---\n\n### 2. Add tunable constants to Config struct\n\n**Update `internal/config/config.go`:**\n\n```go\ntype Config struct {\n    // Existing fields...\n    DatabaseURL           string `env:\"DATABASE_URL,required\"`\n    RedisURL              string `env:\"REDIS_URL,required\"`\n    \n    // NEW: Tunable performance parameters\n    BroadcasterMaxClientsPerSession int           `env:\"BROADCASTER_MAX_CLIENTS_PER_SESSION\" envDefault:\"50\"`\n    BroadcasterTickIntervalMs       int           `env:\"BROADCASTER_TICK_INTERVAL_MS\" envDefault:\"50\"`\n    CleanupIntervalSeconds          int           `env:\"CLEANUP_INTERVAL_SECONDS\" envDefault:\"30\"`\n    OrphanMaxAgeSeconds             int           `env:\"ORPHAN_MAX_AGE_SECONDS\" envDefault:\"30\"`\n    SessionMaxAgeDays               int           `env:\"SESSION_MAX_AGE_DAYS\" envDefault:\"7\"`\n}\n\n// Helper methods for duration conversion\nfunc (c *Config) BroadcasterTickInterval() time.Duration {\n    return time.Duration(c.BroadcasterTickIntervalMs) * time.Millisecond\n}\n\nfunc (c *Config) CleanupInterval() time.Duration {\n    return time.Duration(c.CleanupIntervalSeconds) * time.Second\n}\n\nfunc (c *Config) OrphanMaxAge() time.Duration {\n    return time.Duration(c.OrphanMaxAgeSeconds) * time.Second\n}\n\nfunc (c *Config) SessionMaxAge() time.Duration {\n    return time.Duration(c.SessionMaxAgeDays) * 24 * time.Hour\n}\n```\n\n**Update `.env.example`:**\n```bash\n# Performance Tuning (optional, defaults shown)\nBROADCASTER_MAX_CLIENTS_PER_SESSION=50\nBROADCASTER_TICK_INTERVAL_MS=50\nCLEANUP_INTERVAL_SECONDS=30\nORPHAN_MAX_AGE_SECONDS=30\nSESSION_MAX_AGE_DAYS=7\n```\n\n**Files to modify:**\n- `internal/config/config.go` (add fields + helper methods)\n- `.env.example` (document new vars)\n\n**Time estimate:** 1 hour\n\n---\n\n### 3. Update consumers to use Config\n\n**Broadcaster changes:**\n\n**Before (`internal/broadcast/broadcaster.go`):**\n```go\nconst (\n    maxClientsPerSession = 50\n    tickInterval = 50 * time.Millisecond\n)\n\nfunc NewBroadcaster(...) *Broadcaster {\n    ticker := clock.NewTicker(tickInterval)\n    // ...\n}\n```\n\n**After:**\n```go\ntype Broadcaster struct {\n    // ... existing fields\n    maxClientsPerSession int\n    tickInterval         time.Duration\n}\n\nfunc NewBroadcaster(cfg *config.Config, ...) *Broadcaster {\n    return \u0026Broadcaster{\n        maxClientsPerSession: cfg.BroadcasterMaxClientsPerSession,\n        tickInterval:         cfg.BroadcasterTickInterval(),\n        // ... other fields\n    }\n}\n\nfunc (b *Broadcaster) run() {\n    ticker := b.clock.NewTicker(b.tickInterval)\n    // ... rest unchanged\n}\n```\n\n**App service changes:**\n\n**Before (`internal/app/service.go`):**\n```go\nconst (\n    orphanMaxAge = 30 * time.Second\n    cleanupInterval = 30 * time.Second\n)\n\nfunc NewService(...) *Service {\n    s := \u0026Service{...}\n    s.cleanupTicker = time.NewTicker(cleanupInterval)\n    // ...\n}\n```\n\n**After:**\n```go\ntype Service struct {\n    // ... existing fields\n    orphanMaxAge    time.Duration\n    cleanupInterval time.Duration\n}\n\nfunc NewService(cfg *config.Config, ...) *Service {\n    s := \u0026Service{\n        orphanMaxAge:    cfg.OrphanMaxAge(),\n        cleanupInterval: cfg.CleanupInterval(),\n        // ... other fields\n    }\n    s.cleanupTicker = time.NewTicker(s.cleanupInterval)\n    // ...\n}\n\nfunc (s *Service) CleanupOrphans(ctx context.Context) error {\n    cutoff := s.clock.Now().Add(-s.orphanMaxAge)\n    // ... rest unchanged\n}\n```\n\n**Server changes:**\n\n**Before (`internal/server/server.go`):**\n```go\nconst sessionMaxAgeDays = 7\n\nstore := sessions.NewCookieStore([]byte(cfg.SessionSecret))\nstore.MaxAge(sessionMaxAgeDays * 24 * 60 * 60)\n```\n\n**After:**\n```go\nstore := sessions.NewCookieStore([]byte(cfg.SessionSecret))\nstore.MaxAge(int(cfg.SessionMaxAge().Seconds()))\n```\n\n**Main.go wiring:**\n\n**Before:**\n```go\nbroadcaster := broadcast.NewBroadcaster(engine, onFirstClient, onSessionEmpty, clock)\nappSvc := app.NewService(userRepo, configRepo, sessionRepo, engine, twitchSvc, clock, logger)\n```\n\n**After:**\n```go\nbroadcaster := broadcast.NewBroadcaster(cfg, engine, onFirstClient, onSessionEmpty, clock)\nappSvc := app.NewService(cfg, userRepo, configRepo, sessionRepo, engine, twitchSvc, clock, logger)\n```\n\n**Files to modify:**\n- `internal/broadcast/broadcaster.go` (add config param + fields)\n- `internal/app/service.go` (add config param + fields)\n- `internal/server/server.go` (use cfg.SessionMaxAge())\n- `cmd/server/main.go` (pass cfg to constructors)\n\n**Time estimate:** 2 hours (code changes + testing)\n\n---\n\n### 4. Update tests to use Config\n\n**Test pattern:**\n\n**Before:**\n```go\nbroadcaster := NewBroadcaster(engine, onFirstClient, onSessionEmpty, fakeClock)\n```\n\n**After:**\n```go\ncfg := \u0026config.Config{\n    BroadcasterMaxClientsPerSession: 10, // Lower limit for tests\n    BroadcasterTickIntervalMs:       10, // Faster ticks for tests\n}\nbroadcaster := NewBroadcaster(cfg, engine, onFirstClient, onSessionEmpty, fakeClock)\n```\n\n**Benefits for testing:**\n- Can use faster intervals (10ms tick vs 50ms prod)\n- Can test limit behavior (set maxClients=2, verify cap)\n- Can test cleanup timing (set orphanMaxAge=1s, verify cleanup)\n\n**Files to modify:**\n- `internal/broadcast/broadcaster_test.go`\n- `internal/app/service_test.go`\n\n**Time estimate:** 1 hour\n\n---\n\n### 5. Document tuning guide\n\n**Add to CLAUDE.md:**\n\n```markdown\n## Performance Tuning\n\nThe following parameters are configurable via environment variables for production tuning:\n\n### Broadcaster Settings\n\n**`BROADCASTER_MAX_CLIENTS_PER_SESSION`** (default: 50)\n- Maximum WebSocket connections per session\n- Higher = more viewers per overlay (uses more memory)\n- Lower = prevents resource exhaustion\n- Recommend: 50 for 16GB instance, scale linearly\n\n**`BROADCASTER_TICK_INTERVAL_MS`** (default: 50)\n- How often broadcaster pulls values from Redis + broadcasts\n- Lower = lower latency, higher Redis load\n- Higher = lower Redis load, higher latency\n- Recommend: 50ms (20Hz refresh rate), tune based on Redis P99 latency\n\n### Cleanup Settings\n\n**`CLEANUP_INTERVAL_SECONDS`** (default: 30)\n- How often to scan for orphaned sessions\n- Lower = faster cleanup, higher Redis SCAN overhead\n- Higher = sessions linger longer, lower overhead\n- Recommend: 30s (2 cleanup cycles per minute)\n\n**`ORPHAN_MAX_AGE_SECONDS`** (default: 30)\n- How long before disconnected session is deleted\n- Lower = aggressive cleanup, risk of deleting active sessions (ref count races)\n- Higher = sessions linger, wastes Redis memory\n- Recommend: 30s (balances cleanup speed vs race condition window)\n\n### Session Settings\n\n**`SESSION_MAX_AGE_DAYS`** (default: 7)\n- HTTP session cookie expiry (gorilla/sessions)\n- Lower = users re-login more often\n- Higher = longer sessions, potential security risk\n- Recommend: 7 days (weekly re-auth)\n\n### Tuning Workflow\n\n1. **Establish baseline:** Deploy with defaults, observe metrics\n2. **Identify bottleneck:** Use Prometheus metrics (redis_operation_duration, broadcaster_tick_duration)\n3. **Adjust one parameter:** Change env var, redeploy\n4. **Measure impact:** Compare metrics before/after\n5. **Iterate:** Tune until performance targets met\n\n### Production Recommendations\n\n**16GB instance:**\n- BROADCASTER_MAX_CLIENTS_PER_SESSION=50\n- BROADCASTER_TICK_INTERVAL_MS=50\n\n**32GB instance:**\n- BROADCASTER_MAX_CLIENTS_PER_SESSION=100\n- BROADCASTER_TICK_INTERVAL_MS=50\n\n**High-latency Redis (P99 \u003e 50ms):**\n- BROADCASTER_TICK_INTERVAL_MS=100 (avoid overwhelming Redis)\n\n**High session churn:**\n- CLEANUP_INTERVAL_SECONDS=15 (more aggressive cleanup)\n- ORPHAN_MAX_AGE_SECONDS=15 (faster cleanup)\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Performance Tuning section)\n\n**Time estimate:** 1 hour\n\n---\n\n### 6. Document hardcoded constants rationale\n\n**Add to CLAUDE.md:**\n\n```markdown\n## Hardcoded Constants (Intentionally Not Configurable)\n\nThe following constants are **intentionally hardcoded** based on protocol standards or performance analysis:\n\n### WebSocket Timeouts\n\n**`writeDeadline`** = 5s (internal/broadcast/writer.go)\n- WebSocket write timeout\n- Rationale: Well-tuned for network conditions, changing risks dropped connections\n- If client can't receive in 5s, it's too slow (disconnect intentional)\n\n**`pingInterval`** = 30s (internal/broadcast/writer.go)\n- WebSocket heartbeat ping interval\n- Rationale: RFC 6455 recommends 30s for keepalive\n\n**`pongDeadline`** = 60s (internal/broadcast/writer.go)\n- WebSocket pong response timeout (2× ping interval)\n- Rationale: Standard pattern, allows one missed pong before disconnect\n\n### Redis Timeouts\n\n**`redisTimeout`** = 2s (internal/broadcast/broadcaster.go)\n- Redis operation timeout\n- Rationale: Coordinated with circuit breaker threshold (see ADR-001)\n- Changing independently risks circuit breaker misbehavior\n\n### Goroutine Buffers\n\n**`messageBufferSize`** = 16 (internal/broadcast/writer.go)\n- Per-client send channel buffer\n- Rationale: Premature optimization, 16 is sufficient for bursty broadcasts\n- Increasing doesn't improve throughput (network bottleneck, not buffer)\n\n### Twitch API Timeouts\n\n**`appTokenTimeout`** = 15s (internal/twitch/eventsub.go)\n- Twitch API HTTP client timeout\n- Rationale: Based on Twitch API SLA, their P99 \u003c 5s\n\n### When to Reconsider\n\nIf production metrics show these constants are bottlenecks:\n1. Document findings (metrics, incident reports)\n2. Propose change in ADR (include alternatives, consequences)\n3. Externalize to config with feature flag\n4. A/B test impact before rolling out\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Hardcoded Constants section)\n\n**Time estimate:** 30 minutes\n\n---\n\n## Acceptance Criteria\n\n- ✅ 5 production-tunable constants externalized to env vars\n- ✅ Config helper methods provide type-safe duration conversion\n- ✅ All consumers updated (Broadcaster, App, Server)\n- ✅ Tests use Config (faster intervals for test speed)\n- ✅ `.env.example` documents new vars with defaults\n- ✅ CLAUDE.md documents tuning guide (when/how to adjust)\n- ✅ CLAUDE.md documents hardcoded constants rationale (why not tunable)\n- ✅ No magic numbers in code (all constants explained)\n\n## Files Changed\n\n**Modified:**\n- `internal/config/config.go` (add 5 fields + helper methods)\n- `internal/broadcast/broadcaster.go` (use cfg.BroadcasterTickInterval())\n- `internal/app/service.go` (use cfg.CleanupInterval() + cfg.OrphanMaxAge())\n- `internal/server/server.go` (use cfg.SessionMaxAge())\n- `cmd/server/main.go` (pass cfg to Broadcaster + App)\n- `internal/broadcast/broadcaster_test.go` (use Config in tests)\n- `internal/app/service_test.go` (use Config in tests)\n- `.env.example` (add 5 new vars)\n- `CLAUDE.md` (add Performance Tuning + Hardcoded Constants sections)\n\n## Dependencies\n- None (independent refactoring)\n\n## Effort Estimate\n**Total: 3 hours** (180 minutes)\n- Categorization (1h): Analyze constants, determine tunability\n- Config changes (1h): Add fields, helper methods, .env.example\n- Code updates (2h): Update Broadcaster, App, Server, main.go\n- Test updates (1h): Use Config in tests\n- Documentation (1.5h): Tuning guide + hardcoded constants rationale\n\n## Success Metrics\n- Operators can tune performance without code changes\n- Production runs with custom settings (different from defaults)\n- Test suite runs 5× faster (10ms ticks vs 50ms)\n- Zero magic numbers in code (all constants documented)","status":"open","priority":3,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":180,"created_at":"2026-02-12T17:39:30.04139+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:52.877328+01:00"}
{"id":"twitch-tow-7nt","title":"Discussion: Database connection resilience and transaction patterns","description":"## Issue\nPostgreSQL connection has no retry logic, health monitoring, or connection pool tuning. Transaction failures in critical paths lack proper rollback and error recovery.\n\n## Current State\n- `database.Connect()` fails fast with os.Exit(1) if initial connection fails (main.go:106-108)\n- No connection pool configuration (uses pgxpool defaults: max 4 connections)\n- No connection health monitoring after startup\n- Migration failures are fatal (main.go:110-113)\n- UpsertUser transaction lacks proper error handling for partial failures\n\n## Failure Modes\n1. **Startup fragility**: Temporary DB unavailability during deployment causes crash loop\n2. **Connection pool exhaustion**: 4 connections insufficient for high concurrent load\n3. **No reconnection**: If DB connection drops mid-flight, no automatic recovery\n4. **Migration race conditions**: Multiple instances starting simultaneously could conflict on migrations\n5. **Transaction leaks**: If context is cancelled mid-transaction, connection may not be released\n\n## Risks\n- **Zero-downtime deploys**: Cannot deploy if DB has brief connectivity issue\n- **Scalability**: 4-connection default insufficient for horizontal scaling\n- **Data consistency**: Partial transaction failures could leave inconsistent state\n- **Resource leaks**: Unclosed transactions could exhaust connection pool\n\n## Suggestions\n1. Add connection retry with exponential backoff at startup (e.g., 3 retries, max 30s)\n2. Configure pgxpool explicitly: min/max connections, acquire timeout, health check interval\n3. Add health check endpoint that verifies DB connectivity (`pool.Ping(ctx)`)\n4. Use advisory locks for migration coordination across instances\n5. Add connection metrics (active, idle, wait time)\n6. Implement proper context propagation in transactions\n7. Consider read replica support for GET operations\n\n## Files\n- internal/database/postgres.go:17-69\n- cmd/server/main.go:100-116\n- internal/database/user_repository.go:22-72 (UpsertUser transaction)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:03:48.000201+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:38:41.088974+01:00","closed_at":"2026-02-12T17:38:41.088974+01:00","close_reason":"Superseded by implementation epic twitch-tow-9sk (Database Connection Resilience). Epic provides retry logic, pool config, advisory locks for migrations, health checks, metrics. Addresses all failure modes."}
{"id":"twitch-tow-81p","title":"EPIC: Singleflight Timeout Protection for Session Activation","description":"Add timeout protection to singleflight session activation to prevent slow requests from blocking all concurrent callers.\n\n## User Story\nAs a user connecting to an overlay, I want my WebSocket connection to timeout independently if session activation is slow, so one slow database query doesn't hang all concurrent connections.\n\n## Value Proposition\n- Prevents shared fate failures (one slow DB query blocks all concurrent requests)\n- Individual timeout control per caller (5s per WebSocket, not shared 30s)\n- Better observability (track which activations timeout vs succeed)\n- Graceful degradation (some connections succeed even if first caller fails)\n\n## Background\n\n**Current behavior (app/service.go):**\n```go\nfunc (s *Service) EnsureSessionActive(ctx context.Context, overlayUUID uuid.UUID) error {\n    v, err, _ := s.activationGroup.Do(overlayUUID.String(), func() (interface{}, error) {\n        // All concurrent callers share THIS execution\n        // If this takes 30s, ALL callers wait 30s\n        // If this fails, ALL callers fail\n        return nil, s.activateSession(ctx, overlayUUID)\n    })\n    return err\n}\n```\n\n**Problem:**\n- 100 WebSocket connections arrive simultaneously\n- Singleflight collapses to 1 activation\n- First caller's context timeout = 30s\n- All 100 callers wait up to 30s (even if their timeout is 5s)\n- If activation fails, all 100 connections fail\n\n**Benefits of singleflight (must preserve):**\n- ✅ Prevents duplicate Twitch EventSub subscriptions\n- ✅ Reduces DB load (1 query vs 100)\n- ✅ Prevents Redis write races\n\n**Concerns to address:**\n- ❌ Shared context lifetime\n- ❌ No per-caller timeout\n- ❌ Shared fate on errors\n\n## Tasks\n\n### 1. Add timeout wrapper around singleflight\n\n**Implementation strategy:**\n\nUse **DoChan** instead of **Do** to enable per-caller timeout control:\n\n```go\nfunc (s *Service) EnsureSessionActive(ctx context.Context, overlayUUID uuid.UUID) error {\n    // Use DoChan for non-blocking singleflight\n    ch := s.activationGroup.DoChan(overlayUUID.String(), func() (interface{}, error) {\n        // Create activation context with LONG timeout (30s)\n        // This is the shared execution, should be generous\n        activationCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n        defer cancel()\n        \n        return nil, s.activateSession(activationCtx, overlayUUID)\n    })\n    \n    // Each caller waits with THEIR context timeout\n    select {\n    case result := \u003c-ch:\n        return result.Err\n    case \u003c-ctx.Done():\n        // Caller's context timed out (e.g., 5s WebSocket deadline)\n        // Activation may still be running in background (benefits future callers)\n        s.logger.Warn(\"session activation timeout (caller-specific)\",\n            \"session_uuid\", overlayUUID,\n            \"error\", ctx.Err())\n        return fmt.Errorf(\"session activation timeout: %w\", ctx.Err())\n    }\n}\n```\n\n**Key insight:**\n- Activation runs with **30s timeout** (generous, shared across all callers)\n- Each caller respects **their own context** (e.g., 5s WebSocket deadline)\n- First caller times out at 5s → returns error to that client\n- Activation continues in background → completes at 10s\n- Subsequent callers (arriving at 8s) get immediate success (activation done)\n\n**Files to modify:**\n- `internal/app/service.go` (update EnsureSessionActive method)\n\n**Time estimate:** 1 hour\n\n---\n\n### 2. Add activation timeout metric\n\n**Track timeout events:**\n\n```go\n// Add to observability metrics (if Prometheus is implemented)\nvar activationTimeouts = prometheus.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: \"chatpulse_session_activation_timeouts_total\",\n        Help: \"Total session activation timeouts (caller-specific)\",\n    },\n    []string{\"reason\"}, // \"context_deadline\" vs \"activation_failed\"\n)\n\n// In EnsureSessionActive timeout case:\ncase \u003c-ctx.Done():\n    activationTimeouts.WithLabelValues(\"context_deadline\").Inc()\n    // ... rest of timeout handling\n```\n\n**Benefits:**\n- Track how often callers timeout vs activation succeeds\n- Distinguish caller timeout (5s) from activation failure (DB error)\n- Informs tuning decisions (do we need longer WebSocket deadlines?)\n\n**Files to modify:**\n- `internal/app/service.go` (add metric increment)\n- `internal/app/metrics.go` (if metrics refactored to separate file)\n\n**Dependencies:**\n- Requires Epic for Observability (Prometheus metrics) OR defer to that epic\n\n**Time estimate:** 30 minutes (if metrics already exist), otherwise defer\n\n---\n\n### 3. Update tests for timeout behavior\n\n**Test scenarios:**\n\n**Test 1: Caller timeout before activation completes**\n```go\nfunc TestEnsureSessionActive_CallerTimeout(t *testing.T) {\n    // Mock slow DB query (10s)\n    userRepo := \u0026mockUserRepo{\n        GetByOverlayUUIDFunc: func(ctx context.Context, uuid uuid.UUID) (*domain.User, error) {\n            time.Sleep(10 * time.Second) // Simulate slow query\n            return \u0026domain.User{...}, nil\n        },\n    }\n    \n    app := NewService(cfg, userRepo, ...)\n    \n    // Caller context with 1s timeout\n    ctx, cancel := context.WithTimeout(context.Background(), 1*time.Second)\n    defer cancel()\n    \n    err := app.EnsureSessionActive(ctx, overlayUUID)\n    \n    // Expect timeout error, not DB result\n    assert.ErrorIs(t, err, context.DeadlineExceeded)\n}\n```\n\n**Test 2: Subsequent caller succeeds after first times out**\n```go\nfunc TestEnsureSessionActive_SubsequentCallerSucceeds(t *testing.T) {\n    // Mock slow DB query (2s)\n    userRepo := \u0026mockUserRepo{\n        GetByOverlayUUIDFunc: func(ctx context.Context, uuid uuid.UUID) (*domain.User, error) {\n            time.Sleep(2 * time.Second)\n            return \u0026domain.User{...}, nil\n        },\n    }\n    \n    app := NewService(cfg, userRepo, ...)\n    \n    // First caller: 1s timeout (will fail)\n    ctx1, cancel1 := context.WithTimeout(context.Background(), 1*time.Second)\n    defer cancel1()\n    \n    // Second caller: 5s timeout (will succeed after 2s)\n    ctx2, cancel2 := context.WithTimeout(context.Background(), 5*time.Second)\n    defer cancel2()\n    \n    // Start both simultaneously\n    var wg sync.WaitGroup\n    wg.Add(2)\n    \n    var err1, err2 error\n    go func() {\n        defer wg.Done()\n        err1 = app.EnsureSessionActive(ctx1, overlayUUID)\n    }()\n    go func() {\n        defer wg.Done()\n        err2 = app.EnsureSessionActive(ctx2, overlayUUID)\n    }()\n    \n    wg.Wait()\n    \n    // First caller timed out\n    assert.ErrorIs(t, err1, context.DeadlineExceeded)\n    \n    // Second caller succeeded (activation completed in background)\n    assert.NoError(t, err2)\n}\n```\n\n**Test 3: All callers benefit from completed activation**\n```go\nfunc TestEnsureSessionActive_SingleflightStillWorks(t *testing.T) {\n    callCount := 0\n    userRepo := \u0026mockUserRepo{\n        GetByOverlayUUIDFunc: func(ctx context.Context, uuid uuid.UUID) (*domain.User, error) {\n            callCount++\n            return \u0026domain.User{...}, nil\n        },\n    }\n    \n    app := NewService(cfg, userRepo, ...)\n    \n    // 10 concurrent callers, all with generous timeout\n    var wg sync.WaitGroup\n    for i := 0; i \u003c 10; i++ {\n        wg.Add(1)\n        go func() {\n            defer wg.Done()\n            ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n            defer cancel()\n            err := app.EnsureSessionActive(ctx, overlayUUID)\n            assert.NoError(t, err)\n        }()\n    }\n    \n    wg.Wait()\n    \n    // Singleflight still works: only 1 DB query\n    assert.Equal(t, 1, callCount)\n}\n```\n\n**Files to modify:**\n- `internal/app/service_test.go` (add 3 new test cases)\n\n**Time estimate:** 1 hour\n\n---\n\n### 4. Document singleflight trade-offs in CLAUDE.md\n\n**Add to Application Layer section:**\n\n```markdown\n### Session Activation with Singleflight\n\n`app.Service.EnsureSessionActive` uses `golang.org/x/sync/singleflight` to deduplicate concurrent session activations.\n\n**Pattern: DoChan with per-caller timeout**\n\n```go\nch := s.activationGroup.DoChan(overlayUUID.String(), func() (interface{}, error) {\n    activationCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n    return nil, s.activateSession(activationCtx, overlayUUID)\n})\n\nselect {\ncase result := \u003c-ch:\n    return result.Err\ncase \u003c-ctx.Done():\n    return fmt.Errorf(\"session activation timeout: %w\", ctx.Err())\n}\n```\n\n**Benefits:**\n- ✅ Prevents duplicate Twitch EventSub subscriptions (critical)\n- ✅ Reduces DB load (1 query for N concurrent requests)\n- ✅ Per-caller timeout control (WebSocket 5s, activation 30s)\n\n**Trade-offs:**\n- ⏱️ First caller timeout → other callers may still succeed\n- 🔄 Activation continues in background (benefits future callers)\n- 📊 Metric `session_activation_timeouts_total` tracks caller timeouts\n\n**Why DoChan instead of Do:**\n- `Do` shares first caller's context (all fail together if first times out)\n- `DoChan` decouples activation timeout (30s) from caller timeout (5s)\n- Enables graceful degradation (some succeed, some timeout)\n\n**When activation fails:**\nAll waiting callers receive same error (shared fate). This is intentional - if activation truly failed (DB down, Twitch API error), all callers should fail fast rather than retry individually.\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Singleflight section under Application Layer)\n\n**Time estimate:** 30 minutes\n\n---\n\n## Acceptance Criteria\n\n- ✅ `EnsureSessionActive` uses `DoChan` for per-caller timeout\n- ✅ Activation runs with 30s timeout (generous shared execution)\n- ✅ Each caller respects their own context (5s WebSocket deadline)\n- ✅ First caller timeout doesn't fail subsequent callers\n- ✅ Singleflight still prevents duplicate subscriptions\n- ✅ Tests cover caller timeout scenarios\n- ✅ Documentation explains DoChan rationale\n- ✅ (Optional) Metric tracks timeout events\n\n## Files Changed\n\n**Modified:**\n- `internal/app/service.go` (update EnsureSessionActive to use DoChan)\n- `internal/app/service_test.go` (add 3 timeout test cases)\n- `CLAUDE.md` (add Singleflight section)\n- `internal/app/metrics.go` (optional: add timeout metric)\n\n## Dependencies\n- None (independent improvement)\n- Optional: Depends on Observability epic for metrics\n\n## Effort Estimate\n**Total: 2 hours** (120 minutes)\n- Implementation (1h): DoChan + timeout handling\n- Testing (1h): 3 test cases (caller timeout, subsequent success, singleflight verification)\n- Documentation (30min): CLAUDE.md update\n- Metrics (30min): Optional, defer to Observability epic\n\n## Success Metrics\n- Slow activations don't block all callers (P99 WebSocket connect \u003c5s even with slow DB)\n- Singleflight still works (DB query count = 1 for concurrent requests)\n- Timeout metric shows \u003c1% of activations hit caller timeout","status":"open","priority":3,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":120,"created_at":"2026-02-12T17:41:33.164504+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:51.771945+01:00"}
{"id":"twitch-tow-8dx","title":"EPIC: Code Quality Improvements (Coverage + Package Docs + Naming)","description":"Improve code quality through coverage tracking, package documentation, and naming standardization. Low-effort high-value improvements for maintainability.\n\n## User Story\nAs a new developer joining the project, I need clear package documentation and coverage metrics so I can understand the codebase structure and identify untested areas without hunting through CLAUDE.md.\n\n## Value Proposition\n- Coverage badge provides instant test health visibility\n- Package docs improve godoc usability and IDE hover help\n- Naming standardization eliminates confusion about interfaces vs implementations\n- Low effort (2-3 days) with high impact on developer experience\n\n## Background\n\n**Problems identified:**\n1. **No coverage tracking** - make test-coverage exists but no CI enforcement or badge (twitch-tow-1hb)\n2. **No package docs** - godoc output is sparse, IDE hover shows nothing (twitch-tow-jgs)\n3. **Repository naming inconsistency** - Database repos use 'Repo' vs 'Repository', Redis repos don't (twitch-tow-1iq)\n\n## Tasks\n\n### 1. Add coverage badge and CI enforcement\n\n**Current state:**\n- `make test-coverage` generates coverage.out + coverage.html\n- No coverage requirement, no visibility\n- 141 tests across 8 packages (good baseline)\n\n**Implementation:**\n\n**Step 1:** Choose coverage provider\n- **Option A:** GitHub Actions + Codecov (free for open source)\n- **Option B:** GitHub Actions + built-in coverage report\n- **Recommendation:** GitHub Actions + Codecov (better visualization)\n\n**Step 2:** Update CI workflow (.github/workflows/test.yml)\n```yaml\nname: Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-go@v5\n        with:\n          go-version: '1.26'\n      - name: Run tests with coverage\n        run: |\n          make test-coverage\n      - name: Upload coverage to Codecov\n        uses: codecov/codecov-action@v4\n        with:\n          files: ./coverage.out\n          fail_ci_if_error: true\n      - name: Check coverage threshold\n        run: |\n          coverage=$(go tool cover -func=coverage.out | grep total | awk '{print $3}' | sed 's/%//')\n          echo \"Total coverage: $coverage%\"\n          if (( $(echo \"$coverage \u003c 70\" | bc -l) )); then\n            echo \"Coverage $coverage% is below threshold 70%\"\n            exit 1\n          fi\n```\n\n**Step 3:** Add coverage badge to README.md\n```markdown\n# ChatPulse\n\n[![Tests](https://github.com/user/chatpulse/actions/workflows/test.yml/badge.svg)](https://github.com/user/chatpulse/actions/workflows/test.yml)\n[![Coverage](https://codecov.io/gh/user/chatpulse/branch/main/graph/badge.svg)](https://codecov.io/gh/user/chatpulse)\n\n...\n```\n\n**Step 4:** Document coverage targets in CLAUDE.md\n```markdown\n## Testing\n\n141 tests across 8 packages. Run with `make test` or `go test ./...`.\n\n### Coverage Targets\n\n**Minimum:** 70% overall coverage (enforced in CI)\n\n**Per-package targets:**\n- `sentiment/`: 90%+ (core business logic, critical for correctness)\n- `broadcast/`: 85%+ (concurrency patterns need thorough testing)\n- `redis/`: 80%+ (integration tests cover most Redis operations)\n- `app/`: 80%+ (orchestration layer, many edge cases)\n- `server/`: 75%+ (handler logic, auth flows)\n- `database/`: 75%+ (repository CRUD operations)\n- `crypto/`: 90%+ (security-sensitive code)\n- `config/`: 60%+ (simple validation logic)\n\n**Note:** 100% coverage is not the goal. Focus on critical paths (domain logic, Redis operations, HTTP handlers). Template rendering and simple getters are lower priority.\n```\n\n**Files to modify:**\n- `.github/workflows/test.yml` (add coverage upload + threshold check)\n- `README.md` (add coverage badge)\n- `CLAUDE.md` (add coverage targets section)\n\n**Time estimate:** 3 hours\n\n---\n\n### 2. Add package-level documentation\n\n**Current state:**\n- All 11 packages lack package doc comments\n- godoc output shows types without context\n- CLAUDE.md has excellent docs but not godoc-accessible\n\n**Implementation:**\n\n**Package docs to add (one-liner + overview):**\n\n**`internal/domain/doc.go`:**\n```go\n// Package domain defines the core domain types and interfaces.\n//\n// This package contains concept-oriented files (errors.go, user.go, config.go, session.go, etc.)\n// with shared types and cross-cutting interfaces. No implementation code - just contracts.\n// Prevents circular imports by keeping interfaces on the consumer side.\npackage domain\n```\n\n**`internal/app/doc.go`:**\n```go\n// Package app provides the application service layer.\n//\n// Orchestrates use cases: session activation, config saves, overlay UUID rotation, orphan cleanup.\n// Sits between HTTP handlers and domain repositories. Depends on domain interfaces, not concrete implementations.\npackage app\n```\n\n**`internal/broadcast/doc.go`:**\n```go\n// Package broadcast implements the WebSocket broadcaster using the actor pattern.\n//\n// The Broadcaster pulls current sentiment values from Redis on a 50ms tick and fans out to connected clients.\n// Uses single goroutine + command channel (no mutexes). Per-connection write goroutines handle slow clients gracefully.\npackage broadcast\n```\n\n**`internal/sentiment/doc.go`:**\n```go\n// Package sentiment implements the sentiment calculation engine.\n//\n// The Engine orchestrates vote processing: trigger matching, debounce checks, and atomic vote application via Redis Functions.\n// GetCurrentValue computes time-decayed sentiment. No mutable state (delegates to Redis).\npackage sentiment\n```\n\n**`internal/redis/doc.go`:**\n```go\n// Package redis implements Redis-backed repositories.\n//\n// Provides SessionRepository (session lifecycle + ref counting), SentimentStore (vote application + decay),\n// and Debouncer (per-user rate limiting). Uses Redis Functions for atomic operations.\npackage redis\n```\n\n**`internal/database/doc.go`:**\n```go\n// Package database provides PostgreSQL connectivity and repositories.\n//\n// Uses pgx for connection pooling, tern for migrations, and sqlc for type-safe query generation.\n// Repositories implement domain interfaces: UserRepository, ConfigRepository, EventSubRepository.\npackage database\n```\n\n**`internal/server/doc.go`:**\n```go\n// Package server implements the HTTP server using Echo framework.\n//\n// Routes: auth (OAuth), dashboard (config UI), overlay (WebSocket), API (reset/rotate), webhooks (EventSub).\n// Handlers split by domain: handlers_auth.go, handlers_dashboard.go, handlers_api.go, handlers_overlay.go.\npackage server\n```\n\n**`internal/crypto/doc.go`:**\n```go\n// Package crypto provides encryption services for data at rest.\n//\n// Implements AES-256-GCM encryption for OAuth tokens stored in PostgreSQL.\n// Two implementations: AesGcmCryptoService (production) and NoopService (dev/test plaintext passthrough).\npackage crypto\n```\n\n**`internal/twitch/doc.go`:**\n```go\n// Package twitch integrates with the Twitch API.\n//\n// EventSubManager handles conduit lifecycle and subscription management. Webhook handler processes\n// channel.chat.message events and applies votes via the sentiment engine.\npackage twitch\n```\n\n**`internal/config/doc.go`:**\n```go\n// Package config provides environment-based configuration.\n//\n// Loads from .env file (godotenv), maps to Config struct via go-simpler/env struct tags.\n// Validates required fields and encryption key format.\npackage config\n```\n\n**Files to create:**\n- `internal/domain/doc.go`\n- `internal/app/doc.go`\n- `internal/broadcast/doc.go`\n- `internal/sentiment/doc.go`\n- `internal/redis/doc.go`\n- `internal/database/doc.go`\n- `internal/server/doc.go`\n- `internal/crypto/doc.go`\n- `internal/twitch/doc.go`\n- `internal/config/doc.go`\n\n**Time estimate:** 2 hours (30 min to write, 90 min to review + polish)\n\n---\n\n### 3. Standardize repository naming\n\n**Current inconsistency:**\n- Database repos: Impl = `XxxRepo`, Interface = `XxxRepository`\n- Redis repos: Impl = Interface name (both called same thing)\n\n**Decision:** Standardize on **Repo (impl)** + **Repository (interface)** everywhere\n\n**Rationale:**\n- Consistent with database pattern (already 3/6 repos follow this)\n- IDE 'Find Usages' separates interface from impl\n- Grep distinguishes interface definition from usage\n\n**Changes required:**\n\n**1. Rename Redis interface** `SentimentStore` → `SentimentStoreRepository`:\n- `internal/domain/sentiment.go`: Rename interface\n- `internal/redis/sentiment_store.go`: Update impl to reference new interface name\n- `internal/sentiment/engine.go`: Update field type\n- `cmd/server/main.go`: Update wiring\n\n**Wait, this creates naming confusion!** Let's reconsider:\n\n**Alternative: Keep names distinct by purpose**\n- `SessionRepo` + `SessionRepository` ✅ (general CRUD operations)\n- `SentimentStore` + `SentimentStore` ✅ (specialized storage, not CRUD)\n- `Debouncer` + `Debouncer` ✅ (not a repository pattern, it's a service)\n\n**New decision:** **Don't rename** - the current pattern is intentional:\n- **Repository pattern:** XxxRepo (impl) + XxxRepository (interface) - for CRUD operations\n- **Service pattern:** Same name for both - for specialized non-CRUD operations\n\n**Document the pattern instead:**\n\n**`CLAUDE.md` update:**\n```markdown\n### Repository vs Service Naming\n\n**Repository pattern** (CRUD operations on entities):\n- Interface: `domain.XxxRepository` (consumer-side interface)\n- Implementation: `database.XxxRepo` or `redis.XxxRepo` (producer-side impl)\n- Example: `UserRepository` interface, `database.UserRepo` impl\n\n**Service pattern** (specialized operations, not CRUD):\n- Interface: `domain.XxxService` or `domain.Xxx` (if obvious service)\n- Implementation: Same name as interface (no `Impl` suffix - Go antipattern)\n- Example: `SentimentStore` interface + `redis.SentimentStore` impl, `Debouncer` interface + `redis.Debouncer` impl\n\n**Rationale:** Repository pattern = data access layer (distinguished by Repo vs Repository). Service pattern = domain services (same name, package disambiguates).\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add naming convention section)\n\n**Time estimate:** 30 minutes (documentation only, no code changes)\n\n---\n\n### 4. Improve testcontainers documentation\n\n**Current issue:**\n- Developers may not know about `-short` flag for fast TDD loops\n- Testcontainers startup time (3-7s) not documented\n\n**Implementation:**\n\n**Update `CLAUDE.md` Testing section:**\n```markdown\n## Testing\n\n141 tests across 8 packages. Complete in ~15 seconds (including testcontainers startup).\n\n### Running Tests\n\n```bash\nmake test          # Run all tests (unit + integration, ~15s)\nmake test-short    # Run unit tests only (skip integration, \u003c2s) \nmake test-race     # Run with race detector (~20s)\nmake test-coverage # Generate coverage report\n```\n\n### Integration Tests with Testcontainers\n\nIntegration tests use **testcontainers** for PostgreSQL and Redis (real infrastructure, not mocks):\n\n**Benefits:**\n- Real database behavior (catches schema/query issues)\n- Production-like environment\n- Docker standardization across team\n\n**Overhead:**\n- First run: 3-7s container startup (PostgreSQL + Redis)\n- Subsequent runs: \u003c1s (containers cached in Docker)\n- CI: 15s total (acceptable)\n\n**TDD workflow:**\n1. Use `go test -short ./...` for rapid feedback (\u003c2s)\n2. Run full suite before commit (`make test`)\n3. CI runs full suite on every push\n\n**Why testcontainers over mocks?**\n- Database mocks don't catch SQL errors or constraint violations\n- Redis mocks don't replicate Lua function behavior\n- Integration tests provide higher confidence for data layer\n\n**Skipping integration tests:**\nAll integration tests check `testing.Short()`:\n```go\nif testing.Short() {\n    t.Skip(\"Skipping integration test\")\n}\n```\n```\n\n**Files to modify:**\n- `CLAUDE.md` (expand Testing section)\n- `README.md` (add quick start testing guide)\n\n**Time estimate:** 1 hour\n\n---\n\n### 5. Add template caching documentation\n\n**Current behavior:**\n- Templates parsed once at startup (good for production)\n- No hot-reload for development (restart required)\n\n**Implementation:** Document trade-off, no code changes needed\n\n**`CLAUDE.md` update:**\n```markdown\n### Template Caching\n\nTemplates (`web/templates/*.html`) are parsed once at startup and cached in the `Server` struct:\n- `loginTemplate`\n- `dashboardTemplate`  \n- `overlayTemplate`\n\n**Production behavior:**\n- ✅ Fast rendering (no parsing overhead)\n- ✅ Fail-fast if template invalid (catches errors at startup)\n\n**Development behavior:**\n- ⚠️ Template changes require app restart\n- Rationale: Template changes are infrequent, restart overhead acceptable\n\n**Alternative considered:** Hot-reload in dev mode (check `APP_ENV`, parse on each request)\n- Rejected: Added complexity for minimal DX improvement\n- Workaround: Use air or modd for auto-restart on file changes\n\n**Template paths:**\n- Hardcoded: `web/templates/login.html`, etc.\n- Rationale: Templates location is stable, no need for dynamic paths\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Template Caching section under HTTP Server)\n\n**Time estimate:** 30 minutes\n\n---\n\n## Acceptance Criteria\n\n- ✅ Coverage badge visible in README\n- ✅ CI enforces 70% minimum coverage\n- ✅ Coverage targets documented in CLAUDE.md\n- ✅ All 10 packages have package-level docs (doc.go files)\n- ✅ godoc output shows package overviews\n- ✅ Repository vs Service naming convention documented\n- ✅ Testcontainers usage documented (benefits + TDD workflow)\n- ✅ Template caching trade-off documented\n\n## Files Changed\n\n**Created:**\n- `internal/domain/doc.go`\n- `internal/app/doc.go`\n- `internal/broadcast/doc.go`\n- `internal/sentiment/doc.go`\n- `internal/redis/doc.go`\n- `internal/database/doc.go`\n- `internal/server/doc.go`\n- `internal/crypto/doc.go`\n- `internal/twitch/doc.go`\n- `internal/config/doc.go`\n\n**Modified:**\n- `.github/workflows/test.yml` (add coverage upload + threshold)\n- `README.md` (add coverage badge + testing quick start)\n- `CLAUDE.md` (add coverage targets, naming conventions, testing details, template caching)\n\n## Dependencies\n- None (independent improvements)\n\n## Effort Estimate\n**Total: 4 hours** (240 minutes)\n- Coverage badge + CI (3h): Codecov setup, CI workflow, threshold check, documentation\n- Package docs (2h): Write 10 doc.go files\n- Naming conventions (30min): Document pattern in CLAUDE.md\n- Testcontainers docs (1h): Expand testing section\n- Template caching docs (30min): Add section to CLAUDE.md\n\n## Success Metrics\n- Coverage badge shows \u003e70% in README\n- CI fails if coverage drops below 70%\n- godoc shows package overviews for all packages\n- New contributors reference package docs (track via GitHub issues/PRs)\n- Developers use `-short` flag for TDD (track via team feedback)","status":"open","priority":2,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":240,"created_at":"2026-02-12T17:37:36.203267+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:45.933066+01:00"}
{"id":"twitch-tow-8vo","title":"Solution: Comprehensive ADR documentation plan","description":"SOLUTION PROPOSAL for architecture documentation (twitch-tow-4kh plus input from team)\n\nThis consolidates architectural decisions across maintainability, scalability, and resilience domains into formal ADR documentation.\n\n## Proposed ADRs (Prioritized)\n\n### Tier 1 - Foundation Decisions (Critical for understanding)\n\nADR-001: Redis-only architecture for session state\n- Context: Need shared state across instances, horizontal scaling\n- Decision: All session state in Redis, zero in-memory state per instance\n- Alternatives: Sticky sessions, in-memory with sync, distributed cache\n- Consequences: Enables stateless instances, requires Redis HA, 2-5ms latency per operation\n- Related: Ref counting strategy, eventual consistency acceptance\n\nADR-002: Pull-based broadcaster vs Redis pub/sub\n- Context: Need to broadcast sentiment updates to WebSocket clients\n- Decision: 50ms tick loop pulls from Redis, fan-out in-memory\n- Alternatives: Redis pub/sub, push-based events, SSE\n- Consequences: Simpler code, 50ms staleness, N Redis calls per tick vs 1 pub/sub\n- Related: Tick interval choice, Redis call frequency\n\nADR-003: Twitch EventSub webhooks + conduits vs WebSocket\n- Context: Receive chat messages from Twitch API\n- Decision: Use webhooks via conduit, not EventSub WebSocket\n- Alternatives: EventSub WebSocket, polling, IRC\n- Consequences: Simpler deployment (no long-lived connection), requires public endpoint, webhook verification\n- Related: Single bot account architecture\n\n### Tier 2 - Scaling Decisions (Important for operations)\n\nADR-004: Single bot account reads all channels\n- Context: Need to receive chat messages from multiple streamers\n- Decision: One bot account with user:read:chat scope, streamers grant channel:bot\n- Alternatives: Per-streamer OAuth, separate bots per streamer\n- Consequences: Single rate limit pool, simpler auth, single point of failure\n- Related: EventSub subscription strategy\n\nADR-005: No sticky sessions - stateless instances\n- Context: Load balancing across multiple instances\n- Decision: Any instance can serve any session, state in Redis\n- Alternatives: Sticky sessions via load balancer, session affinity\n- Consequences: Simplified load balancing, requires ref counting, eventual consistency\n- Related: Redis-only architecture, ref counting\n\nADR-006: Ref counting for multi-instance coordination\n- Context: Multiple instances may serve same session, need cleanup coordination\n- Decision: Redis INCR/DECR for ref count, 30s grace period, eventual consistency\n- Alternatives: Distributed locks, leader election, sticky sessions\n- Consequences: Simple but best-effort, race conditions accepted, 30s cleanup delay\n- Related: Orphan cleanup strategy\n\nADR-007: Database vs Redis data separation\n- Context: What data lives in PostgreSQL vs Redis?\n- Decision: Config + users in PostgreSQL (source of truth), sessions + sentiment in Redis (ephemeral cache)\n- Alternatives: All in PostgreSQL, all in Redis, dual-write to both\n- Consequences: PostgreSQL is durable, Redis is fast but losable, cold start requires DB fetch\n- Related: Session activation flow\n\n### Tier 3 - Security and Resilience (Important for production)\n\nADR-008: UUID-based overlay access control\n- Context: Overlay URLs are public (embedded in OBS), need access control\n- Decision: Separate overlay_uuid per user, acts as bearer token, rotatable\n- Alternatives: Streamer passwords, OAuth for viewers, signed URLs with expiry\n- Consequences: Simple but URL leakage = public access, rotation invalidates old URLs\n- Related: CORS policy (accept all origins for OBS)\n\nADR-009: AES-256-GCM token encryption at rest\n- Context: OAuth tokens stored in PostgreSQL\n- Decision: Encrypt with AES-256-GCM, key from env var, nonce prepended to ciphertext\n- Alternatives: Database-level encryption, no encryption (dev only), external KMS\n- Consequences: Protects against DB dumps, key rotation is complex, NoopService for dev\n- Related: Token refresh strategy\n\nADR-010: Graceful degradation - eventual consistency over strong consistency\n- Context: Distributed system, failures inevitable\n- Decision: Accept eventual consistency (ref counting, session state), prioritize availability\n- Alternatives: Strong consistency via distributed locks, sacrifice availability\n- Consequences: Simpler code, faster operations, race conditions possible, self-healing\n- Related: 30s cleanup grace period, ref counting races\n\n### Tier 4 - Implementation Choices (Good to document)\n\nADR-011: Actor pattern for broadcaster concurrency\n- Context: Manage WebSocket connections and broadcast loop\n- Decision: Single goroutine per broadcaster, command channel, no mutexes\n- Alternatives: Mutex-protected shared state, channel per session, separate actors\n- Consequences: Simple reasoning, potential bottleneck \u003e1K sessions, serial tick processing\n- Related: Broadcaster scaling limits\n\nADR-012: Manual dependency injection in main.go\n- Context: How to wire dependencies at startup\n- Decision: Manual constructor calls in main.go, explicit dependency graph\n- Alternatives: DI framework (wire, dig, fx), service locator pattern\n- Consequences: Verbose but explicit, zero runtime overhead, IDE-friendly, boilerplate grows\n- Related: Package organization, testing strategy\n\nADR-013: sqlc for SQL generation\n- Context: Type-safe SQL queries for PostgreSQL\n- Decision: Use sqlc to generate Go code from SQL files\n- Alternatives: ORM (GORM, ent), raw SQL with hand-written types, sqlx\n- Consequences: Type safety, SQL-first workflow, generates boilerplate, excellent performance\n- Related: Migration strategy (tern)\n\nADR-014: Time-decay sentiment algorithm\n- Context: Sentiment should decay toward neutral over time\n- Decision: Exponential decay in Redis Lua function, rate configurable per streamer\n- Alternatives: Linear decay, step decay, no decay, client-side calculation\n- Consequences: Smooth decay, requires clock sync, Lua testing complexity, configurable per-user\n- Related: Redis Functions deployment\n\n### Tier 5 - Operational Decisions (Added from team review)\n\nADR-015: Deployment strategy and zero-downtime deploys\n- Context: How to deploy new versions without downtime\n- Decision: Rolling deploy, tern migrations at startup, /ready endpoint for load balancer\n- Alternatives: Blue/green deployment, separate migration job, downtime window\n- Consequences: Migrations must be backward-compatible, pre-stop hook required, Lua function versioning\n- Related: Expand/contract migration pattern\n\nADR-016: Observability and SLO tracking\n- Context: How to monitor system health and meet reliability targets\n- Decision: Prometheus metrics, health checks, structured logging, no tracing initially\n- Alternatives: DataDog, OpenTelemetry tracing, log aggregation only\n- Consequences: Standard tools, requires scraping, cardinality management, SLO definition needed\n- Related: Error budget tracking, alerting strategy\n\n## ADR Template (Consensus from team)\n\nUse lightweight template with required sections plus optional notes. Format in Markdown for GitHub rendering.\n\n## Implementation Plan\n\nPhase 1 (Week 1): Write Tier 1 ADRs (3 ADRs, foundation decisions)\nPhase 2 (Week 2): Write Tier 2 ADRs (4 ADRs, scaling decisions)\nPhase 3 (Week 3): Write Tier 3 ADRs (3 ADRs, security/resilience)\nPhase 4 (Week 4): Write Tier 4-5 ADRs (6 ADRs, implementation/operations)\n\nEstimated effort: 1 hour per ADR × 16 ADRs = 16 hours total (2 days)\n\n## ADR Organization\n\nLocation: docs/adr/ in repository root\nIndex: docs/adr/README.md with table listing all ADRs by tier\nReference: Add link to CLAUDE.md architecture section\nNaming: NNN-short-kebab-case-title.md\n\n## Success Criteria\n\n- All 16 ADRs written and reviewed\n- Index page allows quick navigation\n- Each ADR follows template consistently\n- Alternatives section documents rejected options\n- Consequences section captures trade-offs\n- Team can reference ADRs when proposing changes\n\n## Notes\n\nADRs are living documents - can be updated when decisions change (status becomes Deprecated/Superseded). This initial batch captures current architecture as of Feb 2026. Future decisions should add new ADRs rather than updating CLAUDE.md only.\n\nVote: +1 from Maintainability architect. This captures the why behind every major decision and prevents cargo-cult refactoring.","notes":"Vote: +1 from architect-resilience (Resilience Expert).\n\n**STRONG APPROVAL** - This comprehensively captures architectural decisions from all three domains with excellent prioritization.\n\n## Answers to Specific Questions\n\n**Q: Does ADR-016 (Observability and SLO tracking) address SLO questions?**\n✅ **YES** - ADR-016 covers the key points from my input:\n- Prometheus metrics (from twitch-tow-eyl)\n- Health checks for load balancer\n- SLO definition needed (I provided 99.5% availability target, RTO 5min, RPO 0s/30s)\n\n**Suggestion:** ADR-016 should reference concrete SLO targets from my input:\n- Availability: 99.5% (3.6 hours downtime/month)\n- Vote processing latency: p99 \u003c 500ms\n- WebSocket broadcast: p99 \u003c 100ms\n- RTO: 5 minutes, RPO: 0s (Postgres), 30s (Redis)\n\n**Q: Should we add ADR-017 for disaster recovery and backup strategy?**\n✅ **YES, ADD ADR-017** to Tier 3 (Security/Resilience):\n\n**ADR-017: Disaster recovery and backup strategy**\n- Context: Need to recover from catastrophic failures (DB corruption, data center outage)\n- Decision: PostgreSQL backups (daily full + WAL archiving), Redis no backups (ephemeral)\n- Alternatives: Redis persistence (AOF/RDB), multi-region replication, no backups\n- Consequences: Can restore users/configs, sessions lost on Redis failure (acceptable)\n- Related: Ephemeral session state (ADR-010)\n\n**Q: Is graceful degradation (ADR-010) well-scoped or should it be split?**\n✅ **WELL-SCOPED AS IS** - ADR-010 captures the high-level philosophy (eventual consistency, fail-open). Specific mechanisms (circuit breaker, debounce fail-open) can be documented separately when implemented.\n\n**Suggestion:** Add reference in ADR-010 to future ADRs:\n- ADR-018: Circuit breaker thresholds (when implemented from twitch-tow-sb3)\n- ADR-019: Error handling philosophy (when implemented from twitch-tow-bqx)\n\n## Additional Missing ADRs (Tier 3)\n\n**ADR-018: WebSocket connection limits and eviction policy** (Tier 3)\n- Context: Prevent resource exhaustion from slow/malicious clients\n- Decision: 50 clients per session, 5s write deadline, non-blocking send, slow eviction\n- Alternatives: Unlimited clients, backpressure, rate limiting\n- Consequences: Hard cap prevents DoS, eviction may surprise users, capacity planning needed\n- Related: twitch-tow-7k9 discussion\n\n**ADR-019: No request-level retries for Twitch API** (Tier 3)\n- Context: Twitch API calls can fail, should we retry?\n- Decision: No automatic retries, rely on EventSub at-least-once delivery + idempotency\n- Alternatives: Exponential backoff retries, circuit breaker only\n- Consequences: Simpler code, duplicate subscriptions prevented, circuit breaker handles outages\n- Related: Circuit breaker (when implemented)\n\n## Prioritization Feedback\n\n**Current prioritization is EXCELLENT:**\n- Tier 1 (Foundation): ✅ Covers Redis-only, pull-based, webhooks - core architectural pillars\n- Tier 2 (Scaling): ✅ Covers single bot, stateless, ref counting - critical for multi-instance\n- Tier 3 (Security/Resilience): ✅ Right priority - important but not blocking understanding\n\n**No moves needed between tiers.** The 7 ADRs in Tier 1-2 are exactly the right foundation.\n\n## Effort Estimate Feedback\n\n**16 hours (1 hour per ADR) is REALISTIC** with the following assumptions:\n- Template is used consistently (saves time)\n- Alternatives are well-known (already discussed in beads)\n- No cross-team review cycles (async approval via beads)\n\n**Parallelization opportunity:**\n- Each architect can own ADRs from their domain simultaneously\n- Maintainability: ADR-011, ADR-012, ADR-013, ADR-014\n- Scalability: ADR-004, ADR-005, ADR-006, ADR-007\n- Resilience: ADR-008, ADR-009, ADR-010, ADR-016, ADR-017 (new)\n\n**Timeline is achievable:** 4 weeks sequential OR 1 week parallel (if all architects contribute).\n\n## Final Recommendations\n\n1. ✅ **Approve as proposed** with additions:\n   - Add ADR-017: Disaster recovery (Tier 3)\n   - Add ADR-018: WebSocket limits (Tier 3)\n   - Add ADR-019: No retries (Tier 3)\n\n2. ✅ **Total: 19 ADRs** (16 proposed + 3 additions) = 19 hours = 2.5 days\n\n3. ✅ **Prioritize Tier 1-2** (7 ADRs) if time-constrained - these are foundation\n\n4. ✅ **Parallelize work** - each architect owns their domain ADRs\n\n**Ready to implement - excellent work consolidating team input!**","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:16:34.548624+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.369252+01:00","closed_at":"2026-02-12T17:56:57.369252+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-8z6","title":"EPIC: Optimize Orphan Cleanup with Redis Sorted Set - Replace SCAN with O(log N) Range Query","description":"## User Story\nAs an operator running ChatPulse at scale with 100K+ Redis keys, I want orphan cleanup to use efficient range queries instead of keyspace scans so cleanup remains fast and doesn't impact Redis performance.\n\n## Problem Statement\n\nThe ListOrphans() method uses Redis SCAN with pattern 'session:*' which scans the entire keyspace. This becomes expensive at scale.\n\n**Current implementation:**\n- SCAN with count=100 hint every 30 seconds\n- O(N) where N = total keys in Redis (not just sessions)\n- Pattern matching on every key\n- Safe but inefficient at scale\n\n**Impact at scale:**\n- 10K sessions: Negligible (\u003c100ms)\n- 100K sessions: Noticeable (1-2 seconds)\n- 1M total keys: Expensive (10+ seconds)\n\n## Solution: Redis Sorted Set\n\nReplace SCAN with sorted set indexed by disconnect timestamp.\n\n### Architecture\n\n**Current (SCAN-based):**\n```\nKeys: session:{uuid} (hash)\nCleanup: SCAN session:* → check last_disconnect field\n```\n\n**New (Sorted Set):**\n```\nKeys: \n  session:{uuid} (hash) - unchanged\n  disconnected_sessions (zset) - NEW\n    score = disconnect timestamp\n    member = session UUID\n```\n\n### Implementation Tasks\n\n#### Task 1: Add to sorted set on disconnect\n**File:** `internal/redis/session_repository.go`\n\n```go\nfunc (r *SessionRepo) MarkDisconnected(ctx context.Context, sessionUUID uuid.UUID) error {\n    now := r.clock.Now()\n    pipe := r.client.Pipeline()\n    \n    // Update session hash (existing)\n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    pipe.HSet(ctx, key, \"last_disconnect\", now.Unix())\n    \n    // Add to sorted set (NEW)\n    pipe.ZAdd(ctx, \"disconnected_sessions\", redis.Z{\n        Score:  float64(now.Unix()),\n        Member: sessionUUID.String(),\n    })\n    \n    _, err := pipe.Exec(ctx)\n    return err\n}\n```\n\n#### Task 2: Remove from sorted set on reconnect\n**File:** `internal/redis/session_repository.go`\n\n```go\nfunc (r *SessionRepo) ActivateSession(ctx, sessionUUID, broadcasterUserID, config) error {\n    pipe := r.client.Pipeline()\n    \n    // Create/update session hash (existing)\n    // ...\n    \n    // Remove from disconnected set (NEW)\n    pipe.ZRem(ctx, \"disconnected_sessions\", sessionUUID.String())\n    \n    _, err := pipe.Exec(ctx)\n    return err\n}\n```\n\n#### Task 3: Replace ListOrphans with range query\n**File:** `internal/redis/session_repository.go`\n\n```go\nfunc (r *SessionRepo) ListOrphans(ctx context.Context, grace time.Duration) ([]uuid.UUID, error) {\n    cutoff := r.clock.Now().Add(-grace).Unix()\n    \n    // Range query: sessions disconnected before cutoff\n    // ZRANGEBYSCORE disconnected_sessions -inf {cutoff}\n    members, err := r.client.ZRangeByScore(ctx, \"disconnected_sessions\", \u0026redis.ZRangeBy{\n        Min: \"-inf\",\n        Max: fmt.Sprintf(\"%d\", cutoff),\n    }).Result()\n    \n    if err != nil {\n        return nil, err\n    }\n    \n    orphans := make([]uuid.UUID, 0, len(members))\n    for _, member := range members {\n        if uuid, err := uuid.Parse(member); err == nil {\n            orphans = append(orphans, uuid)\n        }\n    }\n    \n    return orphans, nil\n}\n```\n\n**Time complexity:**\n- Before: O(N) where N = all keys\n- After: O(log M + K) where M = disconnected sessions, K = orphans found\n\n#### Task 4: Cleanup orphan entries from sorted set\n**File:** `internal/redis/session_repository.go`\n\n```go\nfunc (r *SessionRepo) DeleteSession(ctx context.Context, sessionUUID uuid.UUID) error {\n    pipe := r.client.Pipeline()\n    \n    // Delete session hash (existing)\n    pipe.Del(ctx, fmt.Sprintf(\"session:%s\", sessionUUID))\n    pipe.Del(ctx, fmt.Sprintf(\"ref_count:%s\", sessionUUID))\n    pipe.Del(ctx, fmt.Sprintf(\"broadcaster:%s\", broadcasterUserID))\n    \n    // Remove from sorted set (NEW)\n    pipe.ZRem(ctx, \"disconnected_sessions\", sessionUUID.String())\n    \n    _, err := pipe.Exec(ctx)\n    return err\n}\n```\n\n#### Task 5: Migration script for existing sessions\n**File:** `cmd/migrate-orphan-cleanup/main.go` (NEW)\n\n```go\n// One-time migration: populate sorted set from existing sessions\nfunc migrateExistingSessions(ctx context.Context, rdb *redis.Client) error {\n    var cursor uint64\n    var migrated int\n    \n    for {\n        keys, nextCursor, err := rdb.Scan(ctx, cursor, \"session:*\", 100).Result()\n        if err != nil {\n            return err\n        }\n        \n        for _, key := range keys {\n            // Get disconnect timestamp\n            lastDisconnect, err := rdb.HGet(ctx, key, \"last_disconnect\").Int64()\n            if err == redis.Nil {\n                continue  // Active session, skip\n            }\n            if err != nil {\n                return err\n            }\n            \n            // Extract UUID from key\n            uuidStr := strings.TrimPrefix(key, \"session:\")\n            \n            // Add to sorted set\n            rdb.ZAdd(ctx, \"disconnected_sessions\", redis.Z{\n                Score:  float64(lastDisconnect),\n                Member: uuidStr,\n            })\n            migrated++\n        }\n        \n        cursor = nextCursor\n        if cursor == 0 {\n            break\n        }\n    }\n    \n    log.Printf(\"Migrated %d disconnected sessions to sorted set\", migrated)\n    return nil\n}\n```\n\n#### Task 6: Add sorted set size metric\n**File:** `internal/metrics/metrics.go`\n\n```go\nDisconnectedSessionsCount = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"disconnected_sessions_count\",\n        Help: \"Number of sessions in disconnected state\",\n    },\n)\n```\n\nUpdate in cleanup loop:\n```go\nfunc (s *Service) CleanupOrphans(ctx context.Context) error {\n    // Get sorted set size\n    count, _ := s.sessions.DisconnectedCount(ctx)\n    metrics.DisconnectedSessionsCount.Set(float64(count))\n    \n    // ... rest of cleanup\n}\n```\n\n#### Task 7: Unit tests\n**File:** `internal/redis/session_repository_orphan_test.go` (NEW)\n\nTest scenarios:\n1. MarkDisconnected adds to sorted set\n2. ActivateSession removes from sorted set\n3. ListOrphans returns only sessions past grace period\n4. DeleteSession removes from sorted set\n5. Sorted set size metric updated\n\n#### Task 8: Integration test\n**File:** `internal/redis/session_repository_orphan_integration_test.go` (NEW)\n\nBenchmark comparison:\n```go\nfunc BenchmarkListOrphans_SCAN(b *testing.B) {\n    // Old implementation with 10K sessions\n}\n\nfunc BenchmarkListOrphans_ZSET(b *testing.B) {\n    // New implementation with 10K sessions\n    // Should be 100x+ faster\n}\n```\n\n#### Task 9: Documentation\n**File:** `docs/architecture/orphan-cleanup-optimization.md` (NEW)\n\nDocument:\n- Sorted set architecture\n- Migration procedure\n- Performance comparison\n- Monitoring sorted set size\n\n#### Task 10: Update CLAUDE.md\nDocument new Redis key schema in ## Redis Architecture section.\n\n## Acceptance Criteria\n\n- ✅ MarkDisconnected adds to sorted set atomically\n- ✅ ActivateSession removes from sorted set\n- ✅ ListOrphans uses ZRANGEBYSCORE (O(log N))\n- ✅ Migration script populates sorted set from existing data\n- ✅ Sorted set size metric tracked\n- ✅ Benchmark shows 10x+ improvement with 10K sessions\n- ✅ Unit tests achieve 100% coverage\n- ✅ Integration test verifies correctness\n\n## Files Created/Modified\n\n**New files:**\n- `cmd/migrate-orphan-cleanup/main.go` (100 lines migration)\n- `internal/redis/session_repository_orphan_test.go` (300 lines)\n- `internal/redis/session_repository_orphan_integration_test.go` (200 lines)\n- `docs/architecture/orphan-cleanup-optimization.md` (250 lines)\n\n**Modified files:**\n- `internal/redis/session_repository.go` (add sorted set operations, 40 lines)\n- `internal/metrics/metrics.go` (add sorted set size metric, 10 lines)\n- `CLAUDE.md` (document sorted set in Redis schema)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock Redis with sorted set commands\n- Verify atomic operations (pipeline)\n- Test edge cases (empty set, single item)\n\n**Integration tests:**\n- Real Redis with testcontainers\n- Populate 10K sessions, benchmark ListOrphans\n- Verify migration script correctness\n\n**Performance benchmarks:**\n- SCAN vs ZRANGEBYSCORE comparison\n- Target: 10x improvement at 10K sessions\n- Target: 100x improvement at 100K sessions\n\n## Dependencies\n- None (independent optimization)\n\n## Success Metrics\n- ListOrphans completes in \u003c10ms for 10K sessions (down from 100ms+)\n- Sorted set size matches disconnected session count\n- Zero orphans missed (correctness preserved)\n\n## Effort Estimate\n**4 developer-days**\n\nBreakdown:\n- Sorted set implementation: 1 day\n- Migration script: 0.5 day\n- Unit tests: 1 day\n- Integration tests + benchmarks: 1 day\n- Documentation: 0.5 day\n\n## Migration Plan\n\n1. Deploy new code (sorted set operations added but SCAN still used)\n2. Run migration script to populate sorted set\n3. Monitor sorted set size metric\n4. Switch ListOrphans to use ZRANGEBYSCORE\n5. Monitor for correctness (no missed orphans)\n6. Remove SCAN-based implementation after validation\n\n## Risks \u0026 Mitigation\n- **Risk:** Sorted set gets out of sync\n  - **Mitigation:** Periodic reconciliation job (scan + rebuild)\n  - **Mitigation:** Metric alerts on size anomalies\n- **Risk:** Migration script misses sessions\n  - **Mitigation:** Dry-run mode, verification step\n  - **Mitigation:** Can re-run migration script safely (ZADD is idempotent)","status":"open","priority":3,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:47:32.042755+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:49.945076+01:00"}
{"id":"twitch-tow-91j","title":"Idea: Mock generation strategy - consider mockery","description":"The codebase hand-writes mocks for testing (e.g., mockSessionRepo, mockSentimentStore, mockDebouncer in engine_test.go). This works but has maintainability costs:\n\n**Current approach - Hand-written mocks:**\n- mockSessionRepo: 50+ lines to implement 11-method interface, mostly stubs\n- mockSentimentStore: ~20 lines for 3 methods\n- mockDebouncer: ~10 lines for 1 method\n- Tests define mock behavior inline via function fields\n\n**Concerns:**\n1. **Interface changes break tests**: Adding a method to SessionRepository requires updating all mock implementations\n2. **Boilerplate code**: ~100 lines of mock code in test files vs ~50 lines of actual test logic\n3. **Inconsistent mock patterns**: Some tests use function fields, others inline returns\n4. **No compile-time safety**: Easy to forget updating mocks after interface changes\n\n**Alternative - mockery (or similar tool):**\n- Pros: Auto-generate mocks, compile errors on interface changes, consistent patterns\n- Cons: External dependency, generated code to maintain, slightly more complex test setup\n\n**Data point**: With 141 tests across 8 packages, manual mocks are still manageable. But as the codebase grows, this could become a pain point.\n\n**Recommendation**: Document the decision - either commit to manual mocks or adopt mockery before the codebase scales further.","notes":"RESOLVED: Documentation-only decision. At 141 tests, hand-written mocks are manageable and give more control than generated mocks. Decision: Keep manual mocks, document pattern in CLAUDE.md Testing section. If test count exceeds 300 or interface churn becomes painful, revisit mockery adoption. Current approach is appropriate for codebase scale. No epic needed - pattern is working well.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:57.21103+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:50:18.36605+01:00","closed_at":"2026-02-12T17:50:18.366053+01:00"}
{"id":"twitch-tow-9c6","title":"EPIC: Context Propagation and Configurable Timeouts - Improve Cancellation and Shutdown","description":"**User Story:** As a developer and operator, I need consistent context propagation throughout the codebase so that operations can be cancelled gracefully and timeouts are configurable for testing.\n\n**Problem Context:** Current context usage is inconsistent:\n- Mixed use of `context.Background()` vs request contexts\n- No cancellation propagation in `Stop()` methods\n- Hardcoded timeout values scattered across codebase (2s, 5s, 10s, 15s, 30s)\n- Shutdown can take 30s+ to complete all in-flight operations\n- Tests can't speed up timeouts\n\n**Solution Overview:** Standardize context patterns, add cancellation to Stop() methods, make timeouts configurable via constructors, and document shutdown timing contracts.\n\n## Task Breakdown\n\n### 1. Add Context Cancellation to Broadcaster.Stop()\n\n**File:** `internal/broadcast/broadcaster.go`\n\n**Add shutdown context:**\n```go\ntype Broadcaster struct {\n    // ... existing fields\n    shutdownCtx    context.Context     // NEW: shutdown context\n    shutdownCancel context.CancelFunc  // NEW: cancel function\n}\n\nfunc NewBroadcaster(engine domain.Engine, onFirstClient, onSessionEmpty func(...), clock clockwork.Clock) *Broadcaster {\n    shutdownCtx, shutdownCancel := context.WithCancel(context.Background())\n    \n    b := \u0026Broadcaster{\n        // ... existing fields\n        shutdownCtx:    shutdownCtx,\n        shutdownCancel: shutdownCancel,\n    }\n    go b.run()\n    return b\n}\n\nfunc (b *Broadcaster) Stop() error {\n    // Cancel shutdown context (propagates to in-flight operations)\n    b.shutdownCancel()\n    \n    // Send stop command\n    reply := make(chan error)\n    b.commands \u003c- \u0026stopCmd{reply: reply}\n    err := \u003c-reply\n    \n    // Wait for goroutine (with timeout)\n    select {\n    case \u003c-b.done:\n        return err\n    case \u003c-time.After(5 * time.Second):\n        return fmt.Errorf(\"broadcaster stop timed out after 5s\")\n    }\n}\n\n// Use shutdownCtx for Redis calls in tick loop\nfunc (b *Broadcaster) run() {\n    ticker := b.clock.NewTicker(50 * time.Millisecond)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ticker.Chan():\n            for sessionUUID := range b.activeClients {\n                // Use shutdown context with timeout\n                ctx, cancel := context.WithTimeout(b.shutdownCtx, b.redisTimeout)\n                value, status, err := b.engine.GetCurrentValue(ctx, sessionUUID)\n                cancel()\n                \n                // If shutdownCtx is cancelled, exit immediately\n                if ctx.Err() == context.Canceled {\n                    return\n                }\n                \n                // ... broadcast\n            }\n        case cmd := \u003c-b.commands:\n            // ... handle commands\n        }\n    }\n}\n```\n\n### 2. Add Configurable Timeouts via Constructors\n\n**File:** `internal/broadcast/broadcaster.go`\n\n**Add timeout configuration:**\n```go\ntype BroadcasterConfig struct {\n    TickInterval    time.Duration  // 50ms default\n    RedisTimeout    time.Duration  // 2s default\n    CommandTimeout  time.Duration  // 5s default\n}\n\nfunc NewBroadcaster(cfg BroadcasterConfig, engine domain.Engine, ...) *Broadcaster {\n    if cfg.TickInterval == 0 {\n        cfg.TickInterval = 50 * time.Millisecond\n    }\n    if cfg.RedisTimeout == 0 {\n        cfg.RedisTimeout = 2 * time.Second\n    }\n    if cfg.CommandTimeout == 0 {\n        cfg.CommandTimeout = 5 * time.Second\n    }\n    \n    // ... use cfg values instead of hardcoded constants\n}\n```\n\n**File:** `internal/app/service.go`\n\n**Add cleanup timeouts:**\n```go\ntype ServiceConfig struct {\n    CleanupInterval   time.Duration  // 30s default\n    CleanupScanTimeout time.Duration  // 30s default\n    OrphanMaxAge      time.Duration  // 30s default\n}\n\nfunc NewService(cfg ServiceConfig, ...) *Service {\n    if cfg.CleanupInterval == 0 {\n        cfg.CleanupInterval = 30 * time.Second\n    }\n    // ... use cfg values\n}\n```\n\n### 3. Propagate Request Context Through Stack\n\n**File:** `internal/server/handlers_overlay.go`\n\n**Pass Echo context to broadcaster:**\n```go\nfunc (s *Server) handleOverlayWebSocket(c echo.Context) error {\n    // ... setup\n    \n    // Use Echo request context (already has cancellation)\n    ctx := c.Request().Context()\n    \n    // Register client with context\n    if err := s.broadcaster.RegisterWithContext(ctx, overlayUUID, conn); err != nil {\n        return err\n    }\n    \n    // Read pump respects context cancellation\n    for {\n        select {\n        case \u003c-ctx.Done():\n            // Request cancelled, cleanup\n            s.broadcaster.Unregister(overlayUUID, conn)\n            return ctx.Err()\n        default:\n            _, _, err := conn.ReadMessage()\n            if err != nil {\n                s.broadcaster.Unregister(overlayUUID, conn)\n                return err\n            }\n        }\n    }\n}\n```\n\n**File:** `internal/broadcast/broadcaster.go`\n\n**Add context-aware Register:**\n```go\nfunc (b *Broadcaster) RegisterWithContext(ctx context.Context, sessionUUID uuid.UUID, conn *websocket.Conn) error {\n    reply := make(chan error)\n    cmd := \u0026registerClientCmd{sessionUUID: sessionUUID, conn: conn, reply: reply}\n    \n    select {\n    case b.commands \u003c- cmd:\n        // Wait for reply with context\n        select {\n        case err := \u003c-reply:\n            return err\n        case \u003c-ctx.Done():\n            return ctx.Err()\n        }\n    case \u003c-ctx.Done():\n        return ctx.Err()\n    }\n}\n```\n\n### 4. Document Shutdown Timing Contract\n\n**File:** `CLAUDE.md`\n\n**Add section under \"Concurrency Model\":**\n```markdown\n### Shutdown Timing and Context Propagation\n\n**Context patterns:**\n- Request contexts: Propagated from Echo handlers through all layers (DB, Redis, Twitch)\n- Background contexts: Used for fire-and-forget operations (cleanup unsubscribe)\n- Shutdown context: Injected into Broadcaster, cancelled on Stop() to abort in-flight Redis calls\n\n**Timeout hierarchy:**\n- WebSocket read: Respects request context cancellation (immediate)\n- Broadcaster register: 5s command timeout\n- Broadcaster tick: 2s Redis timeout per session\n- Cleanup scan: 30s scan timeout\n- App shutdown: 5s total (broadcaster stop timeout)\n\n**Graceful shutdown sequence:**\n1. SIGINT/SIGTERM received\n2. HTTP server stop accepting connections (Echo shutdown)\n3. Broadcaster.Stop() cancels shutdown context\n4. In-flight Redis calls abort on context cancellation\n5. Broadcaster waits up to 5s for goroutine exit\n6. App cleanup timer stopped\n7. Database and Redis connections closed\n\n**Total shutdown time:** \u003c10s in normal conditions, max 35s if cleanup scan in progress.\n```\n\n### 5. Add Context Validation Tests\n\n**File:** `internal/broadcast/broadcaster_context_test.go` (NEW)\n\n**Test cases:**\n```go\n// TestBroadcaster_ShutdownCancellation verifies context propagation\nfunc TestBroadcaster_ShutdownCancellation(t *testing.T) {\n    // Setup: Mock engine that sleeps 10s on GetCurrentValue\n    // Act: broadcaster.Stop() after 1s\n    // Assert: Stop() returns in \u003c2s (doesn't wait for 10s Redis call)\n}\n\n// TestBroadcaster_ConfigurableTimeouts verifies constructor config\nfunc TestBroadcaster_ConfigurableTimeouts(t *testing.T) {\n    // Setup: BroadcasterConfig with 100ms Redis timeout\n    // Act: Slow Redis call (200ms)\n    // Assert: Context deadline exceeded error\n}\n\n// TestBroadcaster_RequestContextCancellation verifies Echo context\nfunc TestBroadcaster_RequestContextCancellation(t *testing.T) {\n    // Setup: Echo request with 1s timeout\n    // Act: RegisterWithContext with slow operation\n    // Assert: Returns context.DeadlineExceeded\n}\n```\n\n### 6. Add Timeout Configuration to Config\n\n**File:** `internal/config/config.go`\n\n**Add timeout fields:**\n```go\ntype Config struct {\n    // ... existing fields\n    \n    // Broadcaster timeouts\n    BroadcasterTickInterval    time.Duration `env:\"BROADCASTER_TICK_INTERVAL\" envDefault:\"50ms\"`\n    BroadcasterRedisTimeout    time.Duration `env:\"BROADCASTER_REDIS_TIMEOUT\" envDefault:\"2s\"`\n    BroadcasterCommandTimeout  time.Duration `env:\"BROADCASTER_COMMAND_TIMEOUT\" envDefault:\"5s\"`\n    \n    // App cleanup timeouts\n    CleanupInterval   time.Duration `env:\"CLEANUP_INTERVAL\" envDefault:\"30s\"`\n    CleanupScanTimeout time.Duration `env:\"CLEANUP_SCAN_TIMEOUT\" envDefault:\"30s\"`\n    OrphanMaxAge      time.Duration `env:\"ORPHAN_MAX_AGE\" envDefault:\"30s\"`\n}\n```\n\n**File:** `.env.example`\n\n**Document timeout config:**\n```\n# Timeout configuration (optional, defaults shown)\nBROADCASTER_TICK_INTERVAL=50ms\nBROADCASTER_REDIS_TIMEOUT=2s\nBROADCASTER_COMMAND_TIMEOUT=5s\nCLEANUP_INTERVAL=30s\nCLEANUP_SCAN_TIMEOUT=30s\nORPHAN_MAX_AGE=30s\n```\n\n### 7. Wire Timeouts Through Constructors\n\n**File:** `cmd/server/main.go`\n\n**Pass config to constructors:**\n```go\nbroadcasterCfg := broadcast.BroadcasterConfig{\n    TickInterval:   cfg.BroadcasterTickInterval,\n    RedisTimeout:   cfg.BroadcasterRedisTimeout,\n    CommandTimeout: cfg.BroadcasterCommandTimeout,\n}\nbroadcaster := broadcast.NewBroadcaster(broadcasterCfg, engine, ...)\n\nappCfg := app.ServiceConfig{\n    CleanupInterval:   cfg.CleanupInterval,\n    CleanupScanTimeout: cfg.CleanupScanTimeout,\n    OrphanMaxAge:      cfg.OrphanMaxAge,\n}\nappSvc := app.NewService(appCfg, ...)\n```\n\n## Acceptance Criteria\n\n✅ Broadcaster.Stop() cancels shutdown context, aborting in-flight Redis calls\n✅ Shutdown completes in \u003c10s under normal conditions\n✅ Request context propagated from Echo handlers through all layers\n✅ Timeouts configurable via env vars (for testing and tuning)\n✅ CLAUDE.md documents shutdown timing contract and context patterns\n✅ Tests verify context cancellation propagates correctly\n✅ Tests verify configurable timeouts work as expected\n\n## Dependencies\n\n- None (self-contained refactor)\n- Improves: Graceful shutdown (already implemented)\n\n## Files Modified\n\n**Modified:**\n- internal/broadcast/broadcaster.go (shutdown context, configurable timeouts)\n- internal/broadcast/writer.go (use shutdown context)\n- internal/app/service.go (configurable timeouts)\n- internal/server/handlers_overlay.go (propagate request context)\n- internal/config/config.go (add timeout fields)\n- cmd/server/main.go (wire timeouts)\n- .env.example (document timeout config)\n- CLAUDE.md (document shutdown timing)\n\n**New:**\n- internal/broadcast/broadcaster_context_test.go (context propagation tests)\n\n## Estimated Effort\n\n**Implementation:** 2 developer-days\n- Shutdown context: 3 hours\n- Configurable timeouts: 2 hours\n- Request context propagation: 3 hours\n- Configuration wiring: 2 hours\n- Testing: 1 day (context cancellation requires coordination)\n- Documentation: 2 hours\n\n**Total:** 2 developer-days\n\n## Rollout Strategy\n\n1. Deploy with configurable timeouts (using default values)\n2. Monitor shutdown duration in production (add metric)\n3. Verify shutdown completes in \u003c10s\n4. Test faster timeouts in staging (for faster integration tests)\n5. Document recommended timeout values for different deployment sizes","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:40:07.037588+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:45.217759+01:00"}
{"id":"twitch-tow-9f2","title":"EPIC: Optimize Session Activation Cold Start - Config Caching in Redis","description":"## Epic Overview\nReduce session activation cold start latency from 126-615ms to \u003c50ms by parallelizing database queries and caching config data in Redis.\n\n## Parent Discussion\ntwitch-tow-dwo (Session activation cold start latency)\n\n## User Story\nAs a viewer connecting to an overlay for the first time (or after Redis restart), I need the WebSocket connection to establish quickly so the overlay appears responsive and I don't see a loading state for more than 50ms.\n\n## Problem Analysis\n\n**Current cold start flow (sequential):**\n1. Redis: Check SessionExists → 1-5ms\n2. DB: GetUserByOverlayUUID → 10-50ms ⏸️\n3. DB: GetConfigByUserID → 10-50ms ⏸️\n4. Redis: ActivateSession → 5-10ms\n5. Twitch API: Subscribe EventSub → 100-500ms ⏸️\n\n**Total latency:** 126-615ms\n\n**Frequency:**\n- Once per session on first connection (any instance)\n- Rare after warmup (sessions stay in Redis)\n- Spikes after Redis restart or cleanup\n\n**Key insights:**\n- Steps 2 \u0026 3 (DB queries) are independent → can parallelize\n- Config rarely changes → can cache in Redis\n- Twitch subscribe blocks critical path unnecessarily\n\n## Solution: Three-Phase Optimization\n\n### Phase 1: Parallel Database Queries (Easy Win) ✅\n\n**Before (sequential):**\n```go\nuser, err := userRepo.GetByOverlayUUID(ctx, overlayUUID)  // 10-50ms\nconfig, err := configRepo.GetByUserID(ctx, user.ID)       // 10-50ms\n// Total: 20-100ms\n```\n\n**After (parallel):**\n```go\nvar user domain.User\nvar config domain.Config\n\ng, ctx := errgroup.WithContext(ctx)\n\ng.Go(func() error {\n    var err error\n    user, err = s.users.GetByOverlayUUID(ctx, overlayUUID)\n    return err\n})\n\ng.Go(func() error {\n    var err error\n    // Must wait for user.ID, so this won't work directly\n    // ALTERNATIVE: fetch both with JOIN or cache config separately\n    return nil\n})\n\nif err := g.Wait(); err != nil {\n    return err\n}\n```\n\n**Problem:** Config query depends on user.ID (sequential dependency)\n\n**Solution A:** JOIN query (complex, requires custom SQL)\n**Solution B:** Cache config in Redis by overlayUUID (eliminates DB query entirely)\n\n**Verdict:** Parallel queries alone don't help much due to dependency. Proceed to Phase 2.\n\n### Phase 2: Config Caching in Redis (Big Win) ✅\n\nStore config in Redis during activation, check cache first:\n\n```go\nfunc (s *Service) EnsureSessionActive(ctx, overlayUUID) error {\n    // Check if session already active\n    exists, err := s.sessions.SessionExists(ctx, overlayUUID)\n    if exists {\n        return nil  // Fast path\n    }\n    \n    // Cold start path\n    \n    // Try config cache first\n    configSnapshot, err := s.sessions.GetCachedConfig(ctx, overlayUUID)\n    if err == nil {\n        // Config cache hit - skip DB query!\n        user, err := s.users.GetByOverlayUUID(ctx, overlayUUID)\n        if err != nil {\n            return err\n        }\n        \n        // Activate with cached config\n        return s.sessions.ActivateSession(ctx, overlayUUID, user.TwitchUserID, configSnapshot)\n    }\n    \n    // Config cache miss - fetch from DB\n    user, err := s.users.GetByOverlayUUID(ctx, overlayUUID)\n    if err != nil {\n        return err\n    }\n    \n    config, err := s.configs.GetByUserID(ctx, user.ID)\n    if err != nil {\n        return err\n    }\n    \n    configSnapshot := config.ToSnapshot()\n    \n    // Activate and cache config\n    if err := s.sessions.ActivateSession(ctx, overlayUUID, user.TwitchUserID, configSnapshot); err != nil {\n        return err\n    }\n    \n    // Cache config for future cold starts\n    if err := s.sessions.CacheConfig(ctx, overlayUUID, configSnapshot, 24*time.Hour); err != nil {\n        slog.Warn(\"Failed to cache config\", \"error\", err)\n    }\n    \n    // Subscribe to EventSub\n    return s.twitch.Subscribe(ctx, user.TwitchUserID)\n}\n```\n\n**New Redis operations:**\n```go\n// SessionRepository interface additions\nCacheConfig(ctx, overlayUUID, config, ttl) error\nGetCachedConfig(ctx, overlayUUID) (ConfigSnapshot, error)\n```\n\n**Redis implementation:**\n```go\nfunc (r *SessionRepo) CacheConfig(ctx, overlayUUID, config, ttl) error {\n    key := fmt.Sprintf(\"config_cache:%s\", overlayUUID)\n    configJSON, _ := json.Marshal(config)\n    return r.client.Set(ctx, key, configJSON, ttl).Err()\n}\n\nfunc (r *SessionRepo) GetCachedConfig(ctx, overlayUUID) (ConfigSnapshot, error) {\n    key := fmt.Sprintf(\"config_cache:%s\", overlayUUID)\n    data, err := r.client.Get(ctx, key).Bytes()\n    if err != nil {\n        return ConfigSnapshot{}, err\n    }\n    \n    var config ConfigSnapshot\n    json.Unmarshal(data, \u0026config)\n    return config, nil\n}\n```\n\n**Cache invalidation:**\n- TTL: 24 hours (long, config changes are rare)\n- Explicit invalidation on config save:\n```go\nfunc (s *Service) SaveConfig(ctx, userID, updates) error {\n    // Save to DB\n    if err := s.configs.Update(ctx, userID, updates); err != nil {\n        return err\n    }\n    \n    // Invalidate config cache\n    user, _ := s.users.GetByID(ctx, userID)\n    s.sessions.InvalidateConfigCache(ctx, user.OverlayUUID)\n    \n    return nil\n}\n```\n\n**Latency improvement:**\n- Before: 10-50ms (DB config query)\n- After: 1-5ms (Redis cache hit)\n- **Savings: 9-45ms (45-90% faster)**\n\n### Phase 3: Async EventSub Subscribe (Optional) ⚠️\n\nMove Twitch EventSub subscribe to background goroutine:\n\n```go\nfunc (s *Service) EnsureSessionActive(ctx, overlayUUID) error {\n    // ... activation logic\n    \n    // Subscribe asynchronously\n    go func() {\n        ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)\n        defer cancel()\n        \n        if err := s.twitch.Subscribe(ctx, user.TwitchUserID); err != nil {\n            slog.Error(\"Failed to subscribe to EventSub\", \"error\", err, \"user_id\", user.ID)\n            metrics.EventSubSubscribeFailures.Inc()\n        }\n    }()\n    \n    return nil  // Return immediately\n}\n```\n\n**Benefits:**\n- 100-500ms removed from critical path\n- WebSocket establishes immediately\n\n**Risks:**\n- If subscribe fails silently, no votes will flow\n- Hard to debug (async failure)\n- Race condition: votes might arrive before subscribe completes (unlikely, Twitch has latency)\n\n**Recommendation:** Keep subscribe synchronous but add timeout metric. Only make async if P99 latency unacceptable.\n\n## Implementation Tasks\n\n### Task 1: Add config cache methods to SessionRepository\n**File:** `internal/domain/session.go`\n\n```go\ntype SessionRepository interface {\n    // ... existing methods\n    \n    // Config caching\n    CacheConfig(ctx context.Context, overlayUUID uuid.UUID, config ConfigSnapshot, ttl time.Duration) error\n    GetCachedConfig(ctx context.Context, overlayUUID uuid.UUID) (ConfigSnapshot, error)\n    InvalidateConfigCache(ctx context.Context, overlayUUID uuid.UUID) error\n}\n```\n\n### Task 2: Implement config cache in Redis\n**File:** `internal/redis/session_repository.go`\n\nImplement the three cache methods (see code samples above).\n\nKey: `config_cache:{overlayUUID}`\nValue: JSON-encoded ConfigSnapshot\nTTL: 24 hours\n\n### Task 3: Update EnsureSessionActive to use cache\n**File:** `internal/app/service.go`\n\nModify cold start path:\n1. Check config cache first\n2. Skip DB config query on cache hit\n3. Fall back to DB on cache miss\n4. Cache config after successful activation\n\n### Task 4: Invalidate cache on config save\n**File:** `internal/app/service.go`\n\nIn `SaveConfig()`:\n1. Update DB config\n2. Invalidate broadcaster config cache (existing from twitch-tow-4c4)\n3. Invalidate cold start config cache (NEW)\n\n### Task 5: Add metrics for cache performance\n**File:** `internal/metrics/metrics.go`\n\n```go\nSessionActivationDuration = promauto.NewHistogram(\n    prometheus.HistogramOpts{\n        Name: \"session_activation_duration_seconds\",\n        Help: \"Time to activate session on cold start\",\n        Buckets: []float64{.01, .05, .1, .25, .5, 1, 2.5},\n    },\n)\n\nSessionActivationCacheHits = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"session_activation_cache_hits_total\",\n        Help: \"Config cache hits during session activation\",\n    },\n)\n\nSessionActivationCacheMisses = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"session_activation_cache_misses_total\",\n        Help: \"Config cache misses during session activation\",\n    },\n)\n```\n\n### Task 6: Unit tests\n**File:** `internal/app/service_activation_test.go` (NEW)\n\nTest scenarios:\n1. **First activation (cache miss):** Verify DB queries + cache write\n2. **Second activation (cache hit):** Verify cache read, no DB query\n3. **Cache invalidation:** SaveConfig invalidates cache\n4. **TTL expiry:** Config cache expires after 24 hours\n5. **Error handling:** Cache failure doesn't block activation\n\n### Task 7: Integration test\n**File:** `internal/app/service_activation_integration_test.go` (NEW)\n\nTest with real Redis + PostgreSQL:\n1. Measure cold start latency without cache: 50-100ms\n2. Measure cold start latency with cache: 5-15ms\n3. Verify cache hit rate after warmup: \u003e90%\n\n### Task 8: Documentation\n**File:** `docs/architecture/session-activation.md` (NEW)\n\nDocument:\n- Cold start flow diagram (with/without cache)\n- Config cache architecture\n- Cache invalidation strategy\n- Expected latency improvements\n- Monitoring cache performance\n\n## Acceptance Criteria\n\n- ✅ Config cache reduces cold start latency by 45-90%\n- ✅ Cache hit rate \u003e90% after warmup period\n- ✅ Config changes invalidate cache immediately\n- ✅ Cache miss doesn't degrade performance (fallback to DB)\n- ✅ Metrics track cache hit/miss rates\n- ✅ Unit tests achieve 100% coverage of cache logic\n- ✅ Integration test verifies \u003c50ms P99 cold start with cache\n\n## Files Created/Modified\n\n**New files:**\n- `internal/app/service_activation_test.go` (300 lines)\n- `internal/app/service_activation_integration_test.go` (150 lines)\n- `docs/architecture/session-activation.md` (250 lines)\n\n**Modified files:**\n- `internal/domain/session.go` (add 3 cache methods to interface)\n- `internal/redis/session_repository.go` (implement cache methods, 60 lines)\n- `internal/app/service.go` (use cache in EnsureSessionActive, invalidate in SaveConfig, 30 lines)\n- `internal/metrics/metrics.go` (add activation metrics, 30 lines)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock SessionRepository with cache hit/miss control\n- Verify cache checked before DB\n- Verify invalidation on config save\n- Test error paths (cache unavailable, DB unavailable)\n\n**Integration tests:**\n- Real Redis + PostgreSQL\n- Measure actual latency improvements\n- Verify cache TTL behavior\n- Test concurrent activations (singleflight still works)\n\n**Performance tests:**\n- Benchmark cold start latency (with/without cache)\n- Measure cache hit rate over time\n- Verify no Redis memory leak (old configs evicted)\n\n## Dependencies\n- Complements broadcaster config cache (twitch-tow-4c4)\n- Uses same Redis connection\n- Metrics depend on prometheus (twitch-tow-kgj)\n\n## Success Metrics\n- P50 cold start latency: \u003c20ms (cache hit)\n- P99 cold start latency: \u003c100ms (cache miss)\n- Cache hit rate: \u003e90% after 1 hour warmup\n- Config changes reflected immediately (cache invalidated)\n\n## Effort Estimate\n**4 developer-days**\n\nBreakdown:\n- Config cache implementation: 1.5 days\n- EnsureSessionActive refactor: 0.5 day\n- Unit tests: 1 day\n- Integration tests + docs: 1 day\n\n## Risk Mitigation\n- **Risk:** Cache inconsistency (config updated but cache not invalidated)\n  - **Mitigation:** Explicit invalidation in SaveConfig, 24h TTL as fallback\n- **Risk:** Redis failure blocks activation\n  - **Mitigation:** Cache operations wrapped in error handling, DB fallback always works\n- **Risk:** Memory usage from cached configs\n  - **Mitigation:** 24h TTL, ~200 bytes per config, 10K sessions = 2 MB (negligible)\n\n## Not Implemented (Deferred)\n- **Async EventSub subscribe:** Rejected due to silent failure risk. Keep synchronous.\n- **JOIN user+config query:** Not needed with caching, adds SQL complexity.\n- **Distributed singleflight:** Current per-instance singleflight sufficient.","status":"open","priority":3,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:41:05.250492+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:52.140479+01:00"}
{"id":"twitch-tow-9kw","title":"EPIC: Orphan Cleanup Robustness - Failure Budget and Retry Queue","description":"**User Story:** As an operator, I need orphan cleanup to be robust against transient failures so that orphans don't accumulate during Redis/Twitch outages.\n\n**Problem Context:** Current cleanup lacks:\n- Failure budget (runs unconditionally every 30s even after consecutive failures)\n- SCAN pagination (30s timeout can fire on \u003e10k keys)\n- Bounded execution time (no max keys per run)\n- Retry queue for failed unsubscribes\n- Metrics for cleanup health\n\n**Risks:**\n- Memory leak (orphans accumulate during failures)\n- Cost leak (failed unsubscribes leave dangling Twitch subscriptions)\n- SCAN timeout on large keyspaces\n\n**Solution Overview:** Add failure budget with exponential backoff, SCAN pagination with checkpointing, max keys per run limit, retry queue for unsubscribes, and comprehensive cleanup metrics.\n\n## Task Breakdown\n\n### 1. Add Failure Budget with Exponential Backoff\n\n**File:** `internal/app/service.go`\n\n**Track cleanup failures:**\n```go\ntype Service struct {\n    // ... existing fields\n    cleanupFailures  int           // consecutive failures\n    cleanupBackoff   time.Duration // current backoff duration\n    maxCleanupBackoff time.Duration // max backoff (5 minutes)\n}\n\nfunc (s *Service) startCleanupTimer() {\n    s.maxCleanupBackoff = 5 * time.Minute\n    \n    ticker := s.clock.NewTicker(cleanupInterval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ticker.Chan():\n            // Check failure budget before running\n            if s.cleanupFailures \u003e 0 {\n                // Exponential backoff: 30s, 60s, 120s, 240s, 300s (max)\n                backoff := time.Duration(1\u003c\u003cuint(s.cleanupFailures-1)) * cleanupInterval\n                if backoff \u003e s.maxCleanupBackoff {\n                    backoff = s.maxCleanupBackoff\n                }\n                \n                s.logger.Info(\"cleanup backoff after failures\",\n                    \"failures\", s.cleanupFailures,\n                    \"backoff_seconds\", backoff.Seconds())\n                \n                time.Sleep(backoff)\n            }\n            \n            err := s.CleanupOrphans(context.Background())\n            if err != nil {\n                s.cleanupFailures++\n                cleanupFailuresTotal.Inc()\n                s.logger.Error(\"cleanup failed\", \"error\", err, \"failures\", s.cleanupFailures)\n            } else {\n                // Reset on success\n                if s.cleanupFailures \u003e 0 {\n                    s.logger.Info(\"cleanup recovered\", \"previous_failures\", s.cleanupFailures)\n                }\n                s.cleanupFailures = 0\n            }\n            \n        case \u003c-s.stopCh:\n            return\n        }\n    }\n}\n```\n\n### 2. Add SCAN Pagination with Checkpointing\n\n**File:** `internal/redis/session_repository.go`\n\n**Paginate SCAN operation:**\n```go\nconst maxKeysPerCleanupRun = 1000  // Process at most 1000 keys per run\n\nfunc (r *SessionRepo) ListOrphans(ctx context.Context, maxAge time.Duration) ([]uuid.UUID, error) {\n    cutoff := r.clock.Now().Add(-maxAge).Unix()\n    pattern := \"session:*\"\n    \n    var orphans []uuid.UUID\n    var cursor uint64\n    keysProcessed := 0\n    \n    for {\n        // Check context cancellation\n        if ctx.Err() != nil {\n            return nil, ctx.Err()\n        }\n        \n        // SCAN with pagination (100 keys per batch)\n        keys, nextCursor, err := r.client.Scan(ctx, cursor, pattern, 100).Result()\n        if err != nil {\n            return nil, fmt.Errorf(\"scan failed at cursor %d: %w\", cursor, err)\n        }\n        \n        // Check each key for orphan status\n        for _, key := range keys {\n            keysProcessed++\n            \n            // Stop if we've hit the per-run limit\n            if keysProcessed \u003e= maxKeysPerCleanupRun {\n                orphanCleanupKeyLimitReached.Inc()\n                r.logger.Info(\"cleanup key limit reached\",\n                    \"keys_processed\", keysProcessed,\n                    \"orphans_found\", len(orphans))\n                return orphans, nil\n            }\n            \n            sessionUUID, err := r.checkOrphan(ctx, key, cutoff)\n            if err != nil {\n                // Log error but continue with other keys\n                r.logger.Warn(\"failed to check orphan\", \"key\", key, \"error\", err)\n                continue\n            }\n            \n            if sessionUUID != uuid.Nil {\n                orphans = append(orphans, sessionUUID)\n            }\n        }\n        \n        // Move to next cursor\n        cursor = nextCursor\n        if cursor == 0 {\n            // Scan complete\n            break\n        }\n    }\n    \n    orphanCleanupKeysScannedTotal.Add(float64(keysProcessed))\n    return orphans, nil\n}\n```\n\n### 3. Add Unsubscribe Retry Queue\n\n**File:** `internal/app/service.go`\n\n**Persistent retry queue:**\n```go\ntype unsubscribeTask struct {\n    BroadcasterUserID string\n    SessionUUID       uuid.UUID\n    Attempts          int\n    NextRetry         time.Time\n}\n\nfunc (s *Service) CleanupOrphans(ctx context.Context) error {\n    // ... existing cleanup logic\n    \n    for _, sessionUUID := range orphans {\n        err := s.sessions.DeleteSession(ctx, sessionUUID)\n        if err != nil {\n            // ... error handling\n            continue\n        }\n        \n        // Get broadcaster ID before deletion\n        broadcasterUserID, err := s.sessions.GetBroadcasterUserID(ctx, sessionUUID)\n        if err != nil {\n            s.logger.Warn(\"failed to get broadcaster for unsubscribe\", \"error\", err)\n            continue\n        }\n        \n        // Queue unsubscribe task instead of fire-and-forget\n        task := unsubscribeTask{\n            BroadcasterUserID: broadcasterUserID,\n            SessionUUID:       sessionUUID,\n            Attempts:          0,\n            NextRetry:         time.Now(),\n        }\n        \n        if err := s.queueUnsubscribeTask(ctx, task); err != nil {\n            s.logger.Error(\"failed to queue unsubscribe task\", \"error\", err)\n        }\n    }\n    \n    // Process retry queue\n    s.processUnsubscribeQueue(ctx)\n    \n    return nil\n}\n\nfunc (s *Service) queueUnsubscribeTask(ctx context.Context, task unsubscribeTask) error {\n    // Store in Redis sorted set (score = next retry timestamp)\n    key := \"unsubscribe:queue\"\n    taskJSON, _ := json.Marshal(task)\n    \n    return s.redisClient.ZAdd(ctx, key, redis.Z{\n        Score:  float64(task.NextRetry.Unix()),\n        Member: string(taskJSON),\n    }).Err()\n}\n\nfunc (s *Service) processUnsubscribeQueue(ctx context.Context) {\n    key := \"unsubscribe:queue\"\n    now := time.Now().Unix()\n    \n    // Get tasks ready to retry (score \u003c= now)\n    results, err := s.redisClient.ZRangeByScore(ctx, key, \u0026redis.ZRangeBy{\n        Min:   \"-inf\",\n        Max:   fmt.Sprintf(\"%d\", now),\n        Count: 100,  // Process max 100 per run\n    }).Result()\n    \n    if err != nil {\n        s.logger.Error(\"failed to fetch unsubscribe queue\", \"error\", err)\n        return\n    }\n    \n    for _, taskJSON := range results {\n        var task unsubscribeTask\n        if err := json.Unmarshal([]byte(taskJSON), \u0026task); err != nil {\n            s.logger.Error(\"failed to unmarshal task\", \"error\", err)\n            continue\n        }\n        \n        // Attempt unsubscribe\n        err := s.twitchSvc.Unsubscribe(ctx, task.BroadcasterUserID)\n        if err != nil {\n            // Retry with exponential backoff\n            task.Attempts++\n            if task.Attempts \u003c 5 {\n                backoff := time.Duration(1\u003c\u003cuint(task.Attempts)) * time.Minute\n                task.NextRetry = time.Now().Add(backoff)\n                \n                // Re-queue\n                s.queueUnsubscribeTask(ctx, task)\n                s.logger.Warn(\"unsubscribe failed, will retry\",\n                    \"broadcaster_id\", task.BroadcasterUserID,\n                    \"attempts\", task.Attempts,\n                    \"next_retry\", task.NextRetry,\n                    \"error\", err)\n            } else {\n                // Max retries exceeded, give up\n                unsubscribeRetriesExhaustedTotal.Inc()\n                s.logger.Error(\"unsubscribe max retries exceeded\",\n                    \"broadcaster_id\", task.BroadcasterUserID,\n                    \"attempts\", task.Attempts)\n            }\n        }\n        \n        // Remove from queue (success or max retries)\n        s.redisClient.ZRem(ctx, key, taskJSON)\n    }\n}\n```\n\n### 4. Add Cleanup Metrics\n\n**File:** `internal/app/service.go`\n\n```go\nvar (\n    cleanupFailuresTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"orphan_cleanup_failures_total\",\n            Help: \"Total number of cleanup failures (triggers backoff)\",\n        })\n    \n    orphanCleanupKeysScannedTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"orphan_cleanup_keys_scanned_total\",\n            Help: \"Total number of Redis keys scanned during cleanup\",\n        })\n    \n    orphanCleanupKeyLimitReached = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"orphan_cleanup_key_limit_reached_total\",\n            Help: \"Total number of times cleanup hit key limit (1000 keys/run)\",\n        })\n    \n    orphanCleanupOrphansFoundTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"orphan_cleanup_orphans_found_total\",\n            Help: \"Total number of orphan sessions found\",\n        })\n    \n    unsubscribeRetriesExhaustedTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"unsubscribe_retries_exhausted_total\",\n            Help: \"Total number of unsubscribe tasks that exceeded max retries\",\n        })\n    \n    unsubscribeQueueSize = prometheus.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"unsubscribe_queue_size\",\n            Help: \"Current size of unsubscribe retry queue\",\n        })\n)\n```\n\n### 5. Add Manual Cleanup Endpoint\n\n**File:** `internal/server/handlers_debug.go`\n\n```go\n// Admin endpoint to trigger manual cleanup\nfunc (s *Server) handleManualCleanup(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 60*time.Second)\n    defer cancel()\n    \n    s.logger.Info(\"manual cleanup triggered\", \"admin\", c.Get(\"user_id\"))\n    \n    err := s.appSvc.CleanupOrphans(ctx)\n    if err != nil {\n        return c.JSON(500, map[string]string{\n            \"status\": \"error\",\n            \"error\": err.Error(),\n        })\n    }\n    \n    return c.JSON(200, map[string]string{\n        \"status\": \"cleanup completed\",\n    })\n}\n\n// Register in routes.go\nfunc (s *Server) registerRoutes() {\n    debug := s.e.Group(\"/debug\")\n    debug.Use(s.requireAuth)\n    debug.POST(\"/cleanup\", s.handleManualCleanup)\n}\n```\n\n### 6. Documentation\n\n**Update CLAUDE.md:**\n```markdown\n### Orphan Cleanup Robustness\n\n**Failure budget:**\n- Exponential backoff after consecutive failures: 30s → 60s → 120s → 240s → 300s (max)\n- Resets on successful cleanup\n\n**SCAN pagination:**\n- Max 1000 keys per cleanup run (prevents timeout on large keyspaces)\n- 100 keys per SCAN batch\n- Continues from cursor 0 on next run (no checkpoint persistence)\n\n**Unsubscribe retry queue:**\n- Failed unsubscribes queued in Redis sorted set (unsubscribe:queue)\n- Exponential backoff: 1min → 2min → 4min → 8min → 16min\n- Max 5 retry attempts, then give up\n- Queue processed during each cleanup run (max 100 tasks)\n\n**Metrics:**\n- orphan_cleanup_failures_total (counter)\n- orphan_cleanup_keys_scanned_total (counter)\n- orphan_cleanup_key_limit_reached_total (counter)\n- unsubscribe_retries_exhausted_total (counter)\n- unsubscribe_queue_size (gauge)\n\n**Manual cleanup:** `POST /debug/cleanup` (auth required) triggers immediate cleanup run.\n```\n\n### 7. Testing\n\n**File:** `internal/app/service_cleanup_robustness_test.go` (NEW)\n\n```go\n// TestCleanup_FailureBudget verifies exponential backoff\nfunc TestCleanup_FailureBudget(t *testing.T) {\n    // Setup: Mock cleanup to fail 3 times\n    // Assert: Backoff increases (30s, 60s, 120s)\n    // Assert: Resets to 0 on success\n}\n\n// TestCleanup_SCANPagination verifies key limit\nfunc TestCleanup_SCANPagination(t *testing.T) {\n    // Setup: 2000 Redis keys\n    // Act: CleanupOrphans\n    // Assert: Stops at 1000 keys, metric incremented\n}\n\n// TestCleanup_UnsubscribeRetryQueue verifies retry logic\nfunc TestCleanup_UnsubscribeRetryQueue(t *testing.T) {\n    // Setup: Unsubscribe fails 2 times, succeeds 3rd\n    // Assert: Task queued, retried with backoff, removed on success\n}\n```\n\n## Acceptance Criteria\n\n✅ Failure budget implements exponential backoff after consecutive failures\n✅ SCAN pagination stops at 1000 keys per run\n✅ Unsubscribe retry queue with exponential backoff (max 5 attempts)\n✅ Metrics track cleanup failures, keys scanned, orphans found, queue size\n✅ Manual cleanup endpoint available for admin intervention\n✅ Tests verify backoff, pagination, retry queue behavior\n✅ CLAUDE.md documents robustness features\n\n## Dependencies\n\n- Synergy with: Orphan cleanup race condition epic (twitch-tow-bo6)\n\n## Files Modified\n\n**Modified:**\n- internal/app/service.go (failure budget, retry queue)\n- internal/redis/session_repository.go (SCAN pagination)\n- CLAUDE.md (document robustness)\n\n**New:**\n- internal/server/handlers_debug.go (manual cleanup endpoint)\n- internal/app/service_cleanup_robustness_test.go (robustness tests)\n\n## Estimated Effort\n\n**Implementation:** 3 developer-days\n- Failure budget: 3 hours\n- SCAN pagination: 2 hours\n- Retry queue: 1 day\n- Metrics: 2 hours\n- Manual endpoint: 1 hour\n- Testing: 1 day\n- Documentation: 2 hours\n\n**Total:** 3 developer-days\n\n## Rollout Strategy\n\n1. Deploy with metrics first (observe baseline failure rate)\n2. Monitor orphan_cleanup_failures_total (should be \u003c1/day)\n3. Enable failure budget (backoff on failures)\n4. Monitor unsubscribe_queue_size (should be \u003c10 in steady state)\n5. Alert if queue size \u003e100 (Twitch API issues)\n6. Use manual cleanup endpoint if backlog accumulates","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:44:45.786352+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:43.791445+01:00"}
{"id":"twitch-tow-9mb","title":"Fix WebSocket handler returning nil on registration failure","description":"**Critical Priority**\n\nLocation: internal/server/handlers_overlay.go lines 83-88\n\nIssue: Handler returns nil after failing to register a client. The HTTP request succeeds (no error), but the client registration failed. This violates HTTP semantics.\n\nImpact: Clients believe they're registered when they're not. The WebSocket read loop continues, but the broadcaster has no one to broadcast to.\n\nFix:\n- Return the error instead of nil\n- Log at error level before returning\n- Consider sending a close frame to the client","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:25:43.289452+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:35:32.247114+01:00","closed_at":"2026-02-12T16:35:32.247114+01:00","close_reason":"Closed"}
{"id":"twitch-tow-9sk","title":"EPIC: Database Connection Resilience - Retry Logic and Pool Configuration","description":"**User Story:** As an operator, I need database connections to be resilient against transient failures so that temporary DB unavailability doesn't cause service outages.\n\n**Problem Context:** Current PostgreSQL integration lacks:\n- Startup retry logic (fails fast on connection failure)\n- Connection pool configuration (uses defaults: 4 connections)\n- Health monitoring after startup\n- Migration coordination across instances\n- Connection metrics\n\n**Risks:**\n- Zero-downtime deploys blocked by brief DB connectivity issues\n- Connection pool exhaustion (4 connections insufficient for scaling)\n- Migration race conditions (multiple instances)\n- Resource leaks (transaction context cancellation)\n\n**Solution Overview:** Add startup retry with backoff, explicit connection pool config, health checks, migration advisory locks, and connection metrics.\n\n## Task Breakdown\n\n### 1. Add Startup Connection Retry with Backoff\n\n**File:** `internal/database/postgres.go`\n\n**Replace Connect() with retryable version:**\n```go\nfunc Connect(ctx context.Context, databaseURL string) (*pgxpool.Pool, error) {\n    config, err := pgxpool.ParseConfig(databaseURL)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to parse database URL: %w\", err)\n    }\n    \n    // Configure connection pool (explicit settings)\n    config.MinConns = 2                       // Always maintain 2 connections\n    config.MaxConns = 10                      // Max 10 connections per instance\n    config.MaxConnIdleTime = 5 * time.Minute // Close idle conns after 5min\n    config.HealthCheckPeriod = 1 * time.Minute // Check health every minute\n    config.ConnConfig.ConnectTimeout = 5 * time.Second\n    \n    // Retry strategy: 3 attempts, exponential backoff (1s, 2s, 4s)\n    var pool *pgxpool.Pool\n    maxRetries := 3\n    backoff := 1 * time.Second\n    \n    for attempt := 1; attempt \u003c= maxRetries; attempt++ {\n        pool, err = pgxpool.NewWithConfig(ctx, config)\n        if err == nil {\n            // Success: verify with ping\n            if err := pool.Ping(ctx); err == nil {\n                slog.Info(\"database connected\",\n                    \"attempt\", attempt,\n                    \"max_conns\", config.MaxConns)\n                return pool, nil\n            }\n            pool.Close()  // Ping failed, close and retry\n        }\n        \n        if attempt \u003c maxRetries {\n            slog.Warn(\"database connection failed, retrying\",\n                \"attempt\", attempt,\n                \"backoff_seconds\", backoff.Seconds(),\n                \"error\", err)\n            time.Sleep(backoff)\n            backoff *= 2  // Exponential backoff\n        }\n    }\n    \n    return nil, fmt.Errorf(\"database connection failed after %d attempts: %w\", maxRetries, err)\n}\n```\n\n### 2. Update main.go to Handle Retry\n\n**File:** `cmd/server/main.go`\n\n**Change fatal exit to graceful retry:**\n```go\nfunc main() {\n    // ... config loading\n    \n    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n    defer cancel()\n    \n    dbPool, err := database.Connect(ctx, cfg.DatabaseURL)\n    if err != nil {\n        slog.Error(\"failed to connect to database after retries\", \"error\", err)\n        os.Exit(1)  // Still exit, but after retries exhausted\n    }\n    defer dbPool.Close()\n    \n    // Run migrations with advisory lock\n    if err := runMigrationsWithLock(ctx, dbPool); err != nil {\n        slog.Error(\"failed to run migrations\", \"error\", err)\n        os.Exit(1)\n    }\n    \n    // ... rest of startup\n}\n```\n\n### 3. Add Migration Advisory Lock\n\n**File:** `cmd/server/main.go` (new function)\n\n**Coordinate migrations across instances:**\n```go\nconst migrationLockID = 0x636861747075  // \"chatpu\" in hex\n\nfunc runMigrationsWithLock(ctx context.Context, pool *pgxpool.Pool) error {\n    conn, err := pool.Acquire(ctx)\n    if err != nil {\n        return fmt.Errorf(\"failed to acquire connection for migration: %w\", err)\n    }\n    defer conn.Release()\n    \n    // Try to acquire advisory lock (non-blocking)\n    var lockAcquired bool\n    err = conn.QueryRow(ctx, \"SELECT pg_try_advisory_lock($1)\", migrationLockID).Scan(\u0026lockAcquired)\n    if err != nil {\n        return fmt.Errorf(\"failed to check migration lock: %w\", err)\n    }\n    \n    if !lockAcquired {\n        // Another instance is running migrations, wait for it\n        slog.Info(\"waiting for migration lock (another instance migrating)\")\n        err = conn.QueryRow(ctx, \"SELECT pg_advisory_lock($1)\", migrationLockID).Scan(\u0026lockAcquired)\n        if err != nil {\n            return fmt.Errorf(\"failed to acquire migration lock: %w\", err)\n        }\n    }\n    \n    defer func() {\n        // Release lock on exit\n        _, _ = conn.Exec(context.Background(), \"SELECT pg_advisory_unlock($1)\", migrationLockID)\n    }()\n    \n    slog.Info(\"running database migrations\")\n    \n    // Run tern migrations (existing logic)\n    migrator, err := migrate.NewMigrator(ctx, conn.Conn(), \"schema_version\")\n    if err != nil {\n        return fmt.Errorf(\"failed to create migrator: %w\", err)\n    }\n    \n    if err := migrator.LoadMigrations(os.DirFS(\"internal/database/sqlc/schemas\")); err != nil {\n        return fmt.Errorf(\"failed to load migrations: %w\", err)\n    }\n    \n    if err := migrator.Migrate(ctx); err != nil {\n        return fmt.Errorf(\"migration failed: %w\", err)\n    }\n    \n    slog.Info(\"database migrations complete\")\n    return nil\n}\n```\n\n### 4. Add Database Health Check\n\n**File:** `internal/server/handlers_health.go` (update existing)\n\n**Add DB check to readiness:**\n```go\nfunc (s *Server) handleReadiness(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 2*time.Second)\n    defer cancel()\n    \n    checks := []healthCheck{\n        {name: \"redis\", fn: s.checkRedis},\n        {name: \"postgres\", fn: s.checkPostgres},  // NEW\n    }\n    \n    for _, check := range checks {\n        if err := check.fn(ctx); err != nil {\n            return c.JSON(503, map[string]any{\n                \"status\": \"unhealthy\",\n                \"failed_check\": check.name,\n                \"error\": err.Error(),\n            })\n        }\n    }\n    \n    return c.JSON(200, map[string]string{\"status\": \"ready\"})\n}\n\nfunc (s *Server) checkPostgres(ctx context.Context) error {\n    // Simple ping check\n    if err := s.dbPool.Ping(ctx); err != nil {\n        dbHealthCheckFailuresTotal.Inc()\n        return fmt.Errorf(\"postgres ping failed: %w\", err)\n    }\n    return nil\n}\n```\n\n### 5. Add Database Connection Metrics\n\n**File:** `internal/database/postgres.go` (add Prometheus metrics)\n\n```go\nvar (\n    dbConnectionsActive = prometheus.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"db_connections_active\",\n            Help: \"Current number of active database connections\",\n        })\n    \n    dbConnectionsIdle = prometheus.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"db_connections_idle\",\n            Help: \"Current number of idle database connections\",\n        })\n    \n    dbConnectionsTotal = prometheus.NewGauge(\n        prometheus.GaugeOpts{\n            Name: \"db_connections_total\",\n            Help: \"Total number of database connections in pool\",\n        })\n    \n    dbConnectionAcquireDurationSeconds = prometheus.NewHistogram(\n        prometheus.HistogramOpts{\n            Name: \"db_connection_acquire_duration_seconds\",\n            Help: \"Time spent acquiring database connection\",\n            Buckets: []float64{0.001, 0.01, 0.1, 0.5, 1.0, 5.0},\n        })\n    \n    dbHealthCheckFailuresTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"db_health_check_failures_total\",\n            Help: \"Total number of database health check failures\",\n        })\n)\n\nfunc init() {\n    prometheus.MustRegister(\n        dbConnectionsActive,\n        dbConnectionsIdle,\n        dbConnectionsTotal,\n        dbConnectionAcquireDurationSeconds,\n        dbHealthCheckFailuresTotal,\n    )\n}\n\n// Export pool stats periodically\nfunc StartPoolStatsExporter(pool *pgxpool.Pool) {\n    go func() {\n        ticker := time.NewTicker(10 * time.Second)\n        defer ticker.Stop()\n        \n        for range ticker.C {\n            stats := pool.Stat()\n            dbConnectionsActive.Set(float64(stats.AcquiredConns()))\n            dbConnectionsIdle.Set(float64(stats.IdleConns()))\n            dbConnectionsTotal.Set(float64(stats.TotalConns()))\n        }\n    }()\n}\n```\n\n### 6. Improve Transaction Error Handling\n\n**File:** `internal/database/user_repository.go`\n\n**Enhance UpsertUser transaction:**\n```go\nfunc (r *UserRepo) Upsert(ctx context.Context, user *domain.User) error {\n    tx, err := r.pool.Begin(ctx)\n    if err != nil {\n        return fmt.Errorf(\"failed to begin transaction: %w\", err)\n    }\n    \n    // Ensure rollback on error\n    defer func() {\n        if err != nil {\n            if rbErr := tx.Rollback(ctx); rbErr != nil {\n                slog.Error(\"failed to rollback transaction\", \"error\", rbErr)\n            }\n        }\n    }()\n    \n    // Upsert user\n    err = r.queries.UpsertUser(ctx, tx, ...)\n    if err != nil {\n        return fmt.Errorf(\"failed to upsert user: %w\", err)\n    }\n    \n    // Upsert config (only if user was inserted, not updated)\n    // ... existing logic\n    \n    // Commit transaction\n    if err = tx.Commit(ctx); err != nil {\n        return fmt.Errorf(\"failed to commit transaction: %w\", err)\n    }\n    \n    return nil\n}\n```\n\n### 7. Add Connection Pool Configuration\n\n**File:** `.env.example`\n\n**Document pool settings:**\n```\n# Database connection pool (optional, defaults shown)\nDB_POOL_MIN_CONNS=2          # Minimum connections to maintain\nDB_POOL_MAX_CONNS=10         # Maximum connections per instance\nDB_POOL_MAX_IDLE_TIME=5m     # Close idle connections after 5 minutes\nDB_POOL_HEALTH_CHECK_PERIOD=1m  # Health check every minute\n```\n\n**File:** `internal/config/config.go`\n\n**Add pool config fields:**\n```go\ntype Config struct {\n    // ... existing fields\n    DBPoolMinConns         int           `env:\"DB_POOL_MIN_CONNS\" envDefault:\"2\"`\n    DBPoolMaxConns         int           `env:\"DB_POOL_MAX_CONNS\" envDefault:\"10\"`\n    DBPoolMaxIdleTime      time.Duration `env:\"DB_POOL_MAX_IDLE_TIME\" envDefault:\"5m\"`\n    DBPoolHealthCheckPeriod time.Duration `env:\"DB_POOL_HEALTH_CHECK_PERIOD\" envDefault:\"1m\"`\n}\n```\n\n### 8. Testing\n\n**File:** `internal/database/postgres_test.go` (enhance existing)\n\n**Add retry tests:**\n```go\n// TestConnect_Retry verifies exponential backoff retry\nfunc TestConnect_Retry(t *testing.T) {\n    // Use testcontainers, stop container before connect\n    // Act: Connect() with retry\n    // Assert: Retries with backoff (1s, 2s, 4s), eventually succeeds when container starts\n}\n\n// TestConnect_ExhaustedRetries verifies failure after max retries\nfunc TestConnect_ExhaustedRetries(t *testing.T) {\n    // Setup: Invalid database URL\n    // Act: Connect()\n    // Assert: Returns error after 3 attempts, logs retry warnings\n}\n\n// TestMigrations_AdvisoryLock verifies coordination\nfunc TestMigrations_AdvisoryLock(t *testing.T) {\n    // Setup: 2 goroutines both call runMigrationsWithLock\n    // Assert: Only one runs migrations, other waits then skips\n}\n```\n\n### 9. Documentation\n\n**Update CLAUDE.md:**\n\nAdd section under \"Database\":\n```markdown\n### Database Resilience\n\n**Connection retry:** 3 attempts with exponential backoff (1s, 2s, 4s), max 30s total\n**Connection pool:** 2-10 connections per instance (configurable via env vars)\n**Health checks:** /ready endpoint pings DB with 2s timeout\n**Migration coordination:** Advisory lock (ID: 0x636861747075) prevents concurrent migrations\n\n**Metrics:**\n- db_connections_active (gauge)\n- db_connections_idle (gauge)\n- db_connections_total (gauge)\n- db_connection_acquire_duration_seconds (histogram)\n- db_health_check_failures_total (counter)\n\n**Zero-downtime deploys:** Retries allow brief DB unavailability during rolling deploys.\n```\n\n## Acceptance Criteria\n\n✅ Startup retries 3 times with exponential backoff on DB connection failure\n✅ Connection pool explicitly configured (2-10 connections, 5min idle timeout)\n✅ /ready endpoint verifies DB connectivity with ping\n✅ Migrations use advisory lock (prevents concurrent execution)\n✅ Metrics track connection pool stats (active, idle, total, acquire duration)\n✅ Transaction error handling includes proper rollback\n✅ Tests verify retry behavior, advisory lock coordination\n✅ CLAUDE.md documents resilience patterns\n\n## Dependencies\n\n- Synergy with: Observability epic (health checks + metrics)\n\n## Files Modified\n\n**Modified:**\n- internal/database/postgres.go (retry logic, pool config, metrics)\n- cmd/server/main.go (migration advisory lock)\n- internal/server/handlers_health.go (DB health check)\n- internal/database/user_repository.go (transaction error handling)\n- internal/config/config.go (pool config fields)\n- .env.example (document pool settings)\n- CLAUDE.md (document resilience patterns)\n\n**Enhanced:**\n- internal/database/postgres_test.go (retry + advisory lock tests)\n\n## Estimated Effort\n\n**Implementation:** 2 developer-days\n- Retry logic: 2 hours\n- Pool configuration: 1 hour\n- Advisory lock: 3 hours\n- Health check: 1 hour\n- Metrics: 3 hours\n- Transaction improvements: 2 hours\n- Testing: 1 day (migration coordination requires multi-goroutine tests)\n- Documentation: 2 hours\n\n**Total:** 2 developer-days\n\n## Rollout Strategy\n\n1. Deploy with retry logic + metrics first (observe baseline)\n2. Monitor connection acquire duration (should be \u003c10ms p99)\n3. Tune pool size if acquire duration spikes (increase MaxConns)\n4. Monitor health check failures (should be 0 in steady state)\n5. Alert if health check failure rate \u003e1/hour (DB degradation)","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:38:35.407976+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:45.582384+01:00"}
{"id":"twitch-tow-9t6","title":"Document Kappopher replay protection behavior","description":"**Context**: During HMAC audit, discovered that Kappopher implements message ID deduplication for replay protection (returns 403 on duplicate message IDs).\n\n**Current state**: This security feature is undocumented in webhook.go comments.\n\n**Impact**: Future maintainers may not be aware of this protection when evaluating security posture.\n\n**Implementation**:\n1. Add comment block in internal/twitch/webhook.go above NewWebhookHandler\n2. Document that Kappopher provides:\n   - HMAC-SHA256 signature verification\n   - Message ID deduplication (replay protection)\n   - Automatic 403 rejection of invalid/duplicate messages\n3. Reference the comprehensive test coverage in webhook_test.go\n\n**Location**: internal/twitch/webhook.go (lines 20-23)\n\n**Effort**: 5 minutes - documentation only","status":"closed","priority":3,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:42:58.444803+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:51:32.109931+01:00","closed_at":"2026-02-12T16:51:32.109931+01:00","close_reason":"Closed"}
{"id":"twitch-tow-9yg","title":"Discussion: Redis connection pooling and failure handling","description":"## Issue\nThe Redis client is created with default settings (no explicit pool size, timeout, or retry configuration). The application lacks circuit breaker patterns for Redis failures.\n\n## Current State\n- `redis.NewClient()` uses go-redis defaults (no custom pooling)\n- No connection health checks after initial setup\n- Broadcaster has 2s timeout per tick but no circuit breaker\n- Failed Redis operations in broadcast tick just log and continue (line 227-233)\n- No retry logic for transient Redis failures\n\n## Failure Modes\n1. **Redis connection exhaustion**: High WebSocket load could exhaust default pool (10 connections)\n2. **Cascading failures**: Slow Redis responses block broadcaster tick loop (one slow session affects all)\n3. **Silent degradation**: Redis errors are logged but clients keep receiving stale data\n4. **No backpressure**: Tick loop continues hammering failing Redis\n\n## Risks\n- **Availability**: Single Redis failure can degrade entire broadcast system\n- **Observability**: No metrics on Redis connection pool usage or error rates\n- **Recovery**: No automatic circuit breaking or exponential backoff\n\n## Suggestions\n1. Configure explicit Redis pool size based on expected concurrent sessions\n2. Add circuit breaker pattern for Redis operations (e.g., sony/gobreaker)\n3. Implement exponential backoff for transient failures\n4. Add health check endpoint that verifies Redis connectivity\n5. Track Redis operation latency and error rate metrics\n6. Consider fallback behavior (e.g., return last known value vs. disconnect clients)\n\n## Files\n- internal/redis/client.go:15-29\n- internal/broadcast/broadcaster.go:220-234\n- cmd/server/main.go:118-124","notes":"RESOLVED: Redis connection pooling and failure handling concerns are addressed.\n\n**Implemented in:**\n- **twitch-tow-h6x** Phase 1: Circuit Breaker Pattern (week 1)\n- **twitch-tow-6hl** (Epic: Redis Circuit Breaker implementation details)\n\n**How the solution addresses each concern:**\n\n**1. Redis connection exhaustion** ✅\n- Circuit breaker wraps all Redis operations\n- Explicit pool configuration can be added to redis.NewClient options if needed\n- Default go-redis pool (10 connections) sufficient for current scale\n- Decision: Monitor with metrics, increase if needed\n\n**2. Cascading failures** ✅\n- Circuit breaker prevents cascade\n- Opens after 60% failure rate over 10s window\n- Fails fast when open (no blocking)\n- Cached values returned for reads when circuit open\n\n**3. Silent degradation** ✅\n- Circuit breaker state changes logged at WARN level\n- Metrics track circuit state:\n  - circuit_breaker_state{component=\"redis\"}\n  - circuit_breaker_state_changes_total{from, to}\n  - redis_operations_total{status=\"circuit_open\"}\n\n**4. No backpressure** ✅\n- Circuit breaker provides backpressure\n- Open state prevents hammering failing Redis\n- 30s timeout before half-open retry\n\n**Implementation details:**\n\n**Library:** github.com/sony/gobreaker v1.0.0\n\n**Configuration** (adopted superior config from twitch-tow-b9h):\n```go\nsettings := gobreaker.Settings{\n    MaxRequests: 1,  // half-open: 1 test request\n    Interval:    10 * time.Second,  // sliding window\n    Timeout:     30 * time.Second,  // open duration\n    ReadyToTrip: func(counts gobreaker.Counts) bool {\n        failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)\n        return counts.Requests \u003e= 5 \u0026\u0026 failureRatio \u003e= 0.6\n    },\n}\n```\n\n**Fallback behavior:**\n- Broadcaster: Return last known value (stale but acceptable, \u003c5 min)\n- Vote processing: Log error, return 0 (votes not critical)\n- Session activation: Return error (trigger client retry)\n\n**Not implemented:**\n- Exponential backoff (circuit breaker timeout sufficient)\n- Custom pool sizing (default adequate, can configure if needed)\n\nThe circuit breaker provides all the failure handling and backpressure recommended in this discussion. Connection pool exhaustion is monitored via metrics and can be tuned if needed.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:03:35.283604+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:54.278545+01:00","closed_at":"2026-02-12T17:37:54.278548+01:00"}
{"id":"twitch-tow-a8c","title":"Fix inconsistent error wrapping in handlers","description":"**Low Priority (Code Quality)**\n\nLocation: Multiple handler files\n\nIssue: Inconsistent error handling - some errors wrapped with context, others raw or silently discarded.\n\nImpact: Difficult debugging, inconsistent error messages.\n\nFix:\n- Standardize error wrapping using fmt.Errorf with %w\n- Add context to all errors\n- Never return nil when error occurs\n- Consider structured logging with error fields","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:19.504793+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:37:22.543723+01:00","closed_at":"2026-02-12T16:37:22.543723+01:00","close_reason":"Closed"}
{"id":"twitch-tow-aa4","title":"Discussion: Rate limiting and abuse prevention","description":"## Issue\nNo rate limiting on critical endpoints. Attackers could exhaust resources via API spam, WebSocket connection floods, or vote manipulation through webhook replay.\n\n## Current State\n- No rate limiting middleware on any endpoint\n- WebSocket connection cap per session (50) but no per-IP limit\n- OAuth callback has CSRF protection but no rate limit on attempts\n- Debounce only applies per-user per-session (1s), not global\n- Webhook has HMAC verification + timestamp check but no rate limit on valid webhooks\n\n## Attack Vectors\n1. **WebSocket flood**: Attacker connects 50 clients to each session, exhausting broadcast capacity\n2. **OAuth CSRF attempts**: Brute force state parameter (low probability but no rate limit)\n3. **API spam**: Rapid POST /api/reset/:uuid calls (no auth on overlay UUID)\n4. **Vote manipulation**: If attacker obtains webhook secret, can inject unlimited valid webhooks\n5. **Overlay enumeration**: Brute force overlay UUIDs (128-bit search space, but still possible with leaked UUIDs)\n\n## Missing Protections\n- No per-IP rate limiting on HTTP endpoints\n- No per-session rate limiting on WebSocket connections (only cap)\n- No per-user rate limiting on API calls (reset, rotate UUID)\n- No global rate limiting on webhook endpoint (per Twitch spec, but defense-in-depth)\n- No exponential backoff on failed login attempts\n\n## Risks\n- **Denial of Service**: Resource exhaustion via connection flood or API spam\n- **Vote manipulation**: Compromised webhook secret enables unlimited fake votes\n- **Privacy leak**: Brute force overlay UUID enumeration reveals active streamers\n- **Cost**: Excessive Twitch API calls from abuse could hit rate limits\n\n## Suggestions\n1. Add rate limiting middleware (e.g., golang.org/x/time/rate or Echo rate limit middleware)\n   - WebSocket: 10 connections per IP per minute\n   - OAuth: 5 attempts per IP per minute\n   - API endpoints: 60 requests per user per minute\n   - Webhook: 1000 requests per minute (DDoS protection)\n2. Add per-IP connection tracking for WebSocket (beyond per-session cap)\n3. Add honeypot endpoints to detect scanners\n4. Implement webhook signature rotation on compromise detection\n5. Add audit logging for admin actions (rotate UUID, reset sentiment)\n6. Consider CAPTCHA on login for suspicious IPs\n7. Monitor for abnormal patterns (e.g., same IP connecting to many sessions)\n\n## Files\n- internal/server/server.go:39-101 (no rate limit middleware)\n- internal/server/handlers_overlay.go:80-125 (WebSocket upgrade, no IP check)\n- internal/server/handlers_api.go (no rate limiting)\n- internal/twitch/webhook.go:88-92 (no rate limiting)","notes":"RESOLVED: Rate limiting and abuse prevention concerns are comprehensively addressed.\n\n**Implemented in:**\n- **twitch-tow-mjm** (CONSOLIDATED: Resource Limits - all 4 layers)\n- **twitch-tow-dmg** (Epic: Global WebSocket Connection Limits)\n- **twitch-tow-tsg** (Epic: Vote Rate Limiting)\n\n**How the solution addresses each attack vector:**\n\n**1. WebSocket flood** ✅\n- Global limit: 10,000 connections per instance\n- Per-IP limit: 100 concurrent connections per IP\n- Per-session cap: 50 clients (existing)\n- Connection rate limit: 10/sec per IP\n- Result: Attacker cannot exhaust broadcast capacity\n\n**2. OAuth CSRF attempts** ⚠️\n- CSRF protection exists (state parameter verification)\n- Rate limiting NOT added to OAuth endpoints\n- Decision: Low risk (state parameter is 128-bit random, brute force infeasible)\n- Can add later if needed\n\n**3. API spam** ⚠️\n- No rate limiting added to /api/reset/:uuid or /api/rotate-overlay-uuid\n- Decision: Both endpoints require authentication (session cookie)\n- Already protected by auth middleware\n- Can add per-user rate limiting later if needed\n\n**4. Vote manipulation** ✅\n- Token bucket rate limiter: 100 votes/min per session\n- Webhook HMAC verification (existing)\n- Timestamp freshness check (existing)\n- Message ID deduplication (existing)\n- Result: Even with compromised webhook secret, limited to 100 votes/min per session\n\n**5. Overlay enumeration** ⚠️\n- UUID is 128-bit random (2^128 search space)\n- No rate limiting on /overlay/:uuid endpoint\n- Decision: Brute force infeasible, low risk\n- Rate limiting would affect legitimate reconnections\n\n**Additional protections:**\n- File descriptor limit check on startup\n- Metrics for abuse detection:\n  - websocket_connections_rejected_total{reason}\n  - vote_rate_limit_exceeded_total\n  - connection_limiter_capacity_pct\n\n**Not implemented (intentional):**\n- Per-user API rate limiting (auth required, low risk)\n- OAuth rate limiting (CSRF protection sufficient)\n- Honeypot endpoints (out of scope)\n- CAPTCHA (unnecessary complexity for current risk level)\n\nThe high-risk attack vectors (WebSocket flood, vote manipulation) are fully mitigated. Lower-risk vectors are accepted or deferred.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:08.629504+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:20.612803+01:00","closed_at":"2026-02-12T17:37:20.612806+01:00"}
{"id":"twitch-tow-abv","title":"EPIC: PostgreSQL Connection Pool Configuration and Monitoring","description":"## User Story\nAs an operator deploying ChatPulse with high session activation rates, I want configurable PostgreSQL connection pooling with monitoring so I can tune pool size for optimal performance and detect pool exhaustion before it impacts users.\n\n## Problem Statement\n\nThe PostgreSQL connection pool uses pgxpool defaults with no explicit configuration or monitoring.\n\n**Current state:**\n- Uses pgxpool.ParseConfig() with DATABASE_URL only\n- No MaxConns, MinConns, MaxConnLifetime configured\n- Pool size = max(4, num_cpus) (default)\n- No pool metrics exported\n\n**Why this matters:**\n- Session activation queries DB (user + config lookup)\n- High connection rate could exhaust default pool\n- Pool exhaustion = WebSocket connection failures\n- No visibility into pool health\n\n**Good news:**\n- Vote processing doesn't touch DB (Redis-only hot path) ✓\n- DB queries are infrequent (only on WS connect) ✓\n\n## Solution: Configurable Pool + Monitoring\n\n### Implementation Tasks\n\n#### Task 1: Add pool configuration options\n**File:** `internal/config/config.go`\n\n```go\ntype Config struct {\n    // ... existing fields\n    \n    // PostgreSQL connection pool\n    DBMaxConns         int           `env:\"DB_MAX_CONNS\" default:\"0\"`  // 0 = auto (4 or num_cpus)\n    DBMinConns         int           `env:\"DB_MIN_CONNS\" default:\"2\"`\n    DBMaxConnLifetime  time.Duration `env:\"DB_MAX_CONN_LIFETIME\" default:\"1h\"`\n    DBMaxConnIdleTime  time.Duration `env:\"DB_MAX_CONN_IDLE_TIME\" default:\"30m\"`\n    DBHealthCheckPeriod time.Duration `env:\"DB_HEALTH_CHECK_PERIOD\" default:\"1m\"`\n}\n```\n\n**File:** `.env.example`\n```bash\n# PostgreSQL connection pool configuration\nDB_MAX_CONNS=20              # Maximum pool size (0 = auto-detect from CPU)\nDB_MIN_CONNS=2               # Minimum idle connections\nDB_MAX_CONN_LIFETIME=1h      # Max connection age\nDB_MAX_CONN_IDLE_TIME=30m    # Close idle connections after this duration\nDB_HEALTH_CHECK_PERIOD=1m    # Background health check interval\n```\n\n#### Task 2: Configure pool on startup\n**File:** `internal/database/postgres.go`\n\n```go\nfunc Connect(ctx context.Context, cfg *config.Config) (*pgxpool.Pool, error) {\n    poolConfig, err := pgxpool.ParseConfig(cfg.DatabaseURL)\n    if err != nil {\n        return nil, fmt.Errorf(\"parse database URL: %w\", err)\n    }\n    \n    // Apply custom pool configuration\n    if cfg.DBMaxConns \u003e 0 {\n        poolConfig.MaxConns = int32(cfg.DBMaxConns)\n    }\n    poolConfig.MinConns = int32(cfg.DBMinConns)\n    poolConfig.MaxConnLifetime = cfg.DBMaxConnLifetime\n    poolConfig.MaxConnIdleTime = cfg.DBMaxConnIdleTime\n    poolConfig.HealthCheckPeriod = cfg.DBHealthCheckPeriod\n    \n    // Add query tracer for monitoring\n    poolConfig.ConnConfig.Tracer = \u0026dbTracer{}\n    \n    pool, err := pgxpool.NewWithConfig(ctx, poolConfig)\n    if err != nil {\n        return nil, fmt.Errorf(\"create pool: %w\", err)\n    }\n    \n    // Verify connection\n    if err := pool.Ping(ctx); err != nil {\n        return nil, fmt.Errorf(\"ping database: %w\", err)\n    }\n    \n    slog.Info(\"Database pool configured\",\n        \"max_conns\", poolConfig.MaxConns,\n        \"min_conns\", poolConfig.MinConns,\n        \"max_lifetime\", poolConfig.MaxConnLifetime,\n    )\n    \n    return pool, nil\n}\n```\n\n#### Task 3: Add database tracer for query metrics\n**File:** `internal/database/tracer.go` (NEW)\n\n```go\npackage database\n\nimport (\n    \"context\"\n    \"time\"\n    \n    \"github.com/jackc/pgx/v5\"\n    \"github.com/pscheid92/chatpulse/internal/metrics\"\n)\n\ntype dbTracer struct{}\n\nfunc (t *dbTracer) TraceQueryStart(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryStartData) context.Context {\n    return context.WithValue(ctx, \"query_start\", time.Now())\n}\n\nfunc (t *dbTracer) TraceQueryEnd(ctx context.Context, conn *pgx.Conn, data pgx.TraceQueryEndData) {\n    start, ok := ctx.Value(\"query_start\").(time.Time)\n    if !ok {\n        return\n    }\n    \n    duration := time.Since(start)\n    \n    // Extract query name from SQL (first word typically)\n    queryName := extractQueryName(data.SQL)\n    \n    // Record metrics\n    metrics.DBQueryDuration.WithLabelValues(queryName).Observe(duration.Seconds())\n    \n    if data.Err != nil {\n        metrics.DBErrors.WithLabelValues(queryName).Inc()\n    }\n}\n\nfunc extractQueryName(sql string) string {\n    // Parse SQL to extract operation (SELECT, INSERT, UPDATE)\n    // and table name for labeling\n    // Simplified: return first 20 chars\n    if len(sql) \u003e 20 {\n        return sql[:20]\n    }\n    return sql\n}\n```\n\n#### Task 4: Export pool stats metrics\n**File:** `cmd/server/main.go`\n\n```go\nfunc main() {\n    // ... existing setup\n    \n    db := database.Connect(ctx, cfg)\n    defer db.Close()\n    \n    // Start pool stats exporter\n    go exportPoolStats(ctx, db, 10*time.Second)\n    \n    // ... rest of main\n}\n\nfunc exportPoolStats(ctx context.Context, pool *pgxpool.Pool, interval time.Duration) {\n    ticker := time.NewTicker(interval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ctx.Done():\n            return\n        case \u003c-ticker.C:\n            stats := pool.Stat()\n            \n            metrics.DBPoolAcquireCount.Set(float64(stats.AcquireCount()))\n            metrics.DBPoolAcquiredConns.Set(float64(stats.AcquiredConns()))\n            metrics.DBPoolIdleConns.Set(float64(stats.IdleConns()))\n            metrics.DBPoolMaxConns.Set(float64(stats.MaxConns()))\n            metrics.DBPoolTotalConns.Set(float64(stats.TotalConns()))\n            metrics.DBPoolAcquireDuration.Observe(stats.AcquireDuration().Seconds())\n        }\n    }\n}\n```\n\n#### Task 5: Add pool metrics definitions\n**File:** `internal/metrics/metrics.go`\n\n```go\n// Database query metrics\nDBQueryDuration = promauto.NewHistogramVec(\n    prometheus.HistogramOpts{\n        Name: \"db_query_duration_seconds\",\n        Help: \"Database query duration in seconds\",\n        Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5, 1},\n    },\n    []string{\"query\"},\n)\n\nDBErrors = promauto.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: \"db_errors_total\",\n        Help: \"Database query errors\",\n    },\n    []string{\"query\"},\n)\n\n// Connection pool metrics\nDBPoolAcquireCount = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"db_pool_acquire_count\",\n        Help: \"Cumulative count of successful connection acquisitions\",\n    },\n)\n\nDBPoolAcquiredConns = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"db_pool_acquired_conns\",\n        Help: \"Number of currently acquired connections\",\n    },\n)\n\nDBPoolIdleConns = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"db_pool_idle_conns\",\n        Help: \"Number of currently idle connections\",\n    },\n)\n\nDBPoolMaxConns = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"db_pool_max_conns\",\n        Help: \"Maximum pool size\",\n    },\n)\n\nDBPoolTotalConns = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"db_pool_total_conns\",\n        Help: \"Total number of connections (acquired + idle)\",\n    },\n)\n\nDBPoolAcquireDuration = promauto.NewHistogram(\n    prometheus.HistogramOpts{\n        Name: \"db_pool_acquire_duration_seconds\",\n        Help: \"Time to acquire connection from pool\",\n        Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5},\n    },\n)\n```\n\n#### Task 6: Add health check endpoint enhancement\n**File:** `internal/server/handlers_health.go`\n\n```go\nfunc (s *Server) checkPostgres(ctx context.Context) error {\n    // Check basic connectivity\n    if err := s.db.Ping(ctx); err != nil {\n        return fmt.Errorf(\"ping failed: %w\", err)\n    }\n    \n    // Check pool not exhausted\n    stats := s.db.Stat()\n    if stats.AcquiredConns() \u003e= stats.MaxConns() {\n        return fmt.Errorf(\"pool exhausted: %d/%d connections in use\",\n            stats.AcquiredConns(), stats.MaxConns())\n    }\n    \n    return nil\n}\n```\n\n#### Task 7: Documentation for pool sizing\n**File:** `docs/deployment/database-pool-sizing.md` (NEW)\n\n```markdown\n# PostgreSQL Connection Pool Sizing\n\n## Default Behavior\n- MaxConns: max(4, num_cpus)\n- MinConns: 2\n- MaxConnLifetime: 1 hour\n- MaxConnIdleTime: 30 minutes\n\n## Sizing Guidelines\n\n### Calculate Required Pool Size\n```\nmax_conns = peak_session_activations_per_second * avg_query_latency_seconds * safety_factor\n\nExample:\n- 100 sessions/sec activation rate\n- 0.050 seconds avg DB query latency  \n- 2x safety factor\n= 100 * 0.05 * 2 = 10 connections\n```\n\n### Recommended Values by Scale\n- Small (\u003c100 sessions/min): Default (4-8 conns)\n- Medium (100-500 sessions/min): 10-20 conns\n- Large (500+ sessions/min): 20-50 conns\n\n### Monitoring\nAlert if:\n- `db_pool_acquired_conns / db_pool_max_conns \u003e 0.8` (\u003e80% utilization)\n- `db_pool_acquire_duration_seconds p99 \u003e 0.1` (\u003e100ms wait time)\n\n### Tuning\n1. Monitor `db_pool_acquired_conns` during peak load\n2. If frequently hitting max, increase `DB_MAX_CONNS`\n3. If always low (\u003c30%), decrease to save resources\n```\n\n#### Task 8: Unit tests\n**File:** `internal/database/postgres_test.go`\n\nTest configuration parsing:\n```go\nfunc TestConnect_PoolConfiguration(t *testing.T) {\n    cfg := \u0026config.Config{\n        DatabaseURL:        \"postgres://localhost/test\",\n        DBMaxConns:         20,\n        DBMinConns:         5,\n        DBMaxConnLifetime:  30 * time.Minute,\n        DBMaxConnIdleTime:  10 * time.Minute,\n        DBHealthCheckPeriod: 30 * time.Second,\n    }\n    \n    poolConfig, _ := pgxpool.ParseConfig(cfg.DatabaseURL)\n    // Apply config (extracted to helper function)\n    applyPoolConfig(poolConfig, cfg)\n    \n    assert.Equal(t, int32(20), poolConfig.MaxConns)\n    assert.Equal(t, int32(5), poolConfig.MinConns)\n    assert.Equal(t, 30*time.Minute, poolConfig.MaxConnLifetime)\n}\n```\n\n#### Task 9: Integration test with pool exhaustion\n**File:** `internal/database/postgres_pool_integration_test.go` (NEW)\n\n```go\nfunc TestPoolExhaustion(t *testing.T) {\n    if testing.Short() {\n        t.Skip(\"integration test\")\n    }\n    \n    // Create pool with size 2\n    cfg := testConfig()\n    cfg.DBMaxConns = 2\n    \n    pool := setupTestPool(t, cfg)\n    \n    // Acquire 2 connections (exhaust pool)\n    conn1, _ := pool.Acquire(context.Background())\n    conn2, _ := pool.Acquire(context.Background())\n    \n    // 3rd acquire should timeout\n    ctx, cancel := context.WithTimeout(context.Background(), 100*time.Millisecond)\n    defer cancel()\n    \n    _, err := pool.Acquire(ctx)\n    assert.Error(t, err)\n    assert.Contains(t, err.Error(), \"timeout\")\n    \n    // Release connection\n    conn1.Release()\n    \n    // Now acquire should succeed\n    conn3, err := pool.Acquire(context.Background())\n    assert.NoError(t, err)\n    conn3.Release()\n    conn2.Release()\n}\n```\n\n#### Task 10: Update CLAUDE.md\nAdd database pool configuration section to ## Environment Variables.\n\n## Acceptance Criteria\n\n- ✅ Pool size configurable via DB_MAX_CONNS env var\n- ✅ Pool metrics exported every 10 seconds\n- ✅ Query duration tracked per query type\n- ✅ Health check fails if pool exhausted\n- ✅ Documentation includes sizing calculator\n- ✅ Unit tests verify configuration applied\n- ✅ Integration test simulates pool exhaustion\n- ✅ CLAUDE.md documents new env vars\n\n## Files Created/Modified\n\n**New files:**\n- `internal/database/tracer.go` (100 lines - query tracing)\n- `internal/database/postgres_pool_integration_test.go` (150 lines)\n- `docs/deployment/database-pool-sizing.md` (300 lines)\n\n**Modified files:**\n- `internal/config/config.go` (add 5 pool config fields, 15 lines)\n- `internal/database/postgres.go` (apply pool config, 30 lines)\n- `cmd/server/main.go` (export pool stats, 25 lines)\n- `internal/metrics/metrics.go` (add 6 pool metrics + 2 query metrics, 60 lines)\n- `internal/server/handlers_health.go` (enhance DB health check, 10 lines)\n- `.env.example` (document pool config, 10 lines)\n- `CLAUDE.md` (document pool configuration)\n\n## Testing Strategy\n\n**Unit tests:**\n- Configuration parsing\n- Pool config application\n- Tracer query name extraction\n\n**Integration tests:**\n- Pool exhaustion scenario\n- Connection lifecycle\n- Health check with exhausted pool\n\n**Load tests:**\n- Simulate 100 concurrent session activations\n- Measure pool utilization\n- Verify no connection wait timeouts\n\n## Dependencies\n- Requires Prometheus metrics (twitch-tow-kgj)\n- Enhances health checks (twitch-tow-682)\n\n## Success Metrics\n- Pool utilization visible in metrics\n- Zero connection wait timeouts under normal load\n- Health check catches pool exhaustion\n- Documentation enables operators to tune correctly\n\n## Effort Estimate\n**3 developer-days**\n\nBreakdown:\n- Configuration + tracer: 1 day\n- Metrics export: 0.5 day\n- Testing: 1 day\n- Documentation: 0.5 day\n\n## Recommended Default\nFor production: `DB_MAX_CONNS=20`\n\nRationale:\n- Handles 200 sessions/sec at 50ms latency\n- Leaves headroom for traffic spikes\n- Low resource cost (20 TCP connections)\n\n## Risks \u0026 Mitigation\n- **Risk:** MaxConns too high exhausts PostgreSQL\n  - **Mitigation:** PostgreSQL max_connections typically 100-200, pool of 20 is safe\n- **Risk:** Pool sizing calculator inaccurate\n  - **Mitigation:** Monitor actual metrics, document tuning process\n- **Risk:** Query tracer overhead\n  - **Mitigation:** pgx tracing is lightweight (\u003c1% overhead)","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:49:03.148418+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:42.35699+01:00"}
{"id":"twitch-tow-apx","title":"EPIC: Standardize Error Handling (Documentation + Refactoring)","description":"Implement standardized error handling patterns: write ERROR_HANDLING.md guide, fix double-logging issues, and add linting rules. Merges twitch-tow-oy1 + twitch-tow-ay7 patterns.\n\n## User Story\nAs a developer debugging production issues, I need consistent error handling so I can quickly identify root causes without hunting through duplicate logs or missing context.\n\n## Value Proposition\n- Eliminates double-logging (same error logged at multiple layers)\n- Standardizes when to log vs return errors\n- Clarifies which errors are retryable vs permanent\n- Enables better alerting (transient vs critical errors)\n- Improves production debugging (consistent context fields)\n\n## Background: Current Issues\n\n**Problems identified by architects:**\n1. **Mixed logging locations** - Errors logged in repositories AND handlers (double-logging)\n2. **Wrapped vs sentinel errors** - Inconsistent use of `fmt.Errorf` wrapping breaks `errors.Is` checks\n3. **Inconsistent structured logging** - Missing context fields, inconsistent field names\n4. **No error budgets** - Can't distinguish transient failures from critical issues\n\n## Error Classification (3 Tiers)\n\n### Tier A - Domain Errors (sentinel errors)\n**Definition:** Expected business errors that represent valid application states.\n\n**Examples:**\n- `ErrUserNotFound` - User lookup failed (valid: user doesn't exist)\n- `ErrConfigNotFound` - Config lookup failed (valid: first-time user)\n- `ErrSubscriptionNotFound` - EventSub subscription not found\n\n**Handling:**\n- ✅ Return unwrapped (preserve sentinel for `errors.Is`)\n- ❌ Do NOT log at origin (caller decides)\n- ✅ Used for control flow (if errors.Is(...) { redirect to login })\n\n**Code pattern:**\n```go\n// Repository (origin) - return unwrapped\nif err == pgx.ErrNoRows {\n    return domain.ErrUserNotFound\n}\n\n// Handler (decision boundary) - log if needed\nuser, err := app.GetUserByID(ctx, userID)\nif errors.Is(err, domain.ErrUserNotFound) {\n    // Control flow - don't log (expected case)\n    return c.Redirect(http.StatusFound, \"/auth/login\")\n}\n```\n\n### Tier B - Infrastructure Errors (wrap with context)\n**Definition:** Unexpected failures in external systems (database, Redis, network, APIs).\n\n**Examples:**\n- Database connection failures (`pgx.ErrNoRows` is NOT this, it's Tier A)\n- Redis timeouts or connection errors\n- Network errors (Twitch API unreachable)\n- Twitch API errors (rate limit, 500, invalid response)\n\n**Handling:**\n- ✅ Wrap with context (`fmt.Errorf(\"failed to X: %w\", err)`)\n- ✅ Log at origin with structured context\n- ✅ Return error up the stack (don't swallow)\n\n**Code pattern:**\n```go\n// Repository/Service (origin) - log + wrap + return\nuser, err := q.GetByID(ctx, userID)\nif err \\!= nil {\n    s.logger.Error(\"failed to get user\",\n        \"user_id\", userID,\n        \"error\", err)\n    return nil, fmt.Errorf(\"failed to get user %s: %w\", userID, err)\n}\n```\n\n**Subcategories:**\n\n**B1 - Transient Errors** (retry recommended)\n- Redis connection timeout\n- Network timeouts\n- Twitch API 500/503\n- Database connection pool exhausted\n\n**B2 - Permanent Errors** (retry won't help)\n- Twitch API 401 (bad credentials)\n- Twitch API 404 (broadcaster doesn't exist)\n- Database constraint violation (shouldn't happen if code correct)\n\n**B3 - Rate Limit Errors** (retry with backoff)\n- Twitch API 429\n- Redis command queue full\n\n### Tier C - Programming Errors (panic + recover)\n**Definition:** Bugs in the code (should never happen in production).\n\n**Examples:**\n- Nil pointer dereference\n- Index out of bounds\n- Type assertion failure\n\n**Handling:**\n- ✅ Use panic (let middleware recover)\n- ✅ Middleware logs panic + stack trace\n- ❌ Don't try/catch everywhere (trust middleware)\n\n**Code pattern:**\n```go\n// Let panic propagate, middleware will recover\nuser := users[userID] // panic if not found - that's a bug\n```\n\n## Logging Rules\n\n### Rule 1: Log at Decision Boundary, Not at Origin\n\n**Decision boundary:** Where you decide how to handle the error (handler, middleware).\n\n**Anti-pattern (double-logging):**\n```go\n// Repository logs...\nif err \\!= nil {\n    s.logger.Error(\"query failed\", \"error\", err)\n    return err\n}\n\n// Handler ALSO logs...\nif err \\!= nil {\n    s.logger.Error(\"handler failed\", \"error\", err)\n    return c.JSON(500, ...)\n}\n// Result: Same error logged twice\\!\n```\n\n**Correct pattern:**\n```go\n// Repository does NOT log (unless infrastructure error)\nif err == pgx.ErrNoRows {\n    return domain.ErrUserNotFound // no log\n}\n\n// Handler logs only if unexpected\nuser, err := app.GetUserByID(ctx, userID)\nif err \\!= nil {\n    if \\!errors.Is(err, domain.ErrUserNotFound) {\n        s.logger.Error(\"unexpected error\", \"error\", err) // log once\n    }\n    return c.Redirect(...) // or return error response\n}\n```\n\n### Rule 2: Infrastructure Errors ARE Logged at Origin\n\nException to Rule 1: Infrastructure errors (Tier B) ARE logged at origin because they represent system failures, not business logic.\n\n```go\n// Redis error - log at origin\nif err := s.rdb.Set(ctx, key, value, ttl).Err(); err \\!= nil {\n    s.logger.Error(\"redis write failed\",\n        \"key\", key,\n        \"error\", err) // Log here (infrastructure failure)\n    return fmt.Errorf(\"failed to cache session: %w\", err)\n}\n```\n\n## Structured Logging Standards\n\n### Required Fields (All Logs)\n- `error` - The error value\n- `message` - Human-readable description\n\n### Context Fields (Domain-Specific)\nUse consistent names:\n- `user_id` - User's UUID (not `userID` or `uid`)\n- `session_uuid` - Session overlay UUID (not `sessionID`)\n- `broadcaster_user_id` - Twitch broadcaster ID (not `broadcasterID`)\n- `operation` - Operation name (e.g., \"create_session\", \"apply_vote\")\n\n### Example (Good Structured Logging)\n```go\ns.logger.Error(\"failed to activate session\",\n    \"session_uuid\", overlayUUID,\n    \"broadcaster_user_id\", broadcasterUserID,\n    \"operation\", \"activate_session\",\n    \"error\", err)\n```\n\n## Tasks\n\n### 1. Write ERROR_HANDLING.md guide\n\nCreate comprehensive error handling guide in `docs/ERROR_HANDLING.md`.\n\n**Sections:**\n1. Introduction (why consistent error handling matters)\n2. Error Classification (Tier A/B/C with examples)\n3. Logging Rules (decision boundary vs origin)\n4. Structured Logging Standards (required fields)\n5. Code Examples (good vs bad patterns)\n6. Testing Guidance (how to test error paths)\n\n**Files to create:**\n- `docs/ERROR_HANDLING.md`\n\n**Time estimate:** 60 minutes\n\n### 2. Fix double-logging in app/service.go\n\n**Current issue:** `app.Service` methods log errors that are also logged by handlers.\n\n**Example (line ~150):**\n```go\n// EnsureSessionActive logs...\nif err \\!= nil {\n    s.logger.Error(\"failed to create session\", ...)\n    return err\n}\n\n// Handler ALSO logs...\nif err := app.EnsureSessionActive(...); err \\!= nil {\n    s.logger.Error(\"activation failed\", ...)\n}\n```\n\n**Fix:** Remove logging from `app.Service` methods, let handlers decide.\n\n**Affected methods:**\n- `EnsureSessionActive` (logs session creation failures)\n- `SaveConfig` (logs config update failures)\n- `RotateOverlayUUID` (logs rotation failures)\n\n**Files to modify:**\n- `internal/app/service.go`\n\n**Time estimate:** 30 minutes\n\n### 3. Fix double-logging in broadcast/broadcaster.go\n\n**Current issue:** Broadcaster logs tick errors, but those errors are already logged in `sentiment.Engine`.\n\n**Example (line ~200):**\n```go\n// Tick loop logs...\nvalue, err := b.engine.GetCurrentValue(ctx, uuid)\nif err \\!= nil {\n    b.logger.Error(\"failed to get value\", ...) // Log 1\n    continue\n}\n\n// Engine ALSO logs...\nfunc (e *Engine) GetCurrentValue(...) {\n    if err \\!= nil {\n        e.logger.Error(\"redis read failed\", ...) // Log 2\n    }\n}\n```\n\n**Fix:** Remove logging from broadcaster tick loop (Engine already logs infrastructure errors).\n\n**Files to modify:**\n- `internal/broadcast/broadcaster.go`\n\n**Time estimate:** 20 minutes\n\n### 4. Audit structured logging field names\n\n**Task:** Scan all `.Error()` and `.Warn()` calls, ensure consistent field names.\n\n**Fields to standardize:**\n- `userID` → `user_id`\n- `sessionID` → `session_uuid`\n- `broadcasterID` → `broadcaster_user_id`\n\n**Command:**\n```bash\n# Find all logger calls\nrg '\\.Error\\(|.Warn\\(' --type go\n```\n\n**Files likely affected:**\n- `internal/app/service.go`\n- `internal/broadcast/broadcaster.go`\n- `internal/twitch/webhook.go`\n- `internal/server/handlers_*.go`\n\n**Time estimate:** 40 minutes (audit + fix)\n\n### 5. Add golangci-lint rules\n\n**Edit `.golangci.yml`** to enforce error handling standards:\n\n```yaml\nlinters:\n  enable:\n    - errname       # Check sentinel error names (must be Err*)\n    - errorlint     # Check error wrapping (use %w, not %v)\n    - wrapcheck     # Check errors are wrapped at appropriate layers\n```\n\n**Run linter:**\n```bash\nmake lint\n```\n\n**Fix any violations** (likely in `internal/database/*_repository.go`).\n\n**Files to modify:**\n- `.golangci.yml`\n- Repositories (wrap errors with `fmt.Errorf(..., %w)`)\n\n**Time estimate:** 30 minutes (config + fix violations)\n\n### 6. Update CLAUDE.md to reference ERROR_HANDLING.md\n\nAdd link in Architecture Notes section:\n\n```markdown\n## Error Handling\n\nSee [`docs/ERROR_HANDLING.md`](docs/ERROR_HANDLING.md) for error handling standards: classification (Domain/Infrastructure/Programming), logging rules (decision boundary vs origin), and structured logging conventions.\n```\n\n**Files to modify:**\n- `CLAUDE.md`\n\n**Time estimate:** 5 minutes\n\n### 7. Review and test\n\n- Run tests: `make test`\n- Run linter: `make lint`\n- Verify double-logging eliminated (search logs for duplicates)\n- Check structured logging fields consistent\n\n**Time estimate:** 15 minutes\n\n## Acceptance Criteria\n\n- ✅ `ERROR_HANDLING.md` written with clear examples\n- ✅ Double-logging eliminated in `app/service.go`\n- ✅ Double-logging eliminated in `broadcast/broadcaster.go`\n- ✅ Structured logging fields standardized (snake_case)\n- ✅ golangci-lint rules enabled (errname, errorlint, wrapcheck)\n- ✅ All tests pass (`make test`)\n- ✅ No lint violations (`make lint`)\n- ✅ CLAUDE.md links to ERROR_HANDLING.md\n\n## Files Changed\n\n**Created:**\n- `docs/ERROR_HANDLING.md`\n\n**Modified:**\n- `internal/app/service.go` (remove logs)\n- `internal/broadcast/broadcaster.go` (remove logs)\n- `internal/server/handlers_*.go` (standardize field names)\n- `internal/twitch/webhook.go` (standardize field names)\n- `.golangci.yml` (add linter rules)\n- `CLAUDE.md` (add ERROR_HANDLING.md link)\n\n## Dependencies\n- None (independent refactoring)\n\n## Effort Estimate\n**Total: 3 hours** (180 minutes)\n- ERROR_HANDLING.md (60 min): Write comprehensive guide\n- Fix double-logging (50 min): app/service.go + broadcaster.go\n- Audit field names (40 min): Scan + fix all loggers\n- Add lint rules (30 min): Config + fix violations\n- Integration (15 min): Update CLAUDE.md + test\n\n## Success Metrics\n- Grep logs for duplicate errors → zero matches\n- Production logs have consistent field names\n- Linter enforces error wrapping standards\n- Developers reference ERROR_HANDLING.md in PRs","status":"in_progress","priority":2,"issue_type":"epic","assignee":"dev-quality","owner":"patrick.scheid@deepl.com","estimated_minutes":180,"created_at":"2026-02-12T17:28:52.657798+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:17:01.68228+01:00"}
{"id":"twitch-tow-ay7","title":"Solution: Error handling guidelines and patterns","description":"## Proposal: Document Error Handling Patterns in CLAUDE.md\n\nThis addresses consensus issues:\n- twitch-tow-5sz (Maintainability - mixed patterns)\n- twitch-tow-uf7 (Resilience - silent failures)\n- twitch-tow-4vo (Resilience - structured errors idea)\n\n## Design Philosophy: Pragmatic Error Handling\n\n**Principle:** Simple and consistent, not perfect. Avoid over-engineering.\n\n## Pattern 1: Sentinel Errors for Control Flow\n\n**When to use:** Domain-level \"expected errors\" that callers handle explicitly\n\n```go\n// domain/errors.go\nvar (\n    ErrUserNotFound         = errors.New(\"user not found\")\n    ErrConfigNotFound       = errors.New(\"config not found\")\n    ErrSubscriptionNotFound = errors.New(\"subscription not found\")\n    ErrSessionNotFound      = errors.New(\"session not found\")\n)\n```\n\n**Rules:**\n- Use for NOT FOUND scenarios (map to HTTP 404)\n- Check with errors.Is() in handlers\n- Don't wrap sentinel errors (breaks errors.Is)\n\n**Example:**\n```go\n// Repository (return sentinel, don't wrap)\nfunc (r *UserRepo) GetByID(ctx context.Context, id uuid.UUID) (*domain.User, error) {\n    row := r.db.QueryRow(ctx, \"SELECT ...\", id)\n    if err := row.Scan(...); errors.Is(err, pgx.ErrNoRows) {\n        return nil, domain.ErrUserNotFound  // Don't wrap\n    }\n    return user, nil\n}\n\n// Handler (check sentinel)\nfunc (s *Server) handleDashboard(c echo.Context) error {\n    user, err := s.app.GetUserByID(ctx, userID)\n    if errors.Is(err, domain.ErrUserNotFound) {\n        return c.Redirect(302, \"/auth/login\")  // Expected\n    }\n    if err != nil {\n        slog.Error(\"Failed to get user\", \"error\", err)\n        return c.String(500, \"Internal error\")  // Unexpected\n    }\n    // ...\n}\n```\n\n## Pattern 2: Wrapped Errors for Context\n\n**When to use:** Unexpected errors that need debugging context\n\n```go\nfunc (r *UserRepo) Upsert(ctx context.Context, ...) (*domain.User, error) {\n    tx, err := r.db.Begin(ctx)\n    if err != nil {\n        return nil, fmt.Errorf(\"begin transaction: %w\", err)  // Wrap\n    }\n    \n    if err := tx.QueryRow(...).Scan(...); err != nil {\n        tx.Rollback(ctx)\n        return nil, fmt.Errorf(\"insert user %s: %w\", twitchUserID, err)  // Add context\n    }\n    \n    return user, nil\n}\n```\n\n**Rules:**\n- Use fmt.Errorf with %w for wrapping\n- Add context (user ID, session ID, operation)\n- Don't double-wrap (check if already has context)\n\n## Pattern 3: Fire-and-Forget Errors\n\n**When to use:** Background operations where failure is acceptable\n\n```go\nfunc (s *Service) OnSessionEmpty(ctx context.Context, sessionUUID uuid.UUID) {\n    count, err := s.store.DecrRefCount(ctx, sessionUUID)\n    if err != nil {\n        slog.Error(\"DecrRefCount error\", \"session_uuid\", sessionUUID, \"error\", err)\n        return  // Log but don't propagate (can't retry, not critical)\n    }\n    // ...\n}\n```\n\n**Rules:**\n- Log with structured context (session_uuid, error)\n- Use slog.Error for failures, slog.Warn for expected issues\n- Don't return error to caller (fire-and-forget)\n- Emit metric: background_operation_errors_total{operation}\n\n**When NOT to use:**\n- Critical operations (e.g., Subscribe - rollback on failure)\n- Operations with compensation logic (e.g., SaveConfig - retry)\n\n## Pattern 4: Silent Failures with Metrics\n\n**When to use:** High-frequency operations where logging is noise\n\n```go\nfunc (e *Engine) ProcessVote(ctx context.Context, ...) (float64, bool) {\n    // ... multiple checks ...\n    \n    allowed, err := e.debounce.CheckDebounce(ctx, sessionUUID, chatterUserID)\n    if err != nil {\n        metrics.VotesDropped.WithLabelValues(\"debounce_error\").Inc()\n        return 0, false  // Don't log (happens frequently)\n    }\n    if !allowed {\n        metrics.VotesDropped.WithLabelValues(\"debounced\").Inc()\n        return 0, false  // Expected, not an error\n    }\n    \n    // ...\n}\n```\n\n**Rules:**\n- Only for errors \u003e 10/sec expected rate\n- MUST increment metric (observability)\n- Log at debug level if needed (not info/error)\n- Emit metric: votes_dropped_total{reason}\n\n## Pattern 5: Logging Location\n\n**Where to log:**\n- Repositories: Log unexpected errors (DB failures, Redis timeouts)\n- Services: Log business logic errors (Subscribe failed, Cleanup failed)\n- Handlers: Log HTTP-level errors (auth failures, invalid input)\n- Background jobs: Always log (no caller to propagate to)\n\n**Where NOT to log:**\n- Don't double-log (if you return error, caller logs)\n- Don't log expected errors (ErrUserNotFound)\n- Don't log high-frequency events (debounce hits)\n\n**Example:**\n```go\n// Repository: Log + return error\nfunc (r *UserRepo) GetByID(ctx context.Context, id uuid.UUID) (*domain.User, error) {\n    // ... query ...\n    if err != nil \u0026\u0026 !errors.Is(err, pgx.ErrNoRows) {\n        slog.Error(\"Query failed\", \"user_id\", id, \"error\", err)  // Log DB failure\n        return nil, fmt.Errorf(\"get user by ID: %w\", err)\n    }\n    return user, domain.ErrUserNotFound  // Don't log (expected)\n}\n\n// Handler: Check error + log if unexpected\nfunc (s *Server) handleDashboard(c echo.Context) error {\n    user, err := s.app.GetUserByID(ctx, userID)\n    if errors.Is(err, domain.ErrUserNotFound) {\n        return c.Redirect(302, \"/auth/login\")  // Don't log (expected flow)\n    }\n    if err != nil {\n        // Don't log here - already logged by repository\n        return c.String(500, \"Internal error\")\n    }\n    // ...\n}\n```\n\n## Pattern 6: Structured Logging Context\n\n**Always include:**\n- Relevant IDs (session_uuid, user_id, broadcaster_user_id)\n- Error object (\"error\", err)\n- Operation context (\"operation\", \"cleanup_orphans\")\n\n```go\nslog.Error(\"Failed to delete session\",\n    \"session_uuid\", sessionUUID,\n    \"operation\", \"cleanup_orphans\",\n    \"error\", err)\n```\n\n**Don't include:**\n- Secrets (tokens, passwords)\n- PII (usernames if not necessary)\n- High-cardinality values (message text)\n\n## Trade-offs\n\n**Pros:**\n- Simple patterns, easy to follow\n- No new dependencies (use stdlib + slog)\n- Consistent across codebase\n- Good balance of logging vs. noise\n\n**Cons:**\n- Doesn't solve structured error types (deferred to Phase 2)\n- Metrics dependency (requires Phase 2 observability)\n- Manual discipline required (linter can't enforce)\n\n## Decision: Document in CLAUDE.md\n\nAdd \"Error Handling Patterns\" section to CLAUDE.md with:\n1. When to use sentinel errors\n2. When to wrap errors\n3. Where to log\n4. Structured logging guidelines\n\n## Vote\n- Architect-Resilience: +1 (document patterns, defer structured types)\n- Architect-Scalability: ?\n- Architect-Maintainability: ?\n\n## Files to Modify\n- CLAUDE.md (new \"Error Handling Patterns\" section)\n- internal/domain/errors.go (add missing sentinels if needed)\n- Refactoring: Fix violations gradually (not all at once)","notes":"Vote: +1 from Maintainability architect. 6-pattern breakdown is comprehensive and practical. Excellent examples for each pattern. Complements my twitch-tow-oy1 proposal which adds 3-tier taxonomy. Recommend merge: combine your 6 patterns with oy1 3-tier classification, document in ERROR_HANDLING.md (not CLAUDE.md for better discoverability), add golangci-lint rules. I volunteer to lead this merge.\nVote: +1 with caveat from Maintainability Architect\n\nRATIONALE: 6 patterns are solid foundation. BUT my Epic 4 (twitch-tow-apx) is more comprehensive: 3-tier classification, ERROR_HANDLING.md, code fixes, linting rules.\n\nANSWERS TO QUESTIONS:\n- Q5 (linter rules): YES - Already in my Epic 4 (errname, errorlint, wrapcheck)\n- Q6 (fire-and-forget for OnSessionEmpty): YES - But add metric for dropped errors\n\nPROPOSAL: Merge twitch-tow-ay7 into my Epic 4 (twitch-tow-apx)\n\nMAPPING YOUR PATTERNS TO MY TIERS:\n1. Sentinel errors → Tier A (Domain Errors)\n2. Wrapped errors → Tier B (Infrastructure Errors)\n3. Fire-and-forget → Logging rules (origin vs decision boundary)\n4. Silent failures with metrics → Tier B (log infrastructure errors)\n5. Logging location rules → Rule 1 (log at decision boundary)\n6. Structured logging → Standardized field names\n\nCONSOLIDATION PLAN:\n- Use my 3-tier classification (simpler mental model)\n- Use my ERROR_HANDLING.md (dedicated doc, not in CLAUDE.md)\n- Add your fire-and-forget pattern to my guide\n- Keep your concrete examples (good code snippets)\n- My Epic 4 has implementation tasks (fix double-logging, add linting)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:12:59.495318+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.516471+01:00","closed_at":"2026-02-12T17:56:57.516471+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-b14","title":"Broadcaster.Stop() lacks goroutine synchronization","description":"**Location:** internal/broadcast/broadcaster.go:102-104\n\n**Issue:** Stop() is non-blocking and doesn't wait for the actor goroutine or clientWriter goroutines to exit. Commands can be queued after Stop() is called but never processed if the actor loop has already exited.\n\n**Evidence:** TestBroadcasterStopCleansUpGoroutines shows 5 goroutines still running after Stop() returns (baseline=2, final=7). Test logs show 'Stop() timed out waiting for goroutine exit' messages.\n\n**Impact:**\n- Graceful shutdown can hang if ticker is blocked\n- Race conditions during shutdown\n- Resource leaks (goroutines not cleaned up)\n- No guarantee that Stop() completes before process exits\n\n**Fix:**\n- Add sync.WaitGroup to track actor and clientWriter goroutines\n- Make Stop() block until all goroutines exit\n- Add timeout (5-10 seconds) with context cancellation\n- Close cmdCh to unblock any pending commands\n\n**Related:** This is likely a duplicate of twitch-tow-563. Verify and link.","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:41:57.639892+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:47:17.951029+01:00","closed_at":"2026-02-12T16:47:17.951029+01:00","close_reason":"Duplicate of twitch-tow-563 which has been fixed","labels":["gt:duplicate-check"]}
{"id":"twitch-tow-b9h","title":"Solution: Redis circuit breaker with graceful fallback","description":"## Proposal: Add Circuit Breaker for Redis Operations\n\nThis addresses consensus issues:\n- twitch-tow-usj (Scalability - Redis SPOF)\n- twitch-tow-9yg (Resilience - connection pooling)\n- twitch-tow-fgm (Resilience - circuit breaker idea)\n\n## Design: Fail-Fast with Graceful Degradation\n\n**Library:** github.com/sony/gobreaker v0.5.0\n\n**Configuration:**\n```go\ntype CircuitBreakerConfig struct {\n    MaxRequests  uint32        // Half-open: max requests to test (1)\n    Interval     time.Duration // Window to track failures (10s)\n    Timeout      time.Duration // Time before retry (30s)\n    FailureRatio float64       // Failures / total to open (0.6 = 60%)\n}\n```\n\n**Implementation:**\n```go\ntype ResilientRedisClient struct {\n    client  *redis.Client\n    breaker *gobreaker.CircuitBreaker\n}\n\nfunc (r *ResilientRedisClient) FCall(ctx context.Context, fn string, keys []string, args ...any) *redis.Cmd {\n    result, err := r.breaker.Execute(func() (any, error) {\n        return r.client.FCall(ctx, fn, keys, args...).Result()\n    })\n    \n    if err != nil {\n        // Circuit open - return fallback\n        if errors.Is(err, gobreaker.ErrOpenState) {\n            return redis.NewCmd(ctx) // Empty command\n        }\n        return redis.NewCmdResult(nil, err)\n    }\n    \n    return redis.NewCmdResult(result, nil)\n}\n```\n\n## Fallback Behavior (When Circuit Open)\n\n**1. GetCurrentValue (Broadcaster tick):**\n- Return 0 (neutral sentiment)\n- Continue broadcasting to other sessions\n- Log circuit open event (warn level)\n\n**2. ApplyVote (Webhook processing):**\n- Drop vote silently\n- Increment metric: votes_dropped_total{reason=\"circuit_open\"}\n- Log at debug level (avoid log spam)\n\n**3. EnsureSessionActive (WebSocket connect):**\n- Return error: \"Service temporarily unavailable\"\n- Client sees \"reconnecting...\" indicator\n- Prevent new connections during Redis outage\n\n**4. UpdateConfig (Dashboard save):**\n- Return error: \"Cannot save config, try again later\"\n- DB write succeeds (source of truth)\n- Redis will sync on session reactivation\n\n## Metrics\n\n```go\ncircuit_breaker_state{component=\"redis\"} gauge      // 0=closed, 1=open, 2=half-open\ncircuit_breaker_transitions_total{from, to} counter\nredis_operations_total{status=\"circuit_open\"} counter\n```\n\n## Configuration Values\n\n**Conservative (recommended for MVP):**\n- FailureRatio: 0.6 (open after 60% failures in window)\n- Interval: 10s (sliding window)\n- Timeout: 30s (wait 30s before retry)\n- MaxRequests: 1 (only 1 test request in half-open)\n\n**Why these values:**\n- 60% threshold: Tolerates transient blips (3 failures out of 5)\n- 10s window: Fast detection but not too sensitive\n- 30s timeout: Gives Redis time to recover\n- 1 max request: Conservative test (minimize load)\n\n## Trade-offs\n\n**Pros:**\n- Fail-fast prevents cascade failures (timeout waiting → immediate error)\n- Automatic recovery (half-open → closed when Redis healthy)\n- Graceful degradation (app stays up during Redis outage)\n- Low overhead (in-memory state, no external dependencies)\n\n**Cons:**\n- False positives possible (transient blip opens circuit)\n- Data loss during outage (dropped votes)\n- Complexity (one more failure mode to understand)\n- Tuning required (thresholds depend on traffic patterns)\n\n## Alternative: Retry with Exponential Backoff\n\n**Instead of circuit breaker, retry with backoff:**\n```go\nfunc (r *RedisClient) FCallWithRetry(ctx context.Context, fn string, keys []string, args ...any) (*redis.Cmd, error) {\n    backoff := []time.Duration{10*time.Millisecond, 50*time.Millisecond, 200*time.Millisecond}\n    for i, delay := range backoff {\n        result := r.client.FCall(ctx, fn, keys, args...)\n        if result.Err() == nil || !isRetriable(result.Err()) {\n            return result, result.Err()\n        }\n        if i \u003c len(backoff)-1 {\n            time.Sleep(delay)\n        }\n    }\n    return nil, fmt.Errorf(\"max retries exceeded\")\n}\n```\n\n**Comparison:**\n| Feature | Circuit Breaker | Retry with Backoff |\n|---------|----------------|-------------------|\n| Complexity | Medium | Low |\n| Fail-fast | Yes (after threshold) | No (always retries) |\n| Auto-recovery | Yes | N/A |\n| Overhead | Low (state machine) | Medium (sleep delays) |\n| Best for | Cascading failures | Transient errors |\n\n**My recommendation:** Circuit breaker for Redis (SPOF), retry for Twitch API (external)\n\n## Vote\n- Architect-Resilience: +1 (circuit breaker for Redis)\n- Architect-Scalability: ?\n- Architect-Maintainability: ?\n\n## Files to Modify\n- internal/redis/client.go (wrap client in circuit breaker)\n- internal/sentiment/engine.go (handle circuit open in ProcessVote)\n- internal/broadcast/broadcaster.go (fallback on circuit open)\n- internal/app/service.go (propagate circuit open errors)","notes":"Vote: +1 from Maintainability architect. Excellent circuit breaker implementation detail. FailureRatio 0.6 over 10s window is superior to consecutive error counting - more robust under variable load. gobreaker library is production-ready. Recommend merge into ojd as Phase 1 circuit breaker. Your configuration values should be adopted.\nVote: +1 from Maintainability Architect\n\nRATIONALE: Circuit breaker provides clear failure modes (easier debugging than silent retries). Graceful fallback is sensible. Defer Sentinel to Phase 2 (correct prioritization).\n\nANSWERS TO QUESTIONS:\n- Q3 (60% threshold): Reasonable for Redis. Tune to 70% if false-opens occur in prod.\n- Q4 (Twitch API retry): YES, but separate issue (different failure modes).\n\nINTEGRATION: \n- Circuit breaker failure = Tier B Infrastructure Error (my Epic 4 classification)\n- Need ADR-001 update to document circuit breaker strategy\n- Ensure circuit breaker state transitions are logged with structured context\n\nADDITIONAL SUGGESTIONS:\n- Document circuit breaker state transitions in ERROR_HANDLING.md\n- Add operational runbook: \"What to do when Redis circuit breaker opens\" (check health, investigate logs, manual reset if needed)","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:12:18.417985+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.054182+01:00","closed_at":"2026-02-12T17:56:57.054182+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-bdb","title":"Idea: Add Prometheus metrics export and Grafana dashboards","description":"## Proposal\nInstrument the application with Prometheus metrics and provide reference Grafana dashboards for operational visibility.\n\n## Metrics to Instrument\n\n### Redis Metrics\n```go\nredis_operations_total{operation, status} counter\nredis_operation_duration_seconds{operation} histogram\nredis_pool_connections{state} gauge  // active, idle, stale\nredis_errors_total{operation, error_type} counter\n```\n\n### Database Metrics\n```go\ndb_query_duration_seconds{query} histogram\ndb_pool_connections{state} gauge  // acquiring, acquired, idle, max\ndb_errors_total{query, error_type} counter\n```\n\n### Broadcaster Metrics\n```go\nbroadcaster_active_sessions gauge\nbroadcaster_connected_clients{session_uuid} gauge\nbroadcaster_tick_duration_seconds histogram\nbroadcaster_slow_clients_evicted_total counter\nbroadcaster_command_queue_depth gauge\n```\n\n### WebSocket Metrics\n```go\nwebsocket_connections_total counter\nwebsocket_connection_duration_seconds histogram\nwebsocket_disconnects_total{reason} counter\nwebsocket_message_send_duration_seconds histogram\n```\n\n### Twitch API Metrics\n```go\ntwitch_api_requests_total{endpoint, status} counter\ntwitch_api_request_duration_seconds{endpoint} histogram\ntwitch_eventsub_subscriptions gauge\ntwitch_webhook_events_total{event_type} counter\n```\n\n### Application Metrics\n```go\nvotes_processed_total{result} counter  // applied, debounced, no_match\nvotes_dropped_total{reason} counter\nconfig_updates_total{status} counter\nsession_activations_total{result} counter\norphan_cleanups_total{status} counter\n```\n\n## Implementation\nUse prometheus/client_golang:\n```go\nvar (\n    redisOpsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"redis_operations_total\",\n            Help: \"Total Redis operations by type and status\",\n        },\n        []string{\"operation\", \"status\"},\n    )\n)\n\nfunc init() {\n    prometheus.MustRegister(redisOpsTotal)\n}\n```\n\n## Grafana Dashboards\nProvide 3 reference dashboards:\n1. **Overview**: System health, request rate, error rate, active sessions\n2. **Performance**: Latency percentiles, throughput, resource usage\n3. **Reliability**: Error rates, circuit breaker state, pool exhaustion, orphan accumulation\n\n## Related Issues\n- twitch-tow-c8q (Missing observability)\n\n## Files to Modify\n- internal/server/server.go (add /metrics endpoint)\n- internal/redis/client.go (wrap operations with metrics)\n- internal/broadcast/broadcaster.go (emit metrics on tick)\n- grafana/dashboards/*.json (new directory)","notes":"RESOLVED: Already addressed by consolidated observability proposals (twitch-tow-php, twitch-tow-eyl, twitch-tow-3bx). Prometheus metrics + Grafana dashboards covered in Phase 2-3 of observability epic.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:22.581125+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:44:00.697441+01:00","closed_at":"2026-02-12T17:44:00.697444+01:00"}
{"id":"twitch-tow-bhh","title":"EPIC: Lua Function Robustness - Input Validation and Error Handling","description":"**User Story:** As a developer, I need Redis Functions to be robust against invalid input and edge cases so that corrupted data or extreme values don't cause vote processing failures.\n\n**Problem Context:** Current Lua functions lack:\n- Input validation (decay_rate, delta, dt bounds checking)\n- Nil value handling (corrupted hash fields)\n- Overflow protection (math.exp on extreme values)\n- Idempotency (retry applies vote twice)\n\n**Risks:**\n- Data corruption (invalid hash values → nil propagation)\n- Vote double-counting (retries without idempotency)\n- Silent failures (Lua errors return empty string → parsed as 0)\n\n**Solution Overview:** Add input validation, nil handling, overflow protection, and comprehensive Lua function tests with edge cases.\n\n## Task Breakdown\n\n### 1. Add Input Validation to Lua Functions\n\n**File:** `internal/redis/chatpulse.lua`\n\n**Add bounds checking:**\n```lua\n#!lua name=chatpulse\n\nlocal LIBRARY_VERSION = \"2\"\n\n-- Helper: Validate and clamp numeric value\nlocal function validate_number(value, default, min, max, name)\n    local num = tonumber(value)\n    if num == nil then\n        redis.log(redis.LOG_WARNING, \"Invalid \" .. name .. \": \" .. tostring(value) .. \", using default \" .. default)\n        return default\n    end\n    \n    -- Check for NaN/Inf\n    if num ~= num or num == math.huge or num == -math.huge then\n        redis.log(redis.LOG_WARNING, \"Invalid \" .. name .. \" (NaN/Inf): \" .. tostring(num) .. \", using default \" .. default)\n        return default\n    end\n    \n    -- Clamp to range\n    if num \u003c min then\n        return min\n    elseif num \u003e max then\n        return max\n    else\n        return num\n    end\nend\n\n-- apply_vote_v2: Atomically apply time-decayed vote\nredis.register_function{\n  function_name = 'apply_vote_v2',\n  callback = function(keys, args)\n    local session_key = keys[1]\n    local delta = validate_number(args[1], 0, -100, 100, \"delta\")\n    local now = validate_number(args[2], 0, 0, 2147483647, \"now\")  -- Unix timestamp max\n    \n    -- Read current state (with nil handling)\n    local value_str = redis.call('HGET', session_key, 'value')\n    local value = validate_number(value_str, 0, -100, 100, \"value\")\n    \n    local last_update_str = redis.call('HGET', session_key, 'last_update')\n    local last_update = validate_number(last_update_str, now, 0, 2147483647, \"last_update\")\n    \n    local config_json = redis.call('HGET', session_key, 'config_json')\n    if not config_json then\n        redis.log(redis.LOG_WARNING, \"Missing config_json for session \" .. session_key)\n        return {tostring(value), LIBRARY_VERSION, \"error:missing_config\"}\n    end\n    \n    -- Parse decay_speed from config (with error handling)\n    local decay_speed = 1.0\n    local config_match = string.match(config_json, '\"decay_speed\":(%d+%.?%d*)')\n    if config_match then\n        decay_speed = validate_number(config_match, 1.0, 0.1, 10.0, \"decay_speed\")\n    end\n    \n    -- Apply time decay\n    local dt = math.max(0, now - last_update)  -- Prevent negative dt\n    local decay_factor = math.exp(-decay_speed * dt / 60.0)\n    \n    -- Protect against extreme decay (dt \u003e 1 hour = full decay)\n    if dt \u003e 3600 then\n        value = 0  -- Reset to neutral after 1 hour\n    else\n        value = value * decay_factor\n    end\n    \n    -- Apply vote delta\n    value = value + delta\n    \n    -- Clamp to [-100, 100]\n    if value \u003c -100 then value = -100 end\n    if value \u003e 100 then value = 100 end\n    \n    -- Update Redis\n    redis.call('HSET', session_key, 'value', tostring(value))\n    redis.call('HSET', session_key, 'last_update', tostring(now))\n    \n    return {tostring(value), LIBRARY_VERSION, \"ok\"}\n  end,\n  flags = {}  -- Read-write function\n}\n\n-- get_decayed_value_v2: Read time-decayed value\nredis.register_function{\n  function_name = 'get_decayed_value_v2',\n  callback = function(keys, args)\n    local session_key = keys[1]\n    local now = validate_number(args[1], 0, 0, 2147483647, \"now\")\n    \n    -- Read current state (with nil handling)\n    local value_str = redis.call('HGET', session_key, 'value')\n    if not value_str then\n        return {\"0\", LIBRARY_VERSION, \"ok\"}  -- Session doesn't exist\n    end\n    \n    local value = validate_number(value_str, 0, -100, 100, \"value\")\n    local last_update_str = redis.call('HGET', session_key, 'last_update')\n    local last_update = validate_number(last_update_str, now, 0, 2147483647, \"last_update\")\n    \n    local config_json = redis.call('HGET', session_key, 'config_json')\n    if not config_json then\n        return {tostring(value), LIBRARY_VERSION, \"error:missing_config\"}\n    end\n    \n    -- Parse decay_speed\n    local decay_speed = 1.0\n    local config_match = string.match(config_json, '\"decay_speed\":(%d+%.?%d*)')\n    if config_match then\n        decay_speed = validate_number(config_match, 1.0, 0.1, 10.0, \"decay_speed\")\n    end\n    \n    -- Apply time decay\n    local dt = math.max(0, now - last_update)\n    \n    if dt \u003e 3600 then\n        value = 0  -- Reset after 1 hour\n    else\n        local decay_factor = math.exp(-decay_speed * dt / 60.0)\n        value = value * decay_factor\n    end\n    \n    -- Clamp\n    if value \u003c -100 then value = -100 end\n    if value \u003e 100 then value = 100 end\n    \n    return {tostring(value), LIBRARY_VERSION, \"ok\"}\n  end,\n  flags = { 'no-writes' }\n}\n\n-- Legacy function forwarding (backward compat)\nredis.register_function{\n  function_name = 'apply_vote',\n  callback = function(keys, args)\n    local result = redis.call('FCALL', 'apply_vote_v2', 1, keys[1], unpack(args))\n    return result[1]  -- Return just value (legacy format)\n  end\n}\n\nredis.register_function{\n  function_name = 'get_decayed_value',\n  callback = function(keys, args)\n    local result = redis.call('FCALL', 'get_decayed_value_v2', 1, keys[1], unpack(args))\n    return result[1]  -- Return just value (legacy format)\n  end\n}\n```\n\n### 2. Update Go Code to Handle Error Status\n\n**File:** `internal/redis/sentiment_store.go`\n\n**Parse status from Lua response:**\n```go\nfunc (s *SentimentStore) ApplyVote(ctx context.Context, sessionUUID uuid.UUID, delta float64) (float64, error) {\n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    now := s.clock.Now().Unix()\n    \n    result, err := s.client.FCall(ctx, \"apply_vote_v2\", []string{key}, delta, now).Result()\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to apply vote: %w\", err)\n    }\n    \n    // Parse result: [value, version, status]\n    resultSlice, ok := result.([]interface{})\n    if !ok || len(resultSlice) \u003c 3 {\n        return 0, fmt.Errorf(\"unexpected result format from apply_vote_v2\")\n    }\n    \n    valueStr, _ := resultSlice[0].(string)\n    version, _ := resultSlice[1].(string)\n    status, _ := resultSlice[2].(string)\n    \n    // Check status\n    if status != \"ok\" {\n        luaFunctionErrorsTotal.WithLabelValues(\"apply_vote\", status).Inc()\n        s.logger.Warn(\"lua function error\",\n            \"function\", \"apply_vote_v2\",\n            \"status\", status,\n            \"session_uuid\", sessionUUID)\n    }\n    \n    // Validate version\n    if version != LibraryVersion {\n        luaVersionMismatchesTotal.Inc()\n    }\n    \n    value, err := strconv.ParseFloat(valueStr, 64)\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to parse vote result: %w\", err)\n    }\n    \n    return value, nil\n}\n```\n\n### 3. Add Lua Function Error Metrics\n\n**File:** `internal/redis/client.go`\n\n```go\nvar (\n    luaFunctionErrorsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"lua_function_errors_total\",\n            Help: \"Total number of Lua function errors by type\",\n        },\n        []string{\"function\", \"error_type\"},\n    )\n    \n    luaInputValidationTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"lua_input_validation_total\",\n            Help: \"Total number of input validation corrections in Lua\",\n        },\n        []string{\"field\"},  // \"delta\", \"now\", \"value\", \"decay_speed\"\n    )\n)\n```\n\n### 4. Add Comprehensive Lua Tests\n\n**File:** `internal/redis/sentiment_store_lua_test.go` (NEW)\n\n```go\n// TestLua_NilValueHandling verifies nil handling\nfunc TestLua_NilValueHandling(t *testing.T) {\n    // Setup: Session with missing 'value' field\n    // Act: ApplyVote\n    // Assert: Uses 0 as default, vote applied correctly\n}\n\n// TestLua_CorruptedLastUpdate verifies timestamp handling\nfunc TestLua_CorruptedLastUpdate(t *testing.T) {\n    // Setup: Session with 'last_update' = \"invalid\"\n    // Act: GetSentiment\n    // Assert: Uses current time as default, no error\n}\n\n// TestLua_ExtremeDecayRate verifies overflow protection\nfunc TestLua_ExtremeDecayRate(t *testing.T) {\n    // Setup: Config with decay_speed = 999999\n    // Act: GetSentiment after 10 minutes\n    // Assert: Value clamped to 0 (full decay)\n}\n\n// TestLua_ExtremeDelta verifies delta clamping\nfunc TestLua_ExtremeDelta(t *testing.T) {\n    // Setup: Session at value=50\n    // Act: ApplyVote with delta=200\n    // Assert: Value clamped to 100 (not 250)\n}\n\n// TestLua_NegativeTimestamp verifies dt protection\nfunc TestLua_NegativeTimestamp(t *testing.T) {\n    // Setup: Session with last_update in future (clock skew)\n    // Act: ApplyVote\n    // Assert: dt=0, no decay applied\n}\n\n// TestLua_LongInactivity verifies reset after 1 hour\nfunc TestLua_LongInactivity(t *testing.T) {\n    // Setup: Session inactive for 2 hours\n    // Act: GetSentiment\n    // Assert: Value reset to 0\n}\n\n// TestLua_ConcurrentVotes verifies atomicity\nfunc TestLua_ConcurrentVotes(t *testing.T) {\n    // Setup: 10 goroutines apply +10 vote simultaneously\n    // Assert: Final value = 100 (all votes applied, clamped)\n}\n```\n\n### 5. Documentation\n\n**Update CLAUDE.md:**\n```markdown\n### Lua Function Robustness\n\n**Input validation:**\n- All numeric inputs validated with bounds checking\n- Invalid/nil values replaced with safe defaults (logged)\n- Overflow protection: extreme dt (\u003e1 hour) → reset to 0\n\n**Error handling:**\n- Functions return status: \"ok\" or \"error:reason\"\n- Go code checks status, logs errors, increments metrics\n- Parsing errors don't crash (fallback to safe defaults)\n\n**Edge cases handled:**\n- Corrupted hash fields (nil/non-numeric) → default to 0\n- Negative timestamps (clock skew) → dt=0\n- Extreme decay rates (\u003e10) → clamped\n- Vote delta overflow (\u003e100) → clamped\n\n**Metrics:**\n- lua_function_errors_total{function, error_type}\n- lua_input_validation_total{field}\n```\n\n## Acceptance Criteria\n\n✅ Lua functions validate all numeric inputs with bounds\n✅ Nil/corrupted values handled gracefully (default to 0)\n✅ Overflow protection prevents NaN/Inf from extreme values\n✅ Functions return status code (\"ok\" or \"error:reason\")\n✅ Go code checks status, logs errors, increments metrics\n✅ Tests verify nil handling, overflow, extreme values, concurrency\n✅ CLAUDE.md documents robustness features\n\n## Dependencies\n\n- Synergy with: Lua versioning epic (twitch-tow-0kl)\n\n## Files Modified\n\n**Modified:**\n- internal/redis/chatpulse.lua (input validation, error handling)\n- internal/redis/sentiment_store.go (parse status, handle errors)\n- internal/redis/client.go (error metrics)\n- CLAUDE.md (document robustness)\n\n**New:**\n- internal/redis/sentiment_store_lua_test.go (edge case tests)\n\n## Estimated Effort\n\n**Implementation:** 2 developer-days\n- Lua validation: 1 day\n- Go error handling: 3 hours\n- Metrics: 2 hours\n- Testing: 4 hours\n- Documentation: 1 hour\n\n**Total:** 2 developer-days\n\n## Rollout Strategy\n\n1. Deploy with enhanced validation (backward compatible)\n2. Monitor lua_input_validation_total (observe how often validation triggers)\n3. Monitor lua_function_errors_total (should be 0 in healthy system)\n4. Alert if error rate \u003e10/hour (investigate data corruption)\n5. Use metrics to identify which fields most often need validation","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:47:57.297827+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:42.710267+01:00"}
{"id":"twitch-tow-bl7","title":"Discussion: Singleflight usage in EnsureSessionActive","description":"The app.Service uses golang.org/x/sync/singleflight to deduplicate concurrent session activations. Current implementation in app/service.go collapses concurrent calls for the same overlayUUID so only ONE activation runs, others wait and share result. This prevents race conditions and duplicate Twitch EventSub subscriptions.\n\nBenefits: (1) Thundering herd protection - 100 clients connecting simultaneously results in only 1 Redis check + 1 Twitch API call. (2) Prevents duplicate subscriptions - without this concurrent activations could create multiple EventSub subscriptions. (3) Reduces DB load - avoids N concurrent GetByOverlayUUID queries.\n\nConcerns: (1) Shared context lifetime - all waiters share context of first caller. If first context cancels all fail. (2) Error propagation - if one activation fails ALL waiting requests fail together. (3) No timeout per singleflight call - slow activation blocks all waiters indefinitely. (4) Memory leak potential - if activation hangs forever key stays in map.\n\nTrade-offs: Excellent for preventing duplicate EventSub subscriptions (critical). Reduces load on Twitch API. Simple implementation. But shared fate means one bad request can fail all concurrent requests. No per-caller timeout control. Debug difficulty - hard to trace which caller triggered actual activation.\n\nRecommendation: Keep singleflight since benefits outweigh risks. Consider adding timeout wrapper if activation becomes slow.","notes":"RESOLVED: Converted to Epic 8 (twitch-tow-81p) - Singleflight Timeout Protection. Implements DoChan pattern for per-caller timeout control while preserving thundering herd protection. 2 hours effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:08:21.982586+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:41:39.013787+01:00","closed_at":"2026-02-12T17:41:39.01379+01:00"}
{"id":"twitch-tow-bo6","title":"EPIC: Robust Orphan Cleanup - Prevent Race Condition Deletions","description":"**User Story:** As an operator, I need orphan cleanup to be robust against race conditions so that active sessions are never accidentally deleted during reconnection windows.\n\n**Problem Context:** Current orphan cleanup (30s ticker) has a race condition (CASE 2) where a session can be deleted while a client is reconnecting. Timeline:\n- T=0: Client disconnects, `last_disconnect` set\n- T=31: Cleanup scanner finds session (31s old)\n- T=31.5: Client reconnects (while cleanup processing)\n- T=32: Cleanup deletes session (active session destroyed!)\n\nResult: Client sees \"reconnecting...\" status, next broadcast fails, requires full page reload.\n\n**Solution Overview:** Add ref count validation before deletion + idempotency checks + improved monitoring to make cleanup safe during race windows.\n\n## Task Breakdown\n\n### 1. Add Ref Count Validation to DeleteSession\n\n**File:** `internal/redis/session_repository.go`\n\n**Change:**\n```go\nfunc (r *SessionRepo) DeleteSession(ctx context.Context, sessionUUID uuid.UUID) error {\n    // Check ref count before deleting\n    refCount, err := r.GetRefCount(ctx, sessionUUID)\n    if err != nil \u0026\u0026 !errors.Is(err, redis.Nil) {\n        return fmt.Errorf(\"failed to check ref count before delete: %w\", err)\n    }\n    \n    if refCount \u003e 0 {\n        // Session is still active on another instance, skip deletion\n        return fmt.Errorf(\"%w: ref_count=%d\", ErrSessionActive, refCount)\n    }\n    \n    // Existing deletion logic\n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    pipe := r.client.Pipeline()\n    pipe.Del(ctx, key)\n    pipe.Del(ctx, fmt.Sprintf(\"ref_count:%s\", sessionUUID))\n    // ... broadcaster mapping, etc\n    _, err = pipe.Exec(ctx)\n    return err\n}\n```\n\n**Add sentinel error:**\n```go\n// internal/domain/errors.go\nvar ErrSessionActive = errors.New(\"session is active, cannot delete\")\n```\n\n### 2. Handle ErrSessionActive in Cleanup\n\n**File:** `internal/app/service.go`\n\n**Change:**\n```go\nfunc (s *Service) CleanupOrphans(ctx context.Context) error {\n    orphans, err := s.sessions.ListOrphanSessions(ctx, orphanMaxAge)\n    // ... error handling\n    \n    for _, sessionUUID := range orphans {\n        err := s.sessions.DeleteSession(ctx, sessionUUID)\n        if err != nil {\n            if errors.Is(err, domain.ErrSessionActive) {\n                // Expected: session reconnected during scan, skip\n                s.logger.Debug(\"skipped active session during cleanup\",\n                    \"session_uuid\", sessionUUID)\n                continue\n            }\n            // Unexpected error\n            s.logger.Error(\"failed to delete orphan session\",\n                \"session_uuid\", sessionUUID,\n                \"error\", err)\n            continue\n        }\n        \n        // Background unsubscribe (unchanged)\n        go func(uuid uuid.UUID) {\n            // ... existing background unsubscribe logic\n        }(sessionUUID)\n    }\n    return nil\n}\n```\n\n### 3. Add Idempotent Unsubscribe\n\n**File:** `internal/app/service.go` (background goroutine)\n\n**Change:**\n```go\ngo func(uuid uuid.UUID) {\n    ctx := context.Background()  // Fire-and-forget\n    \n    // Get subscription info before deleting\n    sub, err := s.twitchSvc.GetSubscription(ctx, broadcasterUserID)\n    if err != nil {\n        if errors.Is(err, domain.ErrSubscriptionNotFound) {\n            // Already unsubscribed (idempotent), success\n            s.logger.Debug(\"subscription already removed\", \"session_uuid\", uuid)\n            return\n        }\n        s.logger.Error(\"failed to check subscription\", \"error\", err)\n        return\n    }\n    \n    // Unsubscribe\n    err = s.twitchSvc.Unsubscribe(ctx, broadcasterUserID)\n    if err != nil {\n        s.logger.Error(\"background unsubscribe failed\", \"error\", err)\n        // Metric for monitoring\n        cleanupUnsubscribeErrorsTotal.Inc()\n    }\n}(sessionUUID)\n```\n\n### 4. Add Cleanup Metrics\n\n**File:** `internal/app/service.go` (add Prometheus metrics)\n\n```go\nvar (\n    orphanCleanupScansTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"orphan_cleanup_scans_total\",\n            Help: \"Total number of orphan cleanup scans\",\n        })\n    \n    orphanSessionsDeletedTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"orphan_sessions_deleted_total\",\n            Help: \"Total number of orphan sessions deleted\",\n        })\n    \n    orphanSessionsSkippedTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"orphan_sessions_skipped_total\",\n            Help: \"Total number of sessions skipped during cleanup\",\n        },\n        []string{\"reason\"},  // \"active\", \"error\"\n    )\n    \n    orphanCleanupDurationSeconds = prometheus.NewHistogram(\n        prometheus.HistogramOpts{\n            Name: \"orphan_cleanup_duration_seconds\",\n            Help: \"Duration of orphan cleanup scans\",\n            Buckets: []float64{0.1, 0.5, 1.0, 5.0, 10.0, 30.0},\n        })\n    \n    cleanupUnsubscribeErrorsTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"cleanup_unsubscribe_errors_total\",\n            Help: \"Total number of background unsubscribe failures\",\n        })\n)\n\nfunc init() {\n    prometheus.MustRegister(\n        orphanCleanupScansTotal,\n        orphanSessionsDeletedTotal,\n        orphanSessionsSkippedTotal,\n        orphanCleanupDurationSeconds,\n        cleanupUnsubscribeErrorsTotal,\n    )\n}\n```\n\n### 5. Add Cleanup Duration Tracking\n\n**File:** `internal/app/service.go`\n\n```go\nfunc (s *Service) CleanupOrphans(ctx context.Context) error {\n    start := s.clock.Now()\n    defer func() {\n        orphanCleanupScansTotal.Inc()\n        orphanCleanupDurationSeconds.Observe(s.clock.Since(start).Seconds())\n    }()\n    \n    // ... existing cleanup logic with metrics\n    \n    for _, sessionUUID := range orphans {\n        err := s.sessions.DeleteSession(ctx, sessionUUID)\n        if err != nil {\n            if errors.Is(err, domain.ErrSessionActive) {\n                orphanSessionsSkippedTotal.WithLabelValues(\"active\").Inc()\n                continue\n            }\n            orphanSessionsSkippedTotal.WithLabelValues(\"error\").Inc()\n            continue\n        }\n        orphanSessionsDeletedTotal.Inc()\n        // ... background unsubscribe\n    }\n    return nil\n}\n```\n\n### 6. Testing\n\n**New test file:** `internal/app/service_orphan_test.go`\n\n**Test cases:**\n```go\n// TestCleanupOrphans_RaceCondition verifies ref count prevents deletion\nfunc TestCleanupOrphans_RaceCondition(t *testing.T) {\n    // Setup: Session disconnected 31s ago\n    // Act: IncrRefCount (simulate reconnect) then CleanupOrphans\n    // Assert: Session NOT deleted (ref count \u003e 0)\n}\n\n// TestCleanupOrphans_IdempotentUnsubscribe verifies no error on duplicate\nfunc TestCleanupOrphans_IdempotentUnsubscribe(t *testing.T) {\n    // Setup: Subscription already removed\n    // Act: CleanupOrphans\n    // Assert: No error, metric shows \"already removed\"\n}\n\n// TestCleanupOrphans_MetricsTracking verifies metrics\nfunc TestCleanupOrphans_MetricsTracking(t *testing.T) {\n    // Setup: 10 orphans (5 deleted, 3 active, 2 errors)\n    // Act: CleanupOrphans\n    // Assert: Metrics show correct counts\n}\n```\n\n### 7. Documentation\n\n**Update CLAUDE.md:**\n\nAdd section under \"Application Layer\":\n```markdown\n### Orphan Cleanup Robustness\n\nCleanup runs every 30s, scans for sessions disconnected \u003e30s ago. Race condition handling:\n\n**Race window:** Session reconnects while cleanup is processing. Mitigated by:\n1. Ref count validation before deletion (skip if ref_count \u003e 0)\n2. Idempotent Twitch unsubscribe (no error if already removed)\n3. Metrics track skipped sessions (reason: \"active\" or \"error\")\n\n**Metrics:**\n- orphan_cleanup_scans_total (counter)\n- orphan_sessions_deleted_total (counter)\n- orphan_sessions_skipped_total{reason} (counter)\n- orphan_cleanup_duration_seconds (histogram)\n- cleanup_unsubscribe_errors_total (counter)\n```\n\n## Acceptance Criteria\n\n✅ Ref count validation prevents deletion of active sessions\n✅ Cleanup skips sessions with ref_count \u003e 0 (logs at DEBUG level)\n✅ ErrSessionActive sentinel error returned when session is active\n✅ Background unsubscribe is idempotent (no error if subscription missing)\n✅ Metrics track cleanup scans, deletions, skips (by reason), duration, unsubscribe errors\n✅ Tests verify race condition handling (ref count prevents deletion)\n✅ Tests verify idempotent unsubscribe behavior\n✅ CLAUDE.md documents race condition and mitigation\n\n## Dependencies\n\n- None (self-contained improvement)\n- Synergy with: Observability epic (metrics)\n\n## Files Modified\n\n**Modified:**\n- internal/domain/errors.go (add ErrSessionActive)\n- internal/redis/session_repository.go (add ref count check to DeleteSession)\n- internal/app/service.go (handle ErrSessionActive, add metrics, idempotent unsubscribe)\n- CLAUDE.md (document cleanup robustness)\n\n**New:**\n- internal/app/service_orphan_test.go (orphan cleanup tests)\n\n## Estimated Effort\n\n**Implementation:** 2 developer-days\n- Ref count validation: 2 hours\n- Error handling: 1 hour\n- Metrics: 2 hours\n- Idempotent unsubscribe: 1 hour\n- Testing: 1 day (race condition tests require coordination)\n- Documentation: 2 hours\n\n**Total:** 2 developer-days\n\n## Rollout Strategy\n\n1. Deploy with metrics first (monitor baseline)\n2. Verify ref count check prevents false positives (no active sessions skipped incorrectly)\n3. Monitor skip rate (should be \u003c1% of cleanup scans)\n4. Alert if skip rate \u003e10% (indicates high reconnection churn or bugs)","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:36:16.865465+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:46.638453+01:00"}
{"id":"twitch-tow-bqx","title":"EPIC: Structured Error Handling with Context Propagation","description":"**User Story:** As a developer and operator, I need structured error handling with proper context propagation so I can diagnose issues quickly and handle errors appropriately at each layer.\n\n**Problem Context:** Current error handling issues:\n- Webhook handler drops votes silently without logging chatter ID or broadcaster ID\n- Generic \"failed to process vote\" logs lack actionable context\n- No distinction between transient errors (retry) vs permanent errors (alert)\n- Repository errors lose context during translation (pgx.ErrNoRows → domain.ErrUserNotFound)\n- Panic recovery in Echo middleware catches but doesn't classify errors\n\n**Solution Overview:** Implement structured error handling with:\n1. Sentinel errors with context wrapping (%w)\n2. Error classification (transient, permanent, validation)\n3. Contextual logging at error boundaries\n4. Panic recovery with structured logging\n\n## Task Breakdown\n\n### 1. Define Error Types (internal/domain/errors.go)\n\nAdd error classification:\n```go\npackage domain\n\nimport \"errors\"\n\n// Existing sentinel errors\nvar (\n    ErrUserNotFound         = errors.New(\"user not found\")\n    ErrConfigNotFound       = errors.New(\"config not found\")\n    ErrSubscriptionNotFound = errors.New(\"subscription not found\")\n)\n\n// New: Transient errors (should retry)\nvar (\n    ErrRedisUnavailable = errors.New(\"redis temporarily unavailable\")\n    ErrTwitchAPIDown    = errors.New(\"twitch API temporarily unavailable\")\n    ErrDatabaseTimeout  = errors.New(\"database timeout\")\n)\n\n// New: Validation errors (client error, don't retry)\nvar (\n    ErrInvalidConfig     = errors.New(\"invalid configuration\")\n    ErrInvalidSessionID  = errors.New(\"invalid session ID format\")\n    ErrDebounced         = errors.New(\"vote rate limited\")\n)\n\n// Error classification helper\nfunc IsTransient(err error) bool {\n    return errors.Is(err, ErrRedisUnavailable) ||\n           errors.Is(err, ErrTwitchAPIDown) ||\n           errors.Is(err, ErrDatabaseTimeout)\n}\n\nfunc IsValidationError(err error) bool {\n    return errors.Is(err, ErrInvalidConfig) ||\n           errors.Is(err, ErrInvalidSessionID) ||\n           errors.Is(err, ErrDebounced)\n}\n```\n\n### 2. Add Context to Repository Errors\n\n**UserRepository (internal/database/user_repository.go):**\n```go\nfunc (r *UserRepo) GetByID(ctx context.Context, id uuid.UUID) (*domain.User, error) {\n    // ... query\n    if err := row.Scan(...); err != nil {\n        if errors.Is(err, pgx.ErrNoRows) {\n            // ADD CONTEXT: which user ID was not found\n            return nil, fmt.Errorf(\"%w: id=%s\", domain.ErrUserNotFound, id)\n        }\n        // ADD CONTEXT: what operation failed\n        return nil, fmt.Errorf(\"failed to scan user row for id=%s: %w\", id, err)\n    }\n    // ...\n}\n\nfunc (r *UserRepo) UpdateTokens(ctx context.Context, id uuid.UUID, ...) error {\n    // ... update\n    if result.RowsAffected() == 0 {\n        // ADD CONTEXT: which user's tokens failed to update\n        return fmt.Errorf(\"%w: id=%s\", domain.ErrUserNotFound, id)\n    }\n    return nil\n}\n```\n\nApply same pattern to:\n- ConfigRepository.GetByUserID / Update\n- EventSubRepository operations\n- SessionRepository operations (add session UUID context)\n\n### 3. Structured Logging in Engine.ProcessVote\n\n**Current (internal/sentiment/engine.go):**\n```go\nfunc (e *Engine) ProcessVote(ctx context.Context, broadcasterUserID, chatterUserID, messageText string) error {\n    session, err := e.sessions.GetSessionByBroadcaster(ctx, broadcasterUserID)\n    if err != nil {\n        return fmt.Errorf(\"failed to get session: %w\", err)  // Generic\n    }\n    // ...\n}\n```\n\n**Improved:**\n```go\nfunc (e *Engine) ProcessVote(ctx context.Context, broadcasterUserID, chatterUserID, messageText string) error {\n    session, err := e.sessions.GetSessionByBroadcaster(ctx, broadcasterUserID)\n    if err != nil {\n        // ADD CONTEXT: log at boundary, include all relevant IDs\n        if errors.Is(err, domain.ErrSessionNotFound) {\n            // Expected: streamer not yet activated, log at DEBUG\n            e.logger.Debug(\"session not found for broadcaster\",\n                \"broadcaster_id\", broadcasterUserID,\n                \"chatter_id\", chatterUserID,\n                \"message\", messageText,\n                \"error\", err)\n            return nil  // Don't propagate (not an error condition)\n        }\n        // Unexpected error: log at ERROR\n        e.logger.Error(\"failed to get session for vote\",\n            \"broadcaster_id\", broadcasterUserID,\n            \"chatter_id\", chatterUserID,\n            \"error\", err)\n        return fmt.Errorf(\"failed to get session for broadcaster=%s: %w\", broadcasterUserID, err)\n    }\n\n    config, err := e.sessions.GetSessionConfig(ctx, session.UUID)\n    if err != nil {\n        e.logger.Error(\"failed to get config for vote\",\n            \"session_uuid\", session.UUID,\n            \"broadcaster_id\", broadcasterUserID,\n            \"chatter_id\", chatterUserID,\n            \"error\", err)\n        return fmt.Errorf(\"failed to get config for session=%s: %w\", session.UUID, err)\n    }\n\n    delta := matchTrigger(messageText, config)\n    if delta == 0 {\n        // Not an error, no log needed (or trace level if needed)\n        return nil\n    }\n\n    allowed, err := e.debouncer.CheckDebounce(ctx, session.UUID, chatterUserID)\n    if err != nil {\n        e.logger.Warn(\"debounce check failed, allowing vote\",\n            \"session_uuid\", session.UUID,\n            \"chatter_id\", chatterUserID,\n            \"error\", err)\n        // Continue: better to allow a duplicate vote than drop legitimate one\n    } else if !allowed {\n        // Rate limited: trace level (not an error)\n        e.logger.Debug(\"vote debounced\",\n            \"session_uuid\", session.UUID,\n            \"chatter_id\", chatterUserID)\n        return nil\n    }\n\n    err = e.sentimentStore.ApplyVote(ctx, session.UUID, delta)\n    if err != nil {\n        e.logger.Error(\"failed to apply vote\",\n            \"session_uuid\", session.UUID,\n            \"broadcaster_id\", broadcasterUserID,\n            \"chatter_id\", chatterUserID,\n            \"delta\", delta,\n            \"error\", err)\n        return fmt.Errorf(\"failed to apply vote for session=%s: %w\", session.UUID, err)\n    }\n\n    e.logger.Debug(\"vote processed successfully\",\n        \"session_uuid\", session.UUID,\n        \"broadcaster_id\", broadcasterUserID,\n        \"chatter_id\", chatterUserID,\n        \"delta\", delta)\n    return nil\n}\n```\n\n**Key improvements:**\n- Log at error boundary (Engine, not deep in repositories)\n- Include all relevant IDs (broadcaster, chatter, session UUID)\n- Differentiate expected (session not found) from unexpected errors\n- Use appropriate log levels (DEBUG for expected, ERROR for unexpected)\n- Continue on debounce failure (fail open)\n\n### 4. Webhook Handler Error Logging\n\n**Current (internal/twitch/webhook.go):**\n```go\nfunc (h *WebhookHandler) HandleEventSub(c echo.Context) error {\n    // ... verification\n    if event.Subscription.Type == \"channel.chat.message\" {\n        err := h.engine.ProcessVote(ctx, broadcasterID, chatterID, message)\n        if err != nil {\n            // Silent drop or generic log\n            return nil\n        }\n    }\n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n```\n\n**Improved:**\n```go\nfunc (h *WebhookHandler) HandleEventSub(c echo.Context) error {\n    // ... verification\n    if event.Subscription.Type == \"channel.chat.message\" {\n        err := h.engine.ProcessVote(ctx, broadcasterID, chatterID, message)\n        if err != nil {\n            // Log at webhook boundary with request ID\n            h.logger.Error(\"webhook vote processing failed\",\n                \"request_id\", c.Response().Header().Get(echo.HeaderXRequestID),\n                \"broadcaster_id\", broadcasterID,\n                \"chatter_id\", chatterID,\n                \"message\", message,\n                \"subscription_id\", event.Subscription.ID,\n                \"error\", err)\n            // Still return 200 to Twitch (prevent retry storm)\n        }\n    }\n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n```\n\n### 5. Panic Recovery Middleware\n\n**Add to internal/server/server.go:**\n```go\nfunc panicRecoveryMiddleware(logger *slog.Logger) echo.MiddlewareFunc {\n    return func(next echo.HandlerFunc) echo.HandlerFunc {\n        return func(c echo.Context) error {\n            defer func() {\n                if r := recover(); r != nil {\n                    err, ok := r.(error)\n                    if !ok {\n                        err = fmt.Errorf(\"%v\", r)\n                    }\n                    \n                    // Structured panic log with stack trace\n                    stack := debug.Stack()\n                    logger.Error(\"panic recovered\",\n                        \"error\", err,\n                        \"method\", c.Request().Method,\n                        \"path\", c.Request().URL.Path,\n                        \"request_id\", c.Response().Header().Get(echo.HeaderXRequestID),\n                        \"stack\", string(stack))\n                    \n                    // Return 500 to client\n                    c.JSON(500, map[string]string{\n                        \"error\": \"internal server error\",\n                        \"request_id\": c.Response().Header().Get(echo.HeaderXRequestID),\n                    })\n                }\n            }()\n            return next(c)\n        }\n    }\n}\n\n// In NewServer():\nfunc NewServer(cfg ServerConfig) (*Server, error) {\n    e := echo.New()\n    e.Use(middleware.RequestID())  // Generate request ID first\n    e.Use(panicRecoveryMiddleware(cfg.Logger))  // Add panic recovery\n    e.Use(middleware.Logger())\n    e.Use(middleware.Recover())  // Keep Echo's recover as fallback\n    // ...\n}\n```\n\n### 6. Redis Error Classification\n\n**internal/redis/session_repository.go:**\n```go\nfunc (r *SessionRepo) GetSessionByBroadcaster(ctx context.Context, broadcasterUserID string) (*domain.Session, error) {\n    key := fmt.Sprintf(\"broadcaster:%s\", broadcasterUserID)\n    overlayUUID, err := r.client.Get(ctx, key).Result()\n    if err != nil {\n        if errors.Is(err, redis.Nil) {\n            // Expected: not found\n            return nil, fmt.Errorf(\"%w: broadcaster=%s\", domain.ErrSessionNotFound, broadcasterUserID)\n        }\n        // Classify transient errors\n        if isRedisConnectionError(err) {\n            return nil, fmt.Errorf(\"%w: %s\", domain.ErrRedisUnavailable, err)\n        }\n        return nil, fmt.Errorf(\"redis get failed for broadcaster=%s: %w\", broadcasterUserID, err)\n    }\n    // ...\n}\n\n// Helper to classify Redis errors\nfunc isRedisConnectionError(err error) bool {\n    if err == nil {\n        return false\n    }\n    errStr := err.Error()\n    return strings.Contains(errStr, \"connection refused\") ||\n           strings.Contains(errStr, \"i/o timeout\") ||\n           strings.Contains(errStr, \"connection reset\")\n}\n```\n\n### 7. Add Logger to All Services\n\n**Current:** Logger is only in `app.Service` and `WebhookHandler`.\n\n**Add logger to:**\n- `sentiment.Engine` (for ProcessVote logging)\n- `broadcast.Broadcaster` (for client errors)\n- `redis.SessionRepo` (for Redis operation failures, optional)\n\nPass `*slog.Logger` to constructors, thread from main.go.\n\n### 8. Testing\n\n**Unit tests (internal/sentiment/engine_test.go):**\n- Verify error context includes session UUID\n- Verify error wrapping with %w preserves Is() checks\n- Verify debounce failure doesn't block vote\n\n**Integration tests:**\n- Simulate Redis connection failures, verify ErrRedisUnavailable\n- Simulate database timeouts, verify ErrDatabaseTimeout\n- Trigger panic in handler, verify structured log + 500 response\n\n**Log output validation:**\n- Parse structured logs (JSON), assert required fields present\n- Verify sensitive data NOT logged (message content truncated to 100 chars)\n\n### 9. Documentation Updates\n\n**CLAUDE.md additions:**\n- Error Handling section documenting:\n  - Sentinel errors and when they're returned\n  - Error classification (transient, validation, permanent)\n  - Logging levels by severity (DEBUG, WARN, ERROR)\n  - Context propagation with %w\n  - Panic recovery behavior\n\n## Acceptance Criteria\n\n✅ All repository errors include relevant IDs in context\n✅ Webhook handler logs broadcaster_id, chatter_id on vote failure\n✅ Engine.ProcessVote logs at appropriate levels (DEBUG/ERROR)\n✅ Debounce failures fail open (allow vote + log warning)\n✅ Session not found during vote is DEBUG level (expected)\n✅ Redis connection errors classified as ErrRedisUnavailable\n✅ Panic recovery logs structured error with stack trace + request ID\n✅ Tests verify error wrapping preserves errors.Is() checks\n✅ CLAUDE.md documents error handling patterns\n\n## Dependencies\n\n- Depends on: None (can be implemented independently)\n- Synergy with: Redis Circuit Breaker (both improve resilience)\n\n## Files Modified/Created\n\n**Modified files:**\n- internal/domain/errors.go (add error types + classification helpers)\n- internal/database/user_repository.go (add context to errors)\n- internal/database/config_repository.go (add context to errors)\n- internal/database/eventsub_repository.go (add context to errors)\n- internal/redis/session_repository.go (add context + classify errors)\n- internal/redis/sentiment_store.go (add context to errors)\n- internal/redis/debouncer.go (add context to errors)\n- internal/sentiment/engine.go (add logger + structured logging)\n- internal/twitch/webhook.go (add detailed error logging)\n- internal/server/server.go (add panic recovery middleware)\n- internal/broadcast/broadcaster.go (add logger for client errors)\n- cmd/server/main.go (thread logger to new services)\n- CLAUDE.md (document error handling patterns)\n\n**New files:**\n- None (all modifications to existing files)\n\n## Estimated Effort\n\n**Implementation:** 4-5 developer-days\n**Testing:** 2 developer-days\n**Documentation:** 1 developer-day\n**Total:** ~1.5 developer-weeks\n\n## Rollout Strategy\n\n1. Add error types and classification helpers (backward compatible)\n2. Add loggers to services (no behavior change yet)\n3. Roll out repository error context (one package at a time)\n4. Add structured logging in Engine.ProcessVote\n5. Add webhook error logging\n6. Add panic recovery middleware\n7. Deploy to staging, trigger error scenarios, validate logs\n8. Deploy to production, monitor error rates and log quality","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:26:21.643394+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:48.452564+01:00"}
{"id":"twitch-tow-bwp","title":"Discussion: Broadcaster actor single-goroutine bottleneck","description":"The Broadcaster actor uses a single goroutine with a buffered command channel (cap 256) to process all operations. This creates a potential bottleneck.\n\nArchitecture:\n- Single actor goroutine per Broadcaster instance\n- Command channel: buffered 256\n- Commands: register, unregister, getClientCount, stop\n- Tick: 50ms interval (20 ticks/sec)\n\nPer-tick processing:\n- Iterate all active sessions (O(N))\n- Call Redis for each session (O(N) blocking calls with 2s timeout)\n- JSON marshal once per session\n- Fan-out to all clients per session\n\nBottleneck analysis:\n\n1. Serial tick processing\n- Processes sessions sequentially\n- Blocked on Redis timeout (2s max per session)\n- One slow session delays broadcast to all other sessions\n\n2. Command channel saturation\n- 256 buffer can fill with register/unregister bursts\n- Register command blocks waiting for reply (5s timeout)\n- High WS churn could exhaust buffer\n\n3. Single-threaded JSON marshaling\n- Marshals SessionUpdate for each session every tick\n- At 1K sessions: 20K JSON marshals/sec\n- Not CPU-intensive but adds latency\n\nCurrent protection:\n✓ Timeout on register commands (5s)\n✓ Per-session Redis timeout (2s)\n✓ Non-blocking send to slow clients (disconnects them)\n✓ Context cancellation checks\n\nScalability limits:\n- Single instance can handle ~1K sessions before tick latency increases\n- Command processing latency grows with session count\n- No horizontal scaling within a single instance\n\nPotential solutions:\n\nA. Worker pool for Redis calls (parallel session polling)\nB. Batch Redis calls using MGET/Lua script\nC. Sharded broadcasters (multiple actors, partition by session)\nD. Separate actor per session (extreme, high goroutine count)\n\nRecommendation:\nCurrent design is fine for \u003c1K sessions per instance.\nAt higher scale, implement batched Redis calls (option B).\n\nPriority: P2 - single-goroutine is bottleneck above 1K sessions/instance\n","notes":"CONVERTED TO EPIC: twitch-tow-k24 (Epic: Optimize Broadcaster Performance - Parallel Redis Calls). This epic implements worker pool pattern for parallel Redis queries, improving tick latency from 1s to 100ms at 1,000 sessions (10x improvement). Includes full implementation details, testing strategy, and scaling guidelines.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:20.966215+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:40:02.7051+01:00","closed_at":"2026-02-12T17:40:02.705103+01:00"}
{"id":"twitch-tow-c2c","title":"CONSENSUS: Redis resilience - unified implementation plan","description":"## Consensus Solution: Redis Resilience Strategy\n\n**Status:** Consolidated from 3 architect proposals with strong alignment\n**Related beads:** twitch-tow-ojd, twitch-tow-hdl, twitch-tow-b9h, twitch-tow-usj, twitch-tow-9yg, twitch-tow-fgm\n\n**Votes:**\n- Architect-Resilience: +1\n- Architect-Scalability: +1 (on twitch-tow-ojd)\n- Architect-Maintainability: +1 (on twitch-tow-hdl)\n\n---\n\n## Problem Statement\n\n**Current state:**\n- Single Redis instance = SPOF (total outage if Redis crashes)\n- No circuit breaker = cascading failures when Redis is slow\n- No connection pool tuning = potential exhaustion under load\n- Throughput limit: ~200K ops/sec (blocks scaling beyond 10 instances)\n\n**Consensus:** All 3 architects agree this is the #1 scalability/resilience blocker\n\n---\n\n## Unified 4-Phase Implementation Plan\n\n### Phase 1: Circuit Breaker + Connection Pool (Week 1) - CRITICAL\n\n**Goal:** Prevent cascading failures, tune connection pool\n\n**Circuit Breaker:**\n```go\nimport \"github.com/sony/gobreaker\"\n\nsettings := gobreaker.Settings{\n    Name:        \"redis\",\n    MaxRequests: 1,                    // Half-open: only 1 test request\n    Interval:    10 * time.Second,     // Failure tracking window\n    Timeout:     30 * time.Second,     // Wait before retry\n    ReadyToTrip: func(counts gobreaker.Counts) bool {\n        failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)\n        return counts.Requests \u003e= 5 \u0026\u0026 failureRatio \u003e= 0.6  // 60% failure threshold\n    },\n}\n\ncircuitBreaker := gobreaker.NewCircuitBreaker(settings)\n```\n\n**Fallback Behavior:**\n\n| Operation | Circuit Open | Rationale |\n|-----------|-------------|-----------|\n| GetCurrentValue | Return 0.0 | Neutral sentiment (safe default) |\n| ApplyVote | Drop vote + metric | Vote loss OK vs. cascade |\n| EnsureSessionActive | Return error | Block new sessions |\n| UpdateConfig | Return error | DB is source of truth |\n\n**Connection Pool Configuration:**\n```go\nopts := \u0026redis.Options{\n    Addr:         redisURL,\n    PoolSize:     50,              // Max connections per instance\n    MinIdleConns: 10,              // Keep warm connections\n    MaxConnAge:   5 * time.Minute, // Rotate connections\n    DialTimeout:  5 * time.Second,\n    ReadTimeout:  3 * time.Second,\n    WriteTimeout: 3 * time.Second,\n}\n```\n\n**Deliverables:**\n- Circuit breaker wrapper for Redis client\n- Fallback behavior implemented\n- Connection pool configured\n- Metrics: circuit_breaker_state, redis_errors_total\n\n**Effort:** 12 hours (circuit breaker: 8h, pool config: 4h)\n\n---\n\n### Phase 2: Config Cache (Week 2) - QUICK WIN\n\n**Goal:** Reduce Redis load by 99%\n\n**Current load:**\n- 1K sessions × 20 calls/sec = 20K config reads/sec\n- Config changes infrequently (minutes/hours)\n\n**Implementation:**\n```go\ntype ConfigCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]*cacheEntry\n    ttl     time.Duration // 10 seconds\n}\n\ntype cacheEntry struct {\n    config    domain.ConfigSnapshot\n    expiresAt time.Time\n}\n\nfunc (c *ConfigCache) Get(sessionUUID uuid.UUID) (*ConfigSnapshot, bool) {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[sessionUUID]\n    if !ok || time.Now().After(entry.expiresAt) {\n        return nil, false // Cache miss\n    }\n    return \u0026entry.config, true\n}\n```\n\n**Cache Strategy:**\n- TTL: 10 seconds (acceptable staleness)\n- Invalidation: TTL-based (defer pub/sub until proven need)\n- Memory: ~200 bytes per config × 1K sessions = 200 KB (negligible)\n\n**Impact:**\n- Before: 20K config reads/sec\n- After: 100 config reads/sec\n- **Reduction: 99.5%**\n\n**Deliverables:**\n- In-memory config cache with TTL\n- Integration with Engine.GetCurrentValue\n- Metrics: config_cache_hits, config_cache_misses\n\n**Effort:** 8 hours\n\n---\n\n### Phase 3: Redis Sentinel HA (Week 3-4) - ELIMINATE SPOF\n\n**Goal:** Automatic failover, zero-downtime Redis failures\n\n**Architecture:**\n```\n[App Instance 1] ──┐\n[App Instance 2] ──┼─→ [Sentinel 1] ─┐\n[App Instance N] ──┘   [Sentinel 2] ─┼─→ [Redis Master]\n                       [Sentinel 3] ─┘   [Redis Replica 1]\n                                          [Redis Replica 2]\n```\n\n**Sentinel Configuration:**\n- 3 Sentinel processes (quorum=2)\n- down-after-milliseconds: 10000 (10s failure detection)\n- failover-timeout: 30000 (30s max failover time)\n- parallel-syncs: 1 (safe replica promotion)\n\n**Application Changes:**\n```go\n// Before:\nclient := redis.NewClient(opts)\n\n// After:\nclient := redis.NewFailoverClient(\u0026redis.FailoverOptions{\n    MasterName:    \"chatpulse-master\",\n    SentinelAddrs: []string{\n        \"sentinel1:26379\",\n        \"sentinel2:26379\",\n        \"sentinel3:26379\",\n    },\n    Password:      cfg.RedisPassword,\n    DialTimeout:   5 * time.Second,\n    PoolSize:      50,\n    MinIdleConns:  10,\n})\n```\n\n**Failover Process:**\n1. Master crashes\n2. Sentinels detect failure (10s)\n3. Quorum votes on new master (quorum=2 of 3)\n4. Replica promoted to master (20s)\n5. Clients auto-reconnect to new master\n6. **Total downtime: 30-60 seconds**\n\n**Deployment:**\n- Kubernetes StatefulSets (recommended)\n- 3 Sentinel pods + 1 master + 2 replica pods\n- PersistentVolumes for Redis data\n\n**Deliverables:**\n- Sentinel cluster deployed\n- Application updated to use FailoverClient\n- Failover testing completed\n- Runbook documented\n\n**Effort:** 20 hours (deployment: 12h, testing: 8h)\n\n---\n\n### Phase 4: Read Replicas (Week 5-6) - SCALE READS\n\n**Goal:** 5x read capacity (from 200K to 1M ops/sec)\n\n**Read/Write Split:**\n- **Reads (95% of traffic):** GetSentiment, config reads\n- **Writes (5% of traffic):** ApplyVote, session lifecycle, ref counting\n\n**Implementation:**\n```go\ntype SplitRedisClient struct {\n    master   *redis.Client\n    replicas []*redis.Client\n    counter  uint64\n}\n\nfunc (s *SplitRedisClient) GetSentiment(ctx context.Context, sessionUUID uuid.UUID, ...) (float64, error) {\n    // Round-robin across replicas\n    idx := atomic.AddUint64(\u0026s.counter, 1) % uint64(len(s.replicas))\n    replica := s.replicas[idx]\n    \n    return replica.FCallRO(ctx, \"get_decayed_value\", ...)\n}\n\nfunc (s *SplitRedisClient) ApplyVote(ctx context.Context, sessionUUID uuid.UUID, ...) (float64, error) {\n    // Always write to master\n    return s.master.FCall(ctx, \"apply_vote\", ...)\n}\n```\n\n**Replication Lag:**\n- Typical: 50-200ms\n- Max acceptable: 1 second\n- **Impact:** Overlay shows sentiment value with slight delay (acceptable)\n\n**Capacity:**\n- 1 master: 100K write ops/sec\n- 2 replicas: 200K read ops/sec each = 400K read capacity\n- **Combined: 500K ops/sec (5x improvement)**\n\n**Deliverables:**\n- Read/write split client implementation\n- Replication lag monitoring\n- Load testing to verify 5x capacity\n\n**Effort:** 16 hours (implementation: 8h, testing: 8h)\n\n---\n\n## Timeline Summary\n\n| Phase | Duration | Deliverable | Impact |\n|-------|----------|-------------|--------|\n| 1 | Week 1 | Circuit breaker + pool | Resilience |\n| 2 | Week 2 | Config cache | 99% fewer calls |\n| 3 | Week 3-4 | Sentinel HA | Eliminate SPOF |\n| 4 | Week 5-6 | Read replicas | 5x capacity |\n\n**Total: 6 weeks, 56 developer-hours**\n\n---\n\n## Cost Analysis\n\n**Infrastructure (AWS example):**\n- Current: 1 Redis (r6g.large) = $150/month\n- After Phases 1-2: Same ($150/month)\n- After Phase 3: 3 Sentinels + 1 master + 2 replicas = $450/month\n- **Increase: $300/month**\n\n**Engineering:**\n- 56 developer-hours (~1.5 developer-weeks)\n\n**ROI:**\n- Prevents outages (SPOF eliminated)\n- Enables horizontal scaling (10x capacity)\n- Reduces Redis load (99% fewer calls)\n- Faster incident response (circuit breaker)\n\n**Verdict:** Essential investment for production scale\n\n---\n\n## Success Metrics\n\n**Phase 1:**\n- ✅ Redis failure doesn't crash application\n- ✅ Circuit breaker opens/closes automatically\n- ✅ Graceful degradation (neutral value served)\n\n**Phase 2:**\n- ✅ Redis config reads reduced by 99%+\n- ✅ Cache hit rate \u003e 95%\n- ✅ Memory overhead \u003c 500 KB\n\n**Phase 3:**\n- ✅ Master failure auto-recovers in \u003c 60s\n- ✅ Zero manual intervention\n- ✅ No data loss (replication)\n\n**Phase 4:**\n- ✅ Read capacity 5x (500K ops/sec)\n- ✅ Replication lag \u003c 200ms (p99)\n- ✅ Overlay staleness \u003c 1s\n\n---\n\n## Risk Mitigation\n\n**Risk 1: Circuit breaker false positives**\n- Mitigation: 60% failure threshold (tolerates transient blips)\n- Monitoring: Alert on circuit state changes\n\n**Risk 2: Replication lag causes user confusion**\n- Mitigation: Monitor lag, alert if \u003e 500ms\n- Fallback: Route to master if lag too high\n\n**Risk 3: Sentinel split-brain**\n- Mitigation: 3 Sentinels (quorum=2), proper network setup\n- Documentation: Split-brain recovery runbook\n\n**Risk 4: Config cache staleness**\n- Mitigation: 10s TTL (acceptable delay)\n- Future: Add pub/sub invalidation if needed\n\n---\n\n## Implementation Order\n\n**Recommended:**\n1. ✅ Phase 1 (circuit breaker) - prevents cascades\n2. ✅ Phase 2 (config cache) - quick win, reduces load\n3. ✅ Phase 3 (Sentinel) - eliminates SPOF\n4. ✅ Phase 4 (replicas) - scales reads\n\n**Can parallelize:**\n- Phase 1 + Phase 2 (independent)\n- Phase 3 + Phase 4 (replicas deploy with Sentinel)\n\n---\n\n## Open Questions - NEED TEAM INPUT\n\n**Q1: Cost approval?**\n- $300/month increase for Sentinel infrastructure\n- Team lead approval needed\n\n**Q2: Deployment platform?**\n- Kubernetes StatefulSets (recommended) or VMs?\n- What's the target environment?\n\n**Q3: Monitoring/alerting?**\n- Should we implement Phase 1-2 before observability?\n- Or wait for Prometheus metrics (twitch-tow-3bx)?\n\n**Q4: Rollback plan?**\n- If Sentinel causes issues, can we quickly revert?\n- Need staged rollout (canary → 50% → 100%)?\n\n---\n\n## Decision\n\n**Architect votes:**\n- Architect-Resilience: +1 (all phases)\n- Architect-Scalability: +1 (on twitch-tow-ojd)\n- Architect-Maintainability: +1 (on twitch-tow-hdl)\n\n**Status: APPROVED by architecture team (3/3 votes)**\n\n**Next step:** Await team lead approval for cost and timeline, then create implementation task beads.","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:16:13.377657+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:56.759056+01:00","closed_at":"2026-02-12T17:56:56.759056+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-c8q","title":"Discussion: Missing observability and health check endpoints","description":"## Issue\nNo health check endpoint, no metrics export, and limited structured logging make it difficult to monitor service health and diagnose production issues.\n\n## Current State\n- No /health or /ready endpoint for load balancer health checks\n- No metrics export (Prometheus, StatsD, etc.)\n- Structured logging (slog) is present but inconsistent usage of context loggers\n- No request ID propagation for distributed tracing\n- No correlation between logs (no trace IDs)\n- Error logs lack actionable context (stack traces, request details)\n\n## Missing Observability\n**Metrics needed:**\n- Redis: connection pool usage, operation latency, error rate by operation\n- Database: connection pool usage, query latency, transaction duration\n- Broadcaster: active sessions, connected clients, broadcast latency, slow client evictions\n- WebSocket: connection duration, disconnect reasons, message queue depth\n- Twitch API: request latency, rate limit hits, subscription state\n- HTTP: request rate, response latency, error rate by endpoint\n\n**Health checks needed:**\n- Liveness: process is running (always 200 OK)\n- Readiness: DB pingable, Redis pingable, EventSub configured (if required)\n- Startup probe: migrations complete, Lua functions loaded\n\n## Risks\n- **Incident response**: No visibility into system health during outage\n- **Capacity planning**: Cannot measure actual resource usage vs limits\n- **SLO tracking**: Cannot measure availability or latency percentiles\n- **Root cause analysis**: Insufficient context in logs to diagnose failures\n- **Load balancing**: No health checks could route traffic to unhealthy instances\n\n## Suggestions\n1. Add /health endpoint (liveness), /ready endpoint (readiness)\n2. Instrument with Prometheus metrics (github.com/prometheus/client_golang)\n3. Add structured logging with request IDs (middleware to inject correlation ID)\n4. Export traces to OpenTelemetry collector (optional, for detailed traces)\n5. Add logging helpers for consistent context (session_uuid, user_id, error)\n6. Include stack traces in error logs (use errors.Wrap with %+v format)\n7. Document runbook entries for common failure scenarios\n\n## Files\n- All components lack metrics export\n- internal/logging/logger.go (has helpers but inconsistently used)\n- No health check handlers in internal/server/handlers.go","notes":"RESOLVED: Observability and health check concerns are comprehensively addressed.\n\n**Implemented in:**\n- **twitch-tow-php** (CONSOLIDATED: Comprehensive Observability)\n- **twitch-tow-682** (Epic: Phase 1 Health Checks)\n- **twitch-tow-kgj** (Epic: Phase 2 Prometheus Metrics)\n\n**How the solution addresses each concern:**\n\n**Health checks needed** ✅\n- **Liveness:** GET /health/live (always 200 OK)\n- **Readiness:** GET /health/ready (checks DB, Redis, Redis Functions, optional EventSub)\n- **Version:** GET /version (git SHA, build time, Go version)\n- Response: 200 OK when healthy, 503 with failed_check when unhealthy\n\n**Metrics needed** ✅\nAll requested metrics from this discussion are implemented:\n\n**Redis:** ✅\n- redis_operations_total{operation, status}\n- redis_operation_duration_seconds{operation}\n- redis_connection_errors_total\n- Uses Redis hooks for automatic instrumentation\n\n**Database:** ✅\n- db_query_duration_seconds{query}\n- db_connections_current{state}\n- db_errors_total{query}\n- Uses pgx query tracer\n\n**Broadcaster:** ✅\n- broadcaster_active_sessions\n- broadcaster_connected_clients_total\n- broadcaster_tick_duration_seconds\n- broadcaster_slow_clients_evicted_total\n\n**WebSocket:** ✅\n- websocket_connections_current\n- websocket_connections_total{result}\n- websocket_message_send_duration_seconds\n- websocket_connection_duration_seconds\n\n**Twitch API:** ⚠️\n- Not explicitly instrumented (low priority)\n- EventSub state tracked in database\n- Can add metrics later if needed\n\n**HTTP:** ✅\n- http_requests_total{method, path, status}\n- http_request_duration_seconds{method, path}\n- Via Echo Prometheus middleware\n\n**Structured logging improvements** ✅\n- Request ID propagation (middleware)\n- Consistent field names (session_uuid, user_id, error, duration_ms)\n- Context logger injection\n\n**Not implemented:**\n- OpenTelemetry distributed tracing (deferred to Phase 4, not needed for monolith)\n- Stack traces in all error logs (excessive verbosity)\n\nAll critical observability gaps identified in this discussion are resolved. The /metrics endpoint, Grafana dashboards, and alert rule examples are included in the consolidated solution documentation.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:34.34371+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:35.938965+01:00","closed_at":"2026-02-12T17:37:35.938978+01:00"}
{"id":"twitch-tow-d80","title":"EPIC: WebSocket Connection Lifecycle Management - Idle Timeouts and Graceful Shutdown","description":"**User Story:** As an operator, I need WebSocket connections to have proper lifecycle management (idle timeouts, max age, graceful shutdown) so that zombie connections don't cause resource exhaustion.\n\n**Problem Context:** Current WebSocket implementation lacks:\n- Idle connection timeout (zombie connections stay forever)\n- Maximum connection age (24-hour-old connections accumulate)\n- Graceful shutdown (connections left in TIME_WAIT on crash)\n- Instance-level connection cap (only per-session cap exists: 50)\n- Connection lifecycle metrics\n\n**Risks:**\n- Memory exhaustion (activeClients map grows unbounded)\n- FD exhaustion (OS limits: 1024-65536 open connections)\n- Broadcast latency (more connections = slower fanout)\n\n**Solution Overview:** Add idle timeouts, max connection age, graceful shutdown with draining, instance-level connection cap, and lifecycle metrics.\n\n## Task Breakdown\n\n### 1. Add Idle Connection Timeout\n\n**File:** `internal/broadcast/writer.go`\n\n**Add to clientWriter struct:**\n```go\ntype clientWriter struct {\n    conn           *websocket.Conn\n    send           chan []byte\n    heartbeat      *time.Ticker\n    idleTimeout    time.Duration  // NEW: 5 minutes\n    maxAge         time.Duration  // NEW: 24 hours\n    connectedAt    time.Time      // NEW: connection start time\n    lastActivityAt time.Time      // NEW: last pong received\n    clock          clockwork.Clock\n}\n```\n\n**Update run() to enforce idle timeout:**\n```go\nfunc (w *clientWriter) run() {\n    pongWait := 60 * time.Second\n    w.conn.SetReadDeadline(w.clock.Now().Add(pongWait))\n    w.conn.SetPongHandler(func(string) error {\n        w.lastActivityAt = w.clock.Now()  // Track activity\n        w.conn.SetReadDeadline(w.clock.Now().Add(pongWait))\n        return nil\n    })\n    \n    // Idle timeout ticker\n    idleCheck := time.NewTicker(30 * time.Second)\n    defer idleCheck.Stop()\n    \n    for {\n        select {\n        case message, ok := w.send:\n            // ... existing write logic\n        \n        case \u003c-w.heartbeat.C:\n            // ... existing ping logic\n        \n        case \u003c-idleCheck.C:\n            // Check idle timeout (no pongs received for 5 minutes)\n            if w.clock.Since(w.lastActivityAt) \u003e w.idleTimeout {\n                websocketIdleTimeoutsTotal.Inc()\n                return  // Exit goroutine, close connection\n            }\n            \n            // Check max connection age (24 hours)\n            if w.clock.Since(w.connectedAt) \u003e w.maxAge {\n                websocketMaxAgeTimeoutsTotal.Inc()\n                return  // Exit goroutine, close connection\n            }\n        }\n    }\n}\n```\n\n### 2. Add Instance-Level Connection Cap\n\n**File:** `internal/broadcast/broadcaster.go`\n\n**Add global connection counter:**\n```go\nconst (\n    maxClientsPerSession = 50   // Existing\n    maxClientsTotal      = 5000 // NEW: instance-level cap\n)\n\ntype Broadcaster struct {\n    // ... existing fields\n    totalClients atomic.Int32  // NEW: instance-level count\n}\n\nfunc (b *Broadcaster) Register(sessionUUID uuid.UUID, conn *websocket.Conn) error {\n    // Check instance-level cap first\n    if b.totalClients.Load() \u003e= maxClientsTotal {\n        websocketConnectionsRejectedTotal.WithLabelValues(\"instance_cap\").Inc()\n        return fmt.Errorf(\"instance connection limit reached: %d\", maxClientsTotal)\n    }\n    \n    // ... existing session-level cap check\n    \n    // Increment counters\n    b.totalClients.Add(1)\n    \n    reply := make(chan error)\n    b.commands \u003c- \u0026registerClientCmd{...}\n    return \u003c-reply\n}\n\nfunc (b *Broadcaster) Unregister(sessionUUID uuid.UUID, conn *websocket.Conn) {\n    b.totalClients.Add(-1)  // Decrement counter\n    b.commands \u003c- \u0026unregisterClientCmd{...}\n}\n```\n\n### 3. Graceful Shutdown with Connection Draining\n\n**File:** `internal/broadcast/broadcaster.go`\n\n**Add shutdown with close frame:**\n```go\nfunc (b *Broadcaster) Stop() error {\n    // Send stop command\n    reply := make(chan error)\n    b.commands \u003c- \u0026stopCmd{reply: reply}\n    err := \u003c-reply\n    \n    // Wait for goroutine to finish (with timeout)\n    select {\n    case \u003c-b.done:\n        return err\n    case \u003c-time.After(5 * time.Second):\n        return fmt.Errorf(\"broadcaster stop timed out after 5s\")\n    }\n}\n\n// In run() goroutine, handle stopCmd:\ncase cmd := \u003c-b.commands:\n    if stopCmd, ok := cmd.(*stopCmd); ok {\n        // Send close frame to all clients\n        for sessionUUID, clients := range b.activeClients {\n            for client := range clients {\n                // Send WebSocket close frame (graceful)\n                err := client.conn.WriteControl(\n                    websocket.CloseMessage,\n                    websocket.FormatCloseMessage(websocket.CloseGoingAway, \"server shutting down\"),\n                    time.Now().Add(5*time.Second),\n                )\n                if err != nil {\n                    b.logger.Warn(\"failed to send close frame\", \"error\", err)\n                }\n                client.stop()\n            }\n        }\n        \n        stopCmd.reply \u003c- nil\n        close(b.done)\n        return\n    }\n```\n\n### 4. Add Timeout to clientWriter.stop()\n\n**File:** `internal/broadcast/writer.go`\n\n**Add timeout to stop():**\n```go\nfunc (w *clientWriter) stop() {\n    close(w.send)  // Signal goroutine to exit\n    \n    // Wait for goroutine with timeout\n    done := make(chan struct{})\n    go func() {\n        // Wait for run() to exit (closes done on defer)\n        \u003c-done\n    }()\n    \n    select {\n    case \u003c-done:\n        // Graceful exit\n    case \u003c-time.After(5 * time.Second):\n        // Goroutine didn't exit, log warning\n        websocketStopTimeoutsTotal.Inc()\n    }\n}\n```\n\n### 5. Add Connection Lifecycle Metrics\n\n**File:** `internal/broadcast/broadcaster.go` (add Prometheus metrics)\n\n```go\nvar (\n    websocketConnectionsActive = prometheus.NewGaugeVec(\n        prometheus.GaugeOpts{\n            Name: \"websocket_connections_active\",\n            Help: \"Current number of active WebSocket connections\",\n        },\n        []string{\"session_uuid\"},  // Per-session tracking\n    )\n    \n    websocketConnectionsRejectedTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"websocket_connections_rejected_total\",\n            Help: \"Total number of rejected WebSocket connections\",\n        },\n        []string{\"reason\"},  // \"session_cap\", \"instance_cap\"\n    )\n    \n    websocketConnectionDurationSeconds = prometheus.NewHistogram(\n        prometheus.HistogramOpts{\n            Name: \"websocket_connection_duration_seconds\",\n            Help: \"WebSocket connection duration\",\n            Buckets: []float64{1, 10, 60, 300, 1800, 3600, 14400, 86400},\n        })\n    \n    websocketIdleTimeoutsTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"websocket_idle_timeouts_total\",\n            Help: \"Total number of idle connection timeouts (no pongs for 5 minutes)\",\n        })\n    \n    websocketMaxAgeTimeoutsTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"websocket_max_age_timeouts_total\",\n            Help: \"Total number of max age timeouts (24 hours old)\",\n        })\n    \n    websocketStopTimeoutsTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"websocket_stop_timeouts_total\",\n            Help: \"Total number of clientWriter.stop() timeouts (goroutine didn't exit)\",\n        })\n)\n```\n\n### 6. Add Connection Lifecycle Logging\n\n**File:** `internal/server/handlers_overlay.go`\n\n**Enhanced logging:**\n```go\nfunc (s *Server) handleOverlayWebSocket(c echo.Context) error {\n    // ... session setup\n    \n    // Upgrade to WebSocket\n    conn, err := upgrader.Upgrade(c.Response(), c.Request(), nil)\n    if err != nil {\n        s.logger.Error(\"websocket upgrade failed\",\n            \"session_uuid\", overlayUUID,\n            \"remote_addr\", c.RealIP(),\n            \"error\", err)\n        return err\n    }\n    \n    connectedAt := time.Now()\n    s.logger.Info(\"websocket connected\",\n        \"session_uuid\", overlayUUID,\n        \"remote_addr\", c.RealIP())\n    \n    // ... register client\n    \n    // Read pump blocks\n    _, _, err = conn.ReadMessage()\n    \n    // Unregister on disconnect\n    s.broadcaster.Unregister(overlayUUID, conn)\n    conn.Close()\n    \n    duration := time.Since(connectedAt)\n    websocketConnectionDurationSeconds.Observe(duration.Seconds())\n    \n    s.logger.Info(\"websocket disconnected\",\n        \"session_uuid\", overlayUUID,\n        \"duration_seconds\", duration.Seconds(),\n        \"error\", err)\n    \n    return nil\n}\n```\n\n### 7. Testing\n\n**File:** `internal/broadcast/broadcaster_lifecycle_test.go` (NEW)\n\n**Test cases:**\n```go\n// TestClientWriter_IdleTimeout verifies 5-minute idle disconnect\nfunc TestClientWriter_IdleTimeout(t *testing.T) {\n    fakeClock := clockwork.NewFakeClock()\n    // Setup: clientWriter with 5-minute idle timeout\n    // Act: Advance clock 6 minutes (no pongs)\n    // Assert: clientWriter exits, conn closed\n}\n\n// TestClientWriter_MaxAge verifies 24-hour max age\nfunc TestClientWriter_MaxAge(t *testing.T) {\n    fakeClock := clockwork.NewFakeClock()\n    // Setup: clientWriter connected at T=0\n    // Act: Advance clock 25 hours\n    // Assert: clientWriter exits, conn closed\n}\n\n// TestBroadcaster_InstanceCap verifies 5000 connection limit\nfunc TestBroadcaster_InstanceCap(t *testing.T) {\n    // Setup: Broadcaster with 4999 connected clients\n    // Act: Register 5000th client (success), 5001st client (reject)\n    // Assert: 5001st returns error, metric incremented\n}\n\n// TestBroadcaster_GracefulShutdown verifies close frames sent\nfunc TestBroadcaster_GracefulShutdown(t *testing.T) {\n    // Setup: 10 connected clients\n    // Act: broadcaster.Stop()\n    // Assert: All clients receive close frame, connections closed\n}\n```\n\n### 8. Documentation\n\n**Update CLAUDE.md:**\n\nAdd section under \"Broadcaster\":\n```markdown\n### WebSocket Connection Lifecycle\n\n**Idle timeout:** 5 minutes (no pongs → disconnect)\n**Max age:** 24 hours (force reconnect for long-lived connections)\n**Instance cap:** 5000 total connections (prevent FD exhaustion)\n**Session cap:** 50 connections per session (existing)\n\n**Graceful shutdown:**\n- Send WebSocket close frame (1001 Going Away)\n- Wait up to 5s for clientWriter goroutines to exit\n- Timeout logged as websocket_stop_timeouts_total\n\n**Metrics:**\n- websocket_connections_active{session_uuid} (gauge)\n- websocket_connections_rejected_total{reason} (counter)\n- websocket_connection_duration_seconds (histogram)\n- websocket_idle_timeouts_total (counter)\n- websocket_max_age_timeouts_total (counter)\n- websocket_stop_timeouts_total (counter)\n```\n\n## Acceptance Criteria\n\n✅ Idle connections (no pongs for 5 minutes) disconnected automatically\n✅ 24-hour-old connections force-reconnected\n✅ Instance-level cap (5000) prevents accepting new connections when full\n✅ Graceful shutdown sends close frames to all clients\n✅ clientWriter.stop() times out after 5s (doesn't wait forever)\n✅ Metrics track connection lifecycle (active, rejected, duration, timeouts)\n✅ Logs show connection events (connect, disconnect, duration, reason)\n✅ Tests verify idle timeout, max age, instance cap, graceful shutdown\n✅ CLAUDE.md documents connection lifecycle policies\n\n## Dependencies\n\n- Synergy with: Observability epic (metrics)\n- Synergy with: Global connection limits epic (instance cap)\n\n## Files Modified\n\n**Modified:**\n- internal/broadcast/writer.go (idle timeout, max age, stop timeout)\n- internal/broadcast/broadcaster.go (instance cap, graceful shutdown, metrics)\n- internal/server/handlers_overlay.go (lifecycle logging)\n- CLAUDE.md (document lifecycle policies)\n\n**New:**\n- internal/broadcast/broadcaster_lifecycle_test.go (lifecycle tests)\n\n## Estimated Effort\n\n**Implementation:** 3 developer-days\n- Idle timeout + max age: 4 hours\n- Instance cap: 2 hours\n- Graceful shutdown: 4 hours\n- Stop timeout: 1 hour\n- Metrics: 3 hours\n- Logging: 1 hour\n- Testing: 1 day (requires fake clock coordination)\n- Documentation: 2 hours\n\n**Total:** 3 developer-days\n\n## Rollout Strategy\n\n1. Deploy with metrics only (observe baseline connection lifetimes)\n2. Enable idle timeout (5 minutes) in staging\n3. Monitor idle timeout rate (should be \u003c5% of connections)\n4. Enable max age (24 hours) in production\n5. Monitor forced reconnection rate\n6. Tune thresholds if needed (e.g., increase idle timeout to 10 minutes)","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:37:24.482844+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:46.28531+01:00"}
{"id":"twitch-tow-dmg","title":"EPIC: Global WebSocket Connection Limits - Prevent Resource Exhaustion","description":"## Epic Overview\nImplement multi-layer WebSocket connection limits to prevent DoS attacks and resource exhaustion: global instance limit, per-IP limit, and connection rate limiting.\n\n## User Story\nAs an operator, I need connection limits to prevent attackers from exhausting server resources with connection floods, ensuring legitimate users can always connect.\n\n## Parent Solution\ntwitch-tow-tt0 (Multi-layer Resource Limits and Rate Limiting)\n\n## Problem Analysis\n\n**Attack scenarios:**\n1. **Connection exhaustion:** Botnet opens 10K connections, exhausts file descriptors\n2. **Single-source flood:** One IP opens 1K connections\n3. **Connection spam:** Rapid connection/disconnect cycles\n\n**Defense layers:**\n- **Layer 1:** Global connection limit (10K per instance)\n- **Layer 2:** Per-IP connection limit (100 per IP)\n- **Layer 3:** Connection rate limiting (10/sec per IP)\n\n## Technical Requirements\n\n### Layer 1: Global Connection Limit\n- Max concurrent connections per instance: 10,000 (configurable)\n- Atomic counter (lock-free)\n- Reject with 503 when at capacity\n- Log warning at 80% capacity\n\n### Layer 2: Per-IP Connection Limit\n- Max concurrent connections per IP: 100 (configurable)\n- Protect against single-source attacks\n- Account for NAT/proxy scenarios\n\n### Layer 3: Connection Rate Limiting\n- Max new connections per IP: 10/sec (configurable)\n- Prevent rapid connection spam\n- Use golang.org/x/time/rate\n\n### File Descriptor Check\n- Verify ulimit on startup\n- Warn if \u003c20,000 (need buffer above max connections)\n\n## Implementation Tasks\n\n### Task 1: Create connection limiter\n**File:** `internal/server/connection_limiter.go` (NEW)\n```go\npackage server\n\nimport (\n    \"sync\"\n    \"sync/atomic\"\n    \"time\"\n    \n    \"golang.org/x/time/rate\"\n)\n\n// GlobalConnectionLimiter limits total concurrent connections\ntype GlobalConnectionLimiter struct {\n    current atomic.Int64\n    max     int64\n}\n\nfunc NewGlobalConnectionLimiter(max int64) *GlobalConnectionLimiter {\n    return \u0026GlobalConnectionLimiter{max: max}\n}\n\nfunc (l *GlobalConnectionLimiter) Acquire() bool {\n    for {\n        current := l.current.Load()\n        if current \u003e= l.max {\n            metrics.WebSocketConnectionsRejected.WithLabelValues(\"global_limit\").Inc()\n            return false\n        }\n        if l.current.CompareAndSwap(current, current+1) {\n            return true\n        }\n    }\n}\n\nfunc (l *GlobalConnectionLimiter) Release() {\n    l.current.Add(-1)\n}\n\nfunc (l *GlobalConnectionLimiter) Current() int64 {\n    return l.current.Load()\n}\n\nfunc (l *GlobalConnectionLimiter) CapacityPct() float64 {\n    return float64(l.Current()) / float64(l.max) * 100\n}\n\n// IPConnectionLimiter limits connections per IP\ntype IPConnectionLimiter struct {\n    mu     sync.RWMutex\n    ips    map[string]int\n    maxPer int\n}\n\nfunc NewIPConnectionLimiter(maxPer int) *IPConnectionLimiter {\n    return \u0026IPConnectionLimiter{\n        ips:    make(map[string]int),\n        maxPer: maxPer,\n    }\n}\n\nfunc (l *IPConnectionLimiter) Acquire(ip string) bool {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    if l.ips[ip] \u003e= l.maxPer {\n        metrics.WebSocketConnectionsRejected.WithLabelValues(\"ip_limit\").Inc()\n        return false\n    }\n    l.ips[ip]++\n    return true\n}\n\nfunc (l *IPConnectionLimiter) Release(ip string) {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    if count := l.ips[ip]; count \u003e 0 {\n        l.ips[ip] = count - 1\n        if l.ips[ip] == 0 {\n            delete(l.ips, ip)\n        }\n    }\n}\n\nfunc (l *IPConnectionLimiter) UniqueIPs() int {\n    l.mu.RLock()\n    defer l.mu.RUnlock()\n    return len(l.ips)\n}\n\n// ConnectionRateLimiter limits new connection rate per IP\ntype ConnectionRateLimiter struct {\n    mu       sync.Mutex\n    limiters map[string]*rate.Limiter\n    rate     rate.Limit\n    burst    int\n}\n\nfunc NewConnectionRateLimiter(perSec int, burst int) *ConnectionRateLimiter {\n    return \u0026ConnectionRateLimiter{\n        limiters: make(map[string]*rate.Limiter),\n        rate:     rate.Limit(perSec),\n        burst:    burst,\n    }\n}\n\nfunc (l *ConnectionRateLimiter) Allow(ip string) bool {\n    l.mu.Lock()\n    limiter, exists := l.limiters[ip]\n    if !exists {\n        limiter = rate.NewLimiter(l.rate, l.burst)\n        l.limiters[ip] = limiter\n    }\n    l.mu.Unlock()\n    \n    if !limiter.Allow() {\n        metrics.WebSocketConnectionsRejected.WithLabelValues(\"rate_limit\").Inc()\n        return false\n    }\n    return true\n}\n\n// Periodic cleanup to prevent memory leak\nfunc (l *ConnectionRateLimiter) CleanupStale(maxAge time.Duration) int {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    removed := 0\n    now := time.Now()\n    \n    for ip, limiter := range l.limiters {\n        // Remove limiters that haven't been used recently\n        // Check if all tokens are available (no recent activity)\n        if limiter.Tokens() == float64(l.burst) {\n            delete(l.limiters, ip)\n            removed++\n        }\n    }\n    \n    return removed\n}\n```\n\n### Task 2: Integrate limiters into WebSocket handler\n**File:** `internal/server/handlers_overlay.go`\n```go\nfunc (s *Server) handleWebSocket(c echo.Context) error {\n    // Layer 3: Connection rate limit\n    ip := c.RealIP()\n    if !s.connRateLimiter.Allow(ip) {\n        slog.Warn(\"Connection rate limit exceeded\", \"ip\", ip)\n        return c.String(429, \"Connection rate limit exceeded. Please retry in a few seconds.\")\n    }\n    \n    // Layer 1: Global connection limit\n    if !s.globalConnLimiter.Acquire() {\n        slog.Warn(\"Global connection limit reached\", \"current\", s.globalConnLimiter.Current())\n        return c.String(503, \"Server at capacity. Please retry in a moment.\")\n    }\n    defer s.globalConnLimiter.Release()\n    \n    // Layer 2: Per-IP connection limit\n    if !s.ipConnLimiter.Acquire(ip) {\n        slog.Warn(\"Per-IP connection limit exceeded\", \"ip\", ip, \"limit\", s.maxConnectionsPerIP)\n        return c.String(429, \"Too many connections from your IP address.\")\n    }\n    defer s.ipConnLimiter.Release(ip)\n    \n    // ... existing WebSocket upgrade logic\n}\n```\n\n### Task 3: Add ulimit check on startup\n**File:** `cmd/server/main.go`\n```go\nimport \"syscall\"\n\nfunc checkFileDescriptorLimit() {\n    var rlimit syscall.Rlimit\n    if err := syscall.Getrlimit(syscall.RLIMIT_NOFILE, \u0026rlimit); err != nil {\n        slog.Warn(\"Failed to check file descriptor limit\", \"error\", err)\n        return\n    }\n    \n    recommended := uint64(20000)\n    if rlimit.Cur \u003c recommended {\n        slog.Warn(\"File descriptor limit may be too low for production\",\n            \"current\", rlimit.Cur,\n            \"recommended\", recommended,\n            \"max\", rlimit.Max,\n        )\n        slog.Warn(\"Increase with: ulimit -n 20000\")\n    } else {\n        slog.Info(\"File descriptor limit OK\", \"current\", rlimit.Cur)\n    }\n}\n\nfunc main() {\n    // ... config loading\n    \n    checkFileDescriptorLimit()\n    \n    // ... rest of setup\n}\n```\n\n### Task 4: Add configuration options\n**File:** `.env.example`\n```bash\n# WebSocket connection limits\nMAX_WEBSOCKET_CONNECTIONS=10000    # Global limit per instance\nMAX_CONNECTIONS_PER_IP=100         # Concurrent connections per IP\nCONNECTION_RATE_PER_IP=10          # New connections per second per IP\nCONNECTION_RATE_BURST=20           # Allow brief bursts\n```\n\n**File:** `internal/config/config.go`\n```go\ntype Config struct {\n    // ... existing fields\n    \n    MaxWebSocketConnections int `env:\"MAX_WEBSOCKET_CONNECTIONS\" default:\"10000\"`\n    MaxConnectionsPerIP     int `env:\"MAX_CONNECTIONS_PER_IP\" default:\"100\"`\n    ConnectionRatePerIP     int `env:\"CONNECTION_RATE_PER_IP\" default:\"10\"`\n    ConnectionRateBurst     int `env:\"CONNECTION_RATE_BURST\" default:\"20\"`\n}\n```\n\n### Task 5: Add metrics for capacity monitoring\n**File:** `internal/metrics/metrics.go`\n```go\nWebSocketConnectionsRejected = promauto.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: \"websocket_connections_rejected_total\",\n        Help: \"WebSocket connections rejected by reason\",\n    },\n    []string{\"reason\"}, // global_limit, ip_limit, rate_limit\n)\n\nWebSocketConnectionCapacity = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"websocket_connection_capacity_pct\",\n        Help: \"Current WebSocket connection usage as percentage of capacity\",\n    },\n)\n\nWebSocketUniqueIPs = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"websocket_unique_ips\",\n        Help: \"Number of unique IP addresses with active connections\",\n    },\n)\n```\n\nUpdate metrics periodically:\n```go\n// In Server.Start() or separate goroutine\nticker := time.NewTicker(10 * time.Second)\ngo func() {\n    for range ticker.C {\n        metrics.WebSocketConnectionCapacity.Set(s.globalConnLimiter.CapacityPct())\n        metrics.WebSocketUniqueIPs.Set(float64(s.ipConnLimiter.UniqueIPs()))\n    }\n}()\n```\n\n### Task 6: Unit tests for limiters\n**File:** `internal/server/connection_limiter_test.go` (NEW)\nTest scenarios:\n\n**GlobalConnectionLimiter:**\n1. Acquire succeeds when under limit\n2. Acquire fails when at limit\n3. Release decrements counter\n4. Concurrent acquires are safe (no over-allocation)\n\n**IPConnectionLimiter:**\n1. Acquire succeeds for different IPs\n2. Acquire fails when IP at limit\n3. Release allows new acquire\n4. Map cleanup when count reaches 0\n\n**ConnectionRateLimiter:**\n1. Allows initial burst\n2. Rate limits sustained requests\n3. Different IPs tracked independently\n4. CleanupStale removes inactive limiters\n\n```go\nfunc TestGlobalConnectionLimiter(t *testing.T) {\n    limiter := NewGlobalConnectionLimiter(2)\n    \n    // First two should succeed\n    assert.True(t, limiter.Acquire())\n    assert.True(t, limiter.Acquire())\n    assert.Equal(t, int64(2), limiter.Current())\n    \n    // Third should fail (at capacity)\n    assert.False(t, limiter.Acquire())\n    \n    // Release one\n    limiter.Release()\n    assert.Equal(t, int64(1), limiter.Current())\n    \n    // Should succeed now\n    assert.True(t, limiter.Acquire())\n}\n\nfunc TestGlobalConnectionLimiter_Concurrent(t *testing.T) {\n    limiter := NewGlobalConnectionLimiter(100)\n    \n    var wg sync.WaitGroup\n    successCount := atomic.Int32{}\n    \n    // Try to acquire 200 connections concurrently\n    for i := 0; i \u003c 200; i++ {\n        wg.Add(1)\n        go func() {\n            defer wg.Done()\n            if limiter.Acquire() {\n                successCount.Add(1)\n            }\n        }()\n    }\n    \n    wg.Wait()\n    \n    // Should have exactly 100 successes\n    assert.Equal(t, int32(100), successCount.Load())\n    assert.Equal(t, int64(100), limiter.Current())\n}\n```\n\n### Task 7: Integration test with real WebSocket connections\n**File:** `internal/server/connection_limits_integration_test.go` (NEW)\nTest scenarios:\n1. Connect up to global limit, verify next connection rejected with 503\n2. Connect 100 times from same IP, verify 101st rejected with 429\n3. Rapidly connect from same IP, verify rate limit kicks in\n4. Verify metrics are updated correctly\n\n```go\nfunc TestWebSocketGlobalLimit(t *testing.T) {\n    if testing.Short() {\n        t.Skip(\"integration test\")\n    }\n    \n    // Start server with low limit for testing\n    cfg := testConfig()\n    cfg.MaxWebSocketConnections = 5\n    srv := newTestServerWithConfig(t, cfg)\n    \n    // Open 5 connections (should succeed)\n    conns := make([]*websocket.Conn, 5)\n    for i := 0; i \u003c 5; i++ {\n        conn, _, err := websocket.DefaultDialer.Dial(\n            fmt.Sprintf(\"ws://%s/ws/overlay/%s\", srv.addr, testOverlayUUID),\n            nil,\n        )\n        require.NoError(t, err)\n        conns[i] = conn\n    }\n    defer func() {\n        for _, conn := range conns {\n            conn.Close()\n        }\n    }()\n    \n    // 6th connection should fail with 503\n    _, resp, err := websocket.DefaultDialer.Dial(\n        fmt.Sprintf(\"ws://%s/ws/overlay/%s\", srv.addr, testOverlayUUID),\n        nil,\n    )\n    require.Error(t, err)\n    require.NotNil(t, resp)\n    assert.Equal(t, 503, resp.StatusCode)\n}\n```\n\n### Task 8: Load testing script\n**File:** `scripts/load-test-connections.sh` (NEW)\n```bash\n#!/bin/bash\n# Load test WebSocket connection limits\n\nSERVER=\"localhost:8080\"\nOVERLAY_UUID=\"test-uuid-here\"\nCONNECTIONS=100\n\necho \"Testing connection limits on $SERVER\"\necho \"Opening $CONNECTIONS concurrent connections...\"\n\nfor i in $(seq 1 $CONNECTIONS); do\n    websocat \"ws://$SERVER/ws/overlay/$OVERLAY_UUID\" \u0026\n    PIDS[$i]=$!\ndone\n\necho \"All connections opened. Press Enter to close...\"\nread\n\necho \"Closing connections...\"\nfor pid in ${PIDS[@]}; do\n    kill $pid 2\u003e/dev/null\ndone\n\necho \"Done\"\n```\n\n### Task 9: Documentation\n**File:** `docs/operations/connection-limits.md` (NEW)\nDocument:\n- Connection limit architecture (3 layers)\n- Configuration parameters and tuning\n- Monitoring with metrics\n- Recommended alert rules\n- Troubleshooting guide\n- How to adjust ulimit in production\n\nExample alert:\n```yaml\n- alert: WebSocketCapacityHigh\n  expr: websocket_connection_capacity_pct \u003e 80\n  for: 5m\n  annotations:\n    summary: \"WebSocket connections at 80% capacity\"\n    description: \"Current: {{ $value }}%. Consider scaling.\"\n```\n\n### Task 10: Update CLAUDE.md\nAdd connection limits to ## Architecture Notes section\n\n## Acceptance Criteria\n\n- ✅ Global connection limit prevents \u003e10K connections per instance\n- ✅ Per-IP limit prevents \u003e100 connections from single IP\n- ✅ Connection rate limit prevents \u003e10 connections/sec per IP\n- ✅ Rejected connections receive clear error messages (503 or 429)\n- ✅ Metrics track rejections by reason\n- ✅ Ulimit check warns on startup if too low\n- ✅ Unit tests achieve 100% coverage with -race\n- ✅ Integration tests verify all three layers\n- ✅ Load test script included\n- ✅ Documentation covers tuning and monitoring\n\n## Files Created/Modified\n\n**New files:**\n- `internal/server/connection_limiter.go` (250 lines)\n- `internal/server/connection_limiter_test.go` (400 lines)\n- `internal/server/connection_limits_integration_test.go` (200 lines)\n- `scripts/load-test-connections.sh` (50 lines)\n- `docs/operations/connection-limits.md` (400 lines)\n\n**Modified files:**\n- `internal/server/handlers_overlay.go` (add limiter checks, 20 lines)\n- `cmd/server/main.go` (ulimit check, limiter initialization, 30 lines)\n- `internal/config/config.go` (add 4 config fields)\n- `.env.example` (document connection limit variables)\n- `internal/metrics/metrics.go` (add rejection + capacity metrics)\n- `CLAUDE.md` (document connection limits)\n\n## Testing Strategy\n\n**Unit tests:**\n- Test each limiter independently\n- Test concurrent access with -race\n- Test edge cases (exactly at limit, multiple releases)\n\n**Integration tests:**\n- Test with real WebSocket connections\n- Verify 503/429 responses\n- Test metrics are updated\n- Test all three layers independently\n\n**Load tests:**\n- Script to open N concurrent connections\n- Verify limits enforced\n- Measure overhead (\u003c1% CPU per 1K connections)\n\n**Manual tests:**\n- Deploy to staging\n- Run load test script\n- Monitor metrics\n- Verify alerts trigger at 80% capacity\n\n## Dependencies\n- **Package:** golang.org/x/time/rate (stdlib extension)\n- **Blocks:** None (independent feature)\n\n## Success Metrics\n- Zero resource exhaustion incidents\n- Graceful degradation under attack\n- Clear metrics for capacity planning\n- \u003c1% overhead for limiter checks\n\n## Effort Estimate\n**7 developer-days** (1.5 weeks)\n\nBreakdown:\n- Limiters implementation: 2 days\n- Integration: 1 day\n- Unit tests: 1.5 days\n- Integration + load tests: 1.5 days\n- Documentation: 1 day\n\n## Risk Mitigation\n- **Risk:** Legitimate users rejected due to shared IP (NAT)\n  - **Mitigation:** Per-IP limit set high (100 connections)\n  - **Mitigation:** Configurable, can increase for corporate NATs\n- **Risk:** Rate limiter memory leak (limiters never cleaned up)\n  - **Mitigation:** Periodic cleanup of inactive limiters\n  - **Mitigation:** Monitor memory usage\n- **Risk:** Limits too low, legitimate traffic rejected\n  - **Mitigation:** Conservative defaults (10K global, 100 per IP)\n  - **Mitigation:** Monitor rejection metrics, adjust as needed","status":"in_progress","priority":1,"issue_type":"epic","assignee":"dev-frontend","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:29:16.310442+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:07:31.243034+01:00"}
{"id":"twitch-tow-dmu","title":"EPIC: Write Tier 4-5 Implementation/Operations ADRs (6 ADRs)","description":"Write remaining ADRs covering implementation choices (actor pattern, DI, sqlc, time-decay algorithm) and operational decisions (deployment strategy, observability). Lower priority but completes documentation.\n\n## User Story\nAs a developer or operator, I need to understand implementation-level decisions and operational practices so I can maintain consistency and avoid regressions.\n\n## Value Proposition\n- Documents concurrency patterns (actor model)\n- Explains tooling choices (sqlc, tern, DI)\n- Clarifies deployment and zero-downtime strategy\n- Establishes observability baseline\n\n## Tasks\n\n### 1. Write ADR-011: Actor pattern for broadcaster concurrency\n\n**Context:**\n- Broadcaster manages WebSocket connections and 50ms tick loop\n- Need thread-safe access to `activeClients` map (session UUID → clients)\n- Traditional approach: mutex-protected shared state\n\n**Decision:**\n- Use **actor pattern** - single goroutine owns all mutable state\n- Buffered command channel (cap 256) for typed commands\n- `run()` goroutine with `select` on commands + ticker\n- No mutexes on actor-owned state (`activeClients` map)\n\n**Pattern details:**\n- Command types implement marker interface (`broadcasterCmd`)\n- Fire-and-forget commands (Register, Unregister)\n- Request/reply commands (GetClientCount - embeds reply channel)\n- `Stop()` method sends stop command, waits on done channel\n\n**Alternatives considered:**\n1. **Mutex-protected shared state** - Traditional Go concurrency\n   - Rejected: Harder to reason about (lock ordering, deadlocks)\n   - Rejected: More error-prone (forget to lock, lock too long)\n\n2. **Channel per session** - One actor per session\n   - Rejected: N goroutines for N sessions (resource overhead)\n   - Rejected: Tick loop coordination harder (who owns timer?)\n\n3. **Separate actors** - One for clients, one for tick loop\n   - Rejected: More complex (two actors communicate)\n   - Rejected: Doesn't reduce mutex burden (shared map)\n\n**Consequences:**\n\n✅ **Positive:**\n- Simple reasoning (serial execution, no race conditions)\n- No mutex bugs (deadlocks, forgot to unlock)\n- Easy testing (synchronous barrier calls)\n\n❌ **Negative:**\n- Single goroutine bottleneck (\u003e1000 sessions may struggle)\n- Serial command processing (Register waits for tick to finish)\n\n🔄 **Trade-offs:**\n- Chose simplicity over maximum throughput\n- Accept serial bottleneck for cleaner code\n- Prioritize correctness (no races) over optimization\n\n**Scaling limits:**\n- Tested up to 100 sessions (no issues)\n- Expected max: 500-1000 sessions per instance (50ms tick budget)\n- Beyond 1000: consider sharded broadcasters or separate instances\n\n**Related decisions:**\n- ADR-002: Pull-based broadcaster (consequence: tick loop in actor)\n\n**Files to create:**\n- `docs/adr/011-actor-pattern-broadcaster.md`\n\n**Time estimate:** 60 minutes\n\n### 2. Write ADR-012: Manual dependency injection\n\n**Context:**\n- Need to wire dependencies at startup (repos → services → handlers)\n- Traditional Go: manual constructor calls in `main.go`\n- Alternative: DI frameworks (wire, dig, fx)\n\n**Decision:**\n- Use **manual dependency injection** in `cmd/server/main.go`\n- Explicit constructor calls in dependency order\n- No DI framework, no service locator pattern\n\n**Example:**\n```go\n// main.go - manual wiring\npool := database.Connect(cfg.DatabaseURL)\nuserRepo := database.NewUserRepo(pool, cryptoSvc)\nconfigRepo := database.NewConfigRepo(pool)\nsessionRepo := redis.NewSessionRepo(rdb, clock)\nengine := sentiment.NewEngine(sessionRepo, sentimentStore, debouncer, clock)\nappSvc := app.NewService(userRepo, configRepo, sessionRepo, engine, twitchSvc, clock, logger)\nsrv := server.NewServer(appSvc, logger)\n```\n\n**Alternatives considered:**\n1. **google/wire** - Compile-time DI codegen\n   - Rejected: Generated code hard to debug\n   - Rejected: Extra build step (go generate)\n   - Rejected: Overkill for 10-15 dependencies\n\n2. **uber-go/dig** - Runtime DI with reflection\n   - Rejected: Reflection overhead (negligible but non-zero)\n   - Rejected: Magic (hard to trace where dependencies come from)\n   - Rejected: Harder to IDE navigate (reflection breaks \"Go to definition\")\n\n3. **uber-go/fx** - Opinionated DI + lifecycle\n   - Rejected: Heavy framework (changes app structure)\n   - Rejected: Magic lifecycle hooks (harder to reason about)\n\n**Consequences:**\n\n✅ **Positive:**\n- Explicit and visible (easy to trace)\n- Zero runtime overhead (no reflection)\n- IDE-friendly (Go to definition works)\n- Simple (no framework magic)\n\n❌ **Negative:**\n- Verbose (20-30 lines of constructor calls)\n- Boilerplate grows with dependencies\n- Manual ordering (must wire in correct order)\n\n🔄 **Trade-offs:**\n- Chose explicitness over brevity\n- Accept boilerplate for clarity\n- Prioritize debuggability over DI magic\n\n**Testing:**\n- Tests construct dependencies explicitly (same pattern)\n- Easy to mock (pass interface implementations)\n\n**Related decisions:**\n- None (internal implementation choice)\n\n**Files to create:**\n- `docs/adr/012-manual-dependency-injection.md`\n\n**Time estimate:** 60 minutes\n\n### 3. Write ADR-013: sqlc for SQL generation\n\n**Context:**\n- Need type-safe SQL queries for PostgreSQL\n- Traditional Go: raw SQL with `database/sql` + hand-written types\n- Alternative: ORMs (GORM, ent, sqlx)\n\n**Decision:**\n- Use **sqlc** to generate Go code from SQL files\n- Write SQL queries in `.sql` files (`internal/database/sqlc/queries/`)\n- sqlc generates type-safe Go methods (`internal/database/sqlcgen/`)\n- Repositories wrap sqlc-generated code with domain interfaces\n\n**Configuration (`sqlc.yaml`):**\n```yaml\nversion: \"2\"\nsql:\n  - engine: \"postgresql\"\n    queries: \"internal/database/sqlc/queries\"\n    schema: \"internal/database/sqlc/schemas\"\n    gen:\n      go:\n        package: \"sqlcgen\"\n        out: \"internal/database/sqlcgen\"\n```\n\n**Alternatives considered:**\n1. **ORM (GORM, ent)** - Object-relational mapping\n   - Rejected: N+1 query problems\n   - Rejected: Magic (hard to understand generated SQL)\n   - Rejected: Performance overhead (reflection)\n\n2. **Raw SQL + hand-written types** - No codegen\n   - Rejected: Type mismatches (forgot to update struct)\n   - Rejected: Verbose (scan each column manually)\n   - Rejected: No compile-time safety\n\n3. **sqlx** - Light wrapper around database/sql\n   - Rejected: Still need hand-written types\n   - Rejected: No compile-time safety (SQL typos at runtime)\n\n**Consequences:**\n\n✅ **Positive:**\n- Type safety (compile-time errors for mismatched queries)\n- SQL-first workflow (write SQL, not ORM DSL)\n- Zero runtime overhead (generated code)\n- Excellent performance (no reflection)\n\n❌ **Negative:**\n- Codegen step (must run `sqlc generate`)\n- Boilerplate (generated files committed to repo)\n- Limited to PostgreSQL (can't swap to MySQL easily)\n\n🔄 **Trade-offs:**\n- Chose type safety over zero-codegen\n- Accept codegen step for compile-time guarantees\n- Prioritize performance (no ORM) over portability\n\n**Migration strategy:**\n- tern for migrations (`internal/database/sqlc/schemas/*.sql`)\n- sqlc uses same schema files (dual-purpose)\n\n**Related decisions:**\n- None (internal tooling choice)\n\n**Files to create:**\n- `docs/adr/013-sqlc-for-sql-generation.md`\n\n**Time estimate:** 60 minutes\n\n### 4. Write ADR-014: Time-decay sentiment algorithm\n\n**Context:**\n- Sentiment bar should decay toward neutral over time (not stay at extremes)\n- Need configurable decay rate per streamer\n- Computation must be atomic (vote + decay in single operation)\n\n**Decision:**\n- Use **exponential decay** in Redis Lua function\n- Formula: `decayed_value = current_value * e^(-decay_rate * time_elapsed)`\n- Decay rate configurable per streamer (0.1 to 2.0 per second)\n- Computed in `get_decayed_value` Redis Function (read-only)\n\n**Implementation:**\n```lua\n-- chatpulse.lua (Redis Function Library)\nlocal function get_decayed_value(keys, args)\n    local session_key = keys[1]\n    local current_time = tonumber(args[1])\n    \n    local value = tonumber(redis.call('HGET', session_key, 'value') or 0)\n    local last_update = tonumber(redis.call('HGET', session_key, 'last_update') or current_time)\n    local config = cjson.decode(redis.call('HGET', session_key, 'config_json'))\n    \n    local elapsed = current_time - last_update\n    local decay_factor = math.exp(-config.decay_per_second * elapsed)\n    \n    return value * decay_factor\nend\n```\n\n**Alternatives considered:**\n1. **Linear decay** - Subtract constant per second\n   - Rejected: Reaches zero (should approach neutral)\n   - Rejected: Discontinuous (sudden jumps at zero)\n\n2. **Step decay** - Decay in discrete steps (every 10s)\n   - Rejected: Visible jumps (not smooth)\n   - Rejected: Harder to configure (step size + interval)\n\n3. **No decay** - Sentiment stays until reset\n   - Rejected: Bar stays at extremes (boring for viewers)\n   - Rejected: Manual reset required (bad UX)\n\n4. **Client-side calculation** - Compute decay in browser\n   - Rejected: Clock sync issues (client time ≠ server time)\n   - Rejected: Inconsistent across clients (different clock skew)\n\n**Consequences:**\n\n✅ **Positive:**\n- Smooth decay (continuous, not stepwise)\n- Approaches neutral asymptotically (never jumps to zero)\n- Configurable per streamer (fast vs slow decay)\n- Atomic computation (Lua function, no race conditions)\n\n❌ **Negative:**\n- Requires clock sync (Redis clock = app clock)\n- Lua testing complexity (need Redis to test)\n- Exponential math (harder to understand than linear)\n\n🔄 **Trade-offs:**\n- Chose smooth decay over simple linear\n- Accept clock sync requirement for correctness\n- Prioritize configurability over fixed rate\n\n**Clock sync:**\n- All instances must use same time source (NTP)\n- Redis `TIME` command could be used (1 extra roundtrip)\n- Current: app passes `time.Now().Unix()` (assumes sync)\n\n**Related decisions:**\n- ADR-001: Redis Functions (consequence: decay in Lua)\n\n**Files to create:**\n- `docs/adr/014-time-decay-algorithm.md`\n\n**Time estimate:** 60 minutes\n\n### 5. Write ADR-015: Deployment strategy and zero-downtime deploys\n\n**Context:**\n- Need to deploy new versions without downtime\n- Kubernetes rolling updates (new pods start, old pods stop)\n- Migrations must be backward-compatible (old code + new schema)\n\n**Decision:**\n- Use **rolling deployment** (Kubernetes default)\n- tern migrations run at startup (`database.Connect()` calls `tern migrate`)\n- `/ready` health check endpoint for load balancer\n- Pre-stop hook: wait 10s for connections to drain\n\n**Zero-downtime checklist:**\n1. **Backward-compatible migrations** - Expand/contract pattern\n   - Phase 1: Add new column (old code ignores it)\n   - Phase 2: Deploy new code (uses new column)\n   - Phase 3: Remove old column (separate migration)\n\n2. **Redis Lua function versioning** - Lua library loaded at startup\n   - Old pods use old functions (still loaded in Redis)\n   - New pods load new functions (overwrites library)\n   - Atomic cutover when last old pod stops\n\n3. **Health checks** - `/ready` returns 200 when ready\n   - PostgreSQL connection healthy (ping succeeds)\n   - Redis connection healthy (ping succeeds)\n   - Migrations complete (tern up-to-date)\n\n4. **Graceful shutdown** - SIGTERM handler\n   - Stop accepting new connections\n   - Wait 10s for in-flight requests (pre-stop hook)\n   - Close WebSocket connections cleanly\n   - Cleanup broadcaster, app service\n\n**Alternatives considered:**\n1. **Blue/green deployment** - Two environments, swap traffic\n   - Rejected: 2x infrastructure cost\n   - Rejected: More complex (need separate environments)\n\n2. **Separate migration job** - Kubernetes Job runs migrations\n   - Rejected: Timing issues (job must complete before pods start)\n   - Rejected: More complex (coordinate job + deployment)\n\n3. **Downtime window** - Stop all pods, migrate, start new pods\n   - Rejected: Unacceptable downtime (overlay goes dark)\n\n**Consequences:**\n\n✅ **Positive:**\n- Standard Kubernetes pattern (easy to understand)\n- No separate migration jobs (migrations in app startup)\n- Health checks guide traffic (load balancer waits for ready)\n\n❌ **Negative:**\n- Migrations must be backward-compatible (expand/contract)\n- Lua function versioning complex (old + new pods coexist briefly)\n- Pre-stop hook required (graceful shutdown)\n\n🔄 **Trade-offs:**\n- Chose rolling deploy over blue/green (cost savings)\n- Accept backward-compatible migration burden\n- Prioritize zero-downtime over simplicity\n\n**Related decisions:**\n- ADR-013: sqlc + tern (migration tooling)\n\n**Files to create:**\n- `docs/adr/015-deployment-strategy.md`\n\n**Time estimate:** 60 minutes\n\n### 6. Write ADR-016: Observability and SLO tracking\n\n**Context:**\n- Need to monitor system health and meet reliability targets\n- Standard tools: Prometheus (metrics), structured logs, tracing\n- Need to define SLOs (Service Level Objectives)\n\n**Decision:**\n- **Prometheus metrics** exposed at `/metrics` endpoint\n- **Structured logging** with slog (already implemented)\n- **Health checks** at `/health` (liveness) and `/ready` (readiness)\n- **No tracing initially** (defer OpenTelemetry until needed)\n\n**Key metrics:**\n- `chatpulse_active_sessions` - Current active sessions (gauge)\n- `chatpulse_websocket_clients` - Current WebSocket connections (gauge)\n- `chatpulse_votes_total` - Total votes processed (counter)\n- `chatpulse_broadcasts_total` - Tick broadcasts sent (counter)\n- `chatpulse_redis_errors_total` - Redis operation failures (counter)\n- `chatpulse_twitch_webhook_errors_total` - Webhook processing failures (counter)\n\n**Proposed SLOs:**\n- **Availability:** 99.5% uptime (measured by `/ready` health check)\n- **Vote latency:** P95 \u003c 500ms (webhook → Redis write)\n- **Broadcast latency:** P95 \u003c 100ms (tick → WebSocket send)\n\n**Alternatives considered:**\n1. **DataDog** - Commercial observability platform\n   - Rejected: Cost (self-hosted Prometheus free)\n   - Rejected: Vendor lock-in\n\n2. **OpenTelemetry tracing** - Distributed tracing from day one\n   - Rejected: Overhead (cardinality, storage)\n   - Rejected: Complexity (span propagation)\n   - Deferred: Add if debugging requires it\n\n3. **Log aggregation only** - No metrics, just logs\n   - Rejected: Hard to query (no aggregation)\n   - Rejected: Can't build dashboards easily\n\n**Consequences:**\n\n✅ **Positive:**\n- Standard tools (Prometheus + Grafana ecosystem)\n- Low overhead (metrics are cheap)\n- SLO tracking enables error budgets\n\n❌ **Negative:**\n- Requires scraping infrastructure (Prometheus server)\n- Cardinality management (don't track per-user metrics)\n- No tracing (harder to debug distributed issues)\n\n🔄 **Trade-offs:**\n- Chose metrics over tracing (lower overhead)\n- Accept cardinality management burden\n- Prioritize SLO tracking over full observability\n\n**Related decisions:**\n- None (operational concern)\n\n**Files to create:**\n- `docs/adr/016-observability-slo-tracking.md`\n\n**Time estimate:** 60 minutes\n\n### 7. Update ADR index page\n\nAdd Tier 4-5 ADRs to `docs/adr/README.md`.\n\n**Time estimate:** 10 minutes\n\n### 8. Review and polish\n\n- Read all 6 ADRs for consistency\n- Cross-reference related decisions\n- Verify tier organization makes sense\n\n**Time estimate:** 30 minutes\n\n## Acceptance Criteria\n\n- ✅ All 6 ADRs written\n- ✅ Tier 4 (implementation choices) clear\n- ✅ Tier 5 (operational practices) documented\n- ✅ Index page complete (all 16 ADRs linked)\n\n## Files Changed\n\n**Created:**\n- `docs/adr/011-actor-pattern-broadcaster.md`\n- `docs/adr/012-manual-dependency-injection.md`\n- `docs/adr/013-sqlc-for-sql-generation.md`\n- `docs/adr/014-time-decay-algorithm.md`\n- `docs/adr/015-deployment-strategy.md`\n- `docs/adr/016-observability-slo-tracking.md`\n\n**Modified:**\n- `docs/adr/README.md` (add Tier 4-5 links)\n\n## Dependencies\n- Epic 1-3 complete (Tier 1-3 ADRs)\n\n## Effort Estimate\n**Total: 6 hours** (360 minutes)\n- ADR-011 (60 min): Actor pattern\n- ADR-012 (60 min): Manual DI\n- ADR-013 (60 min): sqlc\n- ADR-014 (60 min): Time-decay algorithm\n- ADR-015 (60 min): Deployment strategy\n- ADR-016 (60 min): Observability + SLOs\n- Polish (30 min): Review + index update","status":"open","priority":3,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":360,"created_at":"2026-02-12T17:30:31.118493+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:53.24215+01:00"}
{"id":"twitch-tow-dvl","title":"Review and document WebSocket CORS policy","description":"**Medium Priority (Security)**\n\nLocation: internal/server/handlers_overlay.go lines 15-20\n\nIssue: CheckOrigin returns true for all origins. Opens door to cross-origin attacks, third-party hijacking, analytics/tracking.\n\nImpact: Security vulnerability if overlay data is sensitive.\n\nFix Options:\n1. Restrict to specific origins (localhost for dev, production domain)\n2. If truly public, add comprehensive security documentation\n3. Consider token-based authentication\n4. Document threat model and accepted risks","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:03.045753+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:38:22.222272+01:00","closed_at":"2026-02-12T16:38:22.222272+01:00","close_reason":"Closed"}
{"id":"twitch-tow-dw8","title":"EPIC: EventSub Graceful Degradation - Retry Logic and Health Monitoring","description":"**User Story:** As an operator, I need the application to gracefully degrade when Twitch EventSub is unavailable so that temporary Twitch API outages don't cause full service outages.\n\n**Problem Context:** Current EventSub integration lacks resilience:\n- Setup failures cause os.Exit(1) (fatal)\n- Subscribe failures delete session (no retry)\n- No backoff for rate limits (429)\n- Unsubscribe failures not retried (orphan subscriptions)\n\n**Risks:**\n- Twitch API downtime prevents app startup\n- New users see \"failed to activate session\"\n- Orphan subscriptions accumulate (cost/quota leak)\n\n**Solution Overview:** Make EventSub optional at startup, add retry with exponential backoff, rate limit handling, subscription health monitoring, and manual reconciliation endpoint.\n\n## Task Breakdown\n\n### 1. Make EventSub Optional at Startup\n\n**File:** `cmd/server/main.go`\n\n**Defer EventSub setup to first use:**\n```go\nfunc main() {\n    // ... existing setup\n    \n    // EventSub is optional (nil if not configured)\n    var eventsubManager *twitch.EventSubManager\n    var webhookHandler twitch.WebhookHandler\n    \n    if cfg.WebhookCallbackURL != \"\" {\n        // Try to setup EventSub, but don't exit on failure\n        manager, handler, err := initWebhooks(ctx, cfg)\n        if err != nil {\n            slog.Warn(\"EventSub setup failed, continuing without webhooks\",\n                \"error\", err,\n                \"impact\", \"votes will not be processed until EventSub recovers\")\n            eventsubSetupFailuresTotal.Inc()\n        } else {\n            eventsubManager = manager\n            webhookHandler = handler\n            slog.Info(\"EventSub configured\", \"callback_url\", cfg.WebhookCallbackURL)\n        }\n    } else {\n        slog.Info(\"EventSub disabled (WEBHOOK_CALLBACK_URL not set)\")\n    }\n    \n    // App service accepts nil eventsubManager (graceful degradation)\n    appSvc := app.NewService(app.ServiceConfig{...}, eventsubManager)\n    \n    // ... rest of startup\n}\n```\n\n### 2. Add Retry Logic to Subscribe/Unsubscribe\n\n**File:** `internal/twitch/eventsub.go`\n\n**Retry with exponential backoff:**\n```go\nfunc (m *EventSubManager) Subscribe(ctx context.Context, broadcasterUserID string) error {\n    maxRetries := 3\n    backoff := 1 * time.Second\n    \n    for attempt := 1; attempt \u003c= maxRetries; attempt++ {\n        err := m.attemptSubscribe(ctx, broadcasterUserID)\n        \n        if err == nil {\n            // Success\n            eventsubSubscribeAttemptsTotal.WithLabelValues(\"success\").Inc()\n            return nil\n        }\n        \n        // Check if error is retriable\n        if isEventSubRateLimited(err) {\n            // 429 rate limit - wait longer\n            backoff = 30 * time.Second\n            m.logger.Warn(\"EventSub rate limited, backing off\",\n                \"broadcaster_id\", broadcasterUserID,\n                \"attempt\", attempt,\n                \"backoff_seconds\", backoff.Seconds())\n        } else if !isEventSubRetriable(err) {\n            // Permanent error (e.g., invalid broadcaster ID)\n            eventsubSubscribeAttemptsTotal.WithLabelValues(\"permanent_error\").Inc()\n            return fmt.Errorf(\"EventSub subscribe failed (permanent): %w\", err)\n        }\n        \n        if attempt \u003c maxRetries {\n            m.logger.Warn(\"EventSub subscribe failed, retrying\",\n                \"broadcaster_id\", broadcasterUserID,\n                \"attempt\", attempt,\n                \"backoff_seconds\", backoff.Seconds(),\n                \"error\", err)\n            \n            time.Sleep(backoff)\n            backoff *= 2  // Exponential backoff\n        }\n    }\n    \n    eventsubSubscribeAttemptsTotal.WithLabelValues(\"exhausted\").Inc()\n    return fmt.Errorf(\"EventSub subscribe failed after %d attempts\", maxRetries)\n}\n\nfunc isEventSubRateLimited(err error) bool {\n    // Check for 429 status code\n    if apiErr, ok := err.(*helix.APIError); ok {\n        return apiErr.StatusCode == 429\n    }\n    return false\n}\n\nfunc isEventSubRetriable(err error) bool {\n    // 5xx errors are retriable, 4xx (except 429) are not\n    if apiErr, ok := err.(*helix.APIError); ok {\n        return apiErr.StatusCode \u003e= 500 || apiErr.StatusCode == 429\n    }\n    return true  // Network errors are retriable\n}\n```\n\n### 3. Add Subscription Health Monitoring\n\n**File:** `internal/app/service.go`\n\n**Track last webhook received:**\n```go\ntype Service struct {\n    // ... existing fields\n    lastWebhookReceived sync.Map  // map[broadcasterUserID]time.Time\n}\n\nfunc (s *Service) OnWebhookReceived(broadcasterUserID string) {\n    s.lastWebhookReceived.Store(broadcasterUserID, time.Now())\n}\n\n// Background health check\nfunc (s *Service) StartSubscriptionHealthCheck() {\n    go func() {\n        ticker := time.NewTicker(5 * time.Minute)\n        defer ticker.Stop()\n        \n        for range ticker.C {\n            s.checkSubscriptionHealth()\n        }\n    }()\n}\n\nfunc (s *Service) checkSubscriptionHealth() {\n    ctx := context.Background()\n    \n    // List all sessions\n    sessions, err := s.sessions.ListAllSessions(ctx)\n    if err != nil {\n        s.logger.Error(\"failed to list sessions for health check\", \"error\", err)\n        return\n    }\n    \n    staleThreshold := 10 * time.Minute\n    \n    for _, sessionUUID := range sessions {\n        broadcasterUserID, err := s.sessions.GetBroadcasterUserID(ctx, sessionUUID)\n        if err != nil {\n            continue\n        }\n        \n        lastWebhook, ok := s.lastWebhookReceived.Load(broadcasterUserID)\n        if !ok {\n            // Never received webhook (new subscription)\n            continue\n        }\n        \n        lastTime := lastWebhook.(time.Time)\n        if time.Since(lastTime) \u003e staleThreshold {\n            eventsubStaleSubscriptionsTotal.Inc()\n            s.logger.Warn(\"subscription appears stale (no webhooks received)\",\n                \"broadcaster_id\", broadcasterUserID,\n                \"last_webhook\", lastTime,\n                \"age_minutes\", time.Since(lastTime).Minutes())\n        }\n    }\n}\n```\n\n### 4. Update Webhook Handler to Track Health\n\n**File:** `internal/twitch/webhook.go`\n\n**Notify service on webhook receipt:**\n```go\nfunc (h *WebhookHandler) HandleEventSub(c echo.Context) error {\n    // ... existing verification\n    \n    if event.Subscription.Type == \"channel.chat.message\" {\n        broadcasterID := event.Event[\"broadcaster_user_id\"].(string)\n        \n        // Track webhook receipt (for health monitoring)\n        h.appService.OnWebhookReceived(broadcasterID)\n        \n        // ... existing vote processing\n    }\n    \n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n```\n\n### 5. Add Manual Reconciliation Endpoint\n\n**File:** `internal/server/handlers_admin.go`\n\n**Admin endpoint to cleanup orphan subscriptions:**\n```go\nfunc (s *Server) handleReconcileSubscriptions(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 60*time.Second)\n    defer cancel()\n    \n    // Get all EventSub subscriptions from Twitch\n    twitchSubs, err := s.twitchSvc.ListAllSubscriptions(ctx)\n    if err != nil {\n        return c.JSON(500, map[string]string{\"error\": fmt.Sprintf(\"failed to list Twitch subscriptions: %s\", err)})\n    }\n    \n    // Get all active sessions from Redis\n    activeSessions, err := s.sessions.ListAllSessions(ctx)\n    if err != nil {\n        return c.JSON(500, map[string]string{\"error\": fmt.Sprintf(\"failed to list sessions: %s\", err)})\n    }\n    \n    // Build set of active broadcaster IDs\n    activeBroadcasters := make(map[string]bool)\n    for _, sessionUUID := range activeSessions {\n        broadcasterID, err := s.sessions.GetBroadcasterUserID(ctx, sessionUUID)\n        if err == nil {\n            activeBroadcasters[broadcasterID] = true\n        }\n    }\n    \n    // Find orphan subscriptions (in Twitch but not in Redis)\n    var orphans []string\n    for _, sub := range twitchSubs {\n        if !activeBroadcasters[sub.BroadcasterUserID] {\n            orphans = append(orphans, sub.BroadcasterUserID)\n            \n            // Delete orphan subscription\n            err := s.twitchSvc.Unsubscribe(ctx, sub.BroadcasterUserID)\n            if err != nil {\n                s.logger.Error(\"failed to delete orphan subscription\",\n                    \"broadcaster_id\", sub.BroadcasterUserID,\n                    \"error\", err)\n            } else {\n                s.logger.Info(\"deleted orphan subscription\",\n                    \"broadcaster_id\", sub.BroadcasterUserID)\n            }\n        }\n    }\n    \n    return c.JSON(200, map[string]interface{}{\n        \"total_twitch_subs\": len(twitchSubs),\n        \"active_sessions\": len(activeSessions),\n        \"orphans_found\": len(orphans),\n        \"orphans\": orphans,\n    })\n}\n\n// Register in routes\nfunc (s *Server) registerRoutes() {\n    admin := s.e.Group(\"/admin\")\n    admin.Use(s.requireAuth)\n    admin.POST(\"/reconcile-subscriptions\", s.handleReconcileSubscriptions)\n}\n```\n\n### 6. Add EventSub Metrics\n\n**File:** `internal/twitch/eventsub.go`\n\n```go\nvar (\n    eventsubSetupFailuresTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"eventsub_setup_failures_total\",\n            Help: \"Total number of EventSub setup failures at startup\",\n        })\n    \n    eventsubSubscribeAttemptsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"eventsub_subscribe_attempts_total\",\n            Help: \"Total number of EventSub subscribe attempts by result\",\n        },\n        []string{\"result\"},  // \"success\", \"exhausted\", \"permanent_error\"\n    )\n    \n    eventsubStaleSubscriptionsTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"eventsub_stale_subscriptions_total\",\n            Help: \"Total number of stale subscriptions detected (no webhooks \u003e10min)\",\n        })\n)\n```\n\n### 7. Documentation\n\n**Update CLAUDE.md:**\n```markdown\n### EventSub Graceful Degradation\n\n**Startup resilience:**\n- EventSub setup failures don't prevent app startup (logged as warning)\n- App continues without webhooks (overlay works, votes not processed)\n\n**Subscribe retry:**\n- 3 attempts with exponential backoff (1s, 2s, 4s)\n- 429 rate limit → 30s backoff\n- Permanent errors (4xx except 429) fail immediately\n\n**Subscription health monitoring:**\n- Track last webhook received per broadcaster\n- Stale threshold: 10 minutes without webhooks\n- Background check every 5 minutes, logs warnings\n\n**Manual reconciliation:**\n- `POST /admin/reconcile-subscriptions` (auth required)\n- Compares Twitch subscriptions vs active sessions\n- Deletes orphan subscriptions (not in Redis)\n\n**Metrics:**\n- eventsub_setup_failures_total (counter)\n- eventsub_subscribe_attempts_total{result} (counter)\n- eventsub_stale_subscriptions_total (counter)\n\n**Fallback mode:** Overlay displays current value (from Redis) even without live votes.\n```\n\n### 8. Testing\n\n**File:** `internal/twitch/eventsub_retry_test.go` (NEW)\n\n```go\n// TestSubscribe_RetrySuccess verifies retry logic\nfunc TestSubscribe_RetrySuccess(t *testing.T) {\n    // Setup: Mock Twitch API fails twice, succeeds third time\n    // Act: Subscribe\n    // Assert: 3 attempts made, success returned\n}\n\n// TestSubscribe_RateLimitBackoff verifies 429 handling\nfunc TestSubscribe_RateLimitBackoff(t *testing.T) {\n    // Setup: Mock returns 429\n    // Act: Subscribe\n    // Assert: 30s backoff applied, retry attempted\n}\n\n// TestSubscribe_PermanentError verifies no retry\nfunc TestSubscribe_PermanentError(t *testing.T) {\n    // Setup: Mock returns 400 (invalid broadcaster)\n    // Act: Subscribe\n    // Assert: Returns immediately, no retries\n}\n\n// TestSubscriptionHealthCheck verifies stale detection\nfunc TestSubscriptionHealthCheck(t *testing.T) {\n    // Setup: Session active, last webhook 15 minutes ago\n    // Act: checkSubscriptionHealth\n    // Assert: Stale subscription metric incremented\n}\n```\n\n## Acceptance Criteria\n\n✅ EventSub setup failures don't prevent app startup (logged, app continues)\n✅ Subscribe retries 3 times with exponential backoff\n✅ 429 rate limit triggers 30s backoff\n✅ Permanent errors (4xx except 429) fail immediately without retry\n✅ Subscription health check detects stale subscriptions (\u003e10min)\n✅ Manual reconciliation endpoint cleans up orphan subscriptions\n✅ Metrics track setup failures, subscribe attempts, stale subscriptions\n✅ Tests verify retry logic, rate limit handling, health checks\n✅ CLAUDE.md documents graceful degradation behavior\n\n## Dependencies\n\n- Synergy with: Cleanup robustness epic (twitch-tow-9kw) for unsubscribe retry queue\n\n## Files Modified\n\n**Modified:**\n- cmd/server/main.go (optional EventSub setup)\n- internal/twitch/eventsub.go (retry logic, metrics)\n- internal/app/service.go (subscription health check)\n- internal/twitch/webhook.go (track webhook receipt)\n- CLAUDE.md (document degradation)\n\n**New:**\n- internal/server/handlers_admin.go (reconciliation endpoint)\n- internal/twitch/eventsub_retry_test.go (retry tests)\n\n## Estimated Effort\n\n**Implementation:** 3 developer-days\n- Optional startup: 2 hours\n- Retry logic: 1 day\n- Health monitoring: 4 hours\n- Reconciliation endpoint: 3 hours\n- Metrics: 2 hours\n- Testing: 1 day\n- Documentation: 2 hours\n\n**Total:** 3 developer-days\n\n## Rollout Strategy\n\n1. Deploy with optional EventSub (no behavior change if configured)\n2. Monitor eventsub_subscribe_attempts_total{result=\"success\"} (should be \u003e95%)\n3. Simulate Twitch API failures in staging (verify retry works)\n4. Monitor eventsub_stale_subscriptions_total (should be 0 in healthy system)\n5. Run manual reconciliation weekly to cleanup orphans\n6. Alert if stale subscription count \u003e10 (Twitch webhook delivery issues)","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:49:12.529437+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:42.00073+01:00"}
{"id":"twitch-tow-dwo","title":"Discussion: Session activation cold start latency","description":"EnsureSessionActive uses singleflight to collapse concurrent activations but cold starts still require DB roundtrips.\n\nCurrent flow (first connection to a session):\n1. Check Redis: SessionExists (1 RTT)\n2. If not exists:\n   - GetUserByOverlayUUID (DB query)\n   - GetConfigByUserID (DB query)  \n   - ActivateSession (Redis pipeline: HSET + SET)\n   - Subscribe to EventSub (Twitch API call)\n\nLatency breakdown (estimated):\n- Redis check: 1-5ms\n- DB user lookup: 10-50ms\n- DB config lookup: 10-50ms (can't batch, different tables)\n- Redis activation: 5-10ms\n- Twitch subscribe: 100-500ms\nTotal: 126-615ms for cold start\n\nFrequency:\n- Once per session on first connection (any instance)\n- Rare after warmup (sessions stay in Redis)\n- Spikes after Redis restart or cleanup\n\nScalability concerns:\n\n1. DB queries are serial\n- Two queries: user, then config\n- Could be parallel (separate goroutines)\n- Or single JOIN query\n\n2. No caching of user/config\n- Every cold start hits DB\n- Same user data fetched repeatedly\n- Config rarely changes\n\n3. Twitch API call in critical path\n- 100-500ms API latency\n- Blocks WebSocket upgrade\n- No retry on transient failure\n\n4. Singleflight per session UUID\n- Multiple instances can activate simultaneously\n- Each does full DB + API round-trip\n- Only protects against concurrent requests on same instance\n\nPotential optimizations:\n\nA. Cache user+config in Redis\n- Store user:{userID} and config:{userID}\n- Check cache before DB\n- Invalidate on update\n- Reduces cold start by 20-100ms\n\nB. Parallel DB queries\n- Fetch user and config concurrently\n- Use errgroup.WithContext\n- Reduces latency to max(user, config)\n\nC. JOIN user+config in single query\n- sqlc doesn't support this easily\n- Custom query or raw SQL\n- Best latency but more complex\n\nD. Async Twitch subscribe\n- Activate session immediately\n- Subscribe in background goroutine\n- Vote processing works even if subscribe pending\n- Risk: votes lost if subscribe fails\n\nE. Distributed singleflight\n- Use Redis SETNX for cross-instance locking\n- Only one instance does DB + API work\n- Others wait on Redis key\n- Complex, may not be worth it\n\nRecommendation:\n1. Implement parallel DB queries (option B) - easy win\n2. Add user+config caching in Redis (option A) - bigger win\n3. Consider async EventSub subscribe (option D) - reduces tail latency\n4. Defer distributed singleflight (option E) - premature\n\nExpected improvement:\n- Parallel queries: 50-100ms → 50ms (eliminates serial penalty)\n- With cache: 50ms → 5ms (Redis-only cold start)\n\nPriority: P2 - cold start latency is noticeable but infrequent\n","notes":"CONVERTED TO EPIC: twitch-tow-9f2 (Epic: Optimize Session Activation Cold Start). This epic implements config caching in Redis to reduce cold start latency from 126-615ms to \u003c50ms. Includes cache invalidation strategy, metrics, and full testing plan. Complements the broadcaster config cache (twitch-tow-4c4) for comprehensive config caching architecture.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:14.098267+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:41:10.680191+01:00","closed_at":"2026-02-12T17:41:10.680194+01:00"}
{"id":"twitch-tow-e4l","title":"Solution: Documentation improvements","description":"SOLUTION PROPOSAL for documentation concerns (twitch-tow-4kh ADRs, twitch-tow-jgs package docs, twitch-tow-0jo dependency visualization)\n\nProblem: Excellent CLAUDE.md but missing package-level docs, architecture decision records, and dependency diagrams.\n\nProposed Solution: Three-part documentation enhancement\n\nPart 1 - Architecture Decision Records (ADRs):\nCreate docs/adr/ directory with lightweight ADR format.\nTemplate: Title, Context (why decision needed), Decision (what was chosen), Alternatives Considered, Consequences (trade-offs), Status (accepted/deprecated).\n\nPriority ADRs to document:\n1. ADR-001: Redis-only architecture (no in-memory state)\n2. ADR-002: Pull-based broadcaster vs pub/sub\n3. ADR-003: Actor pattern for broadcaster concurrency\n4. ADR-004: Webhooks + conduits vs EventSub WebSocket\n5. ADR-005: Single bot account for all streamers\n6. ADR-006: UUID-based overlay access control\n7. ADR-007: sqlc for SQL generation\n8. ADR-008: Manual dependency injection\n\nEstimated effort: 1 hour per ADR, 8 hours total.\n\nPart 2 - Package Documentation:\nAdd package-level doc comments to all internal packages.\nFormat: Standard Go package comment (before package declaration).\n\nExample for domain package:\n// Package domain defines the core business types and interfaces for ChatPulse.\n// It has zero internal dependencies and serves as the foundation for all other layers.\n\nExample for sentiment package:\n// Package sentiment implements the sentiment calculation engine\n// orchestrating session queries, vote application, and debounce checks.\n\nTarget: 1-2 sentence summary per package.\nEstimated effort: 2 hours (11 packages × 10 minutes).\n\nPart 3 - Dependency Visualization:\nAdd Mermaid diagram to CLAUDE.md showing package dependencies.\n\nDiagram structure:\n- Layer 1: domain (foundation)\n- Layer 2: config, crypto, logging (utilities)\n- Layer 3: database, redis, sentiment (infrastructure)\n- Layer 4: app (application)\n- Layer 5: server, broadcast, twitch (transport)\n- Layer 6: cmd/server/main.go (composition root)\n\nMermaid syntax renders natively on GitHub.\nEstimated effort: 2 hours (create + validate).\n\nTotal Estimated Effort: 12 hours (8h ADRs, 2h package docs, 2h diagram).\n\nTrade-offs:\nPros: Captures historical context (ADRs). Improves godoc discoverability (package docs). Visual onboarding aid (diagram). Low maintenance burden.\nCons: ADRs need to be kept up to date. Risk of docs drifting from code. Initial time investment.\n\nVote: +1 from Maintainability architect. These are high-value, low-effort improvements that significantly enhance onboarding and historical context.","notes":"Vote: +1 from architect-scalability\n\nRATIONALE: 8 ADRs + package docs is comprehensive. Documentation of architectural decisions prevents knowledge loss.\n\nADRs I'd like to see from scalability perspective:\n1. Why pull-based broadcasting (not Redis pub/sub)\n2. Why actor pattern for Broadcaster (not worker pool)\n3. Why Redis-only state (not DB-backed cache)\n4. Why Lua functions (not application-side logic)\n5. Ref counting strategy across instances\n\nAGREEMENT: Package-level docs for broadcast, redis, sentiment packages are high value. These have complex concurrency patterns.\n\nMaintainability architect leads implementation.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:13:06.76955+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.444943+01:00","closed_at":"2026-02-12T17:56:57.444943+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-e6s","title":"EPIC: Add Package Dependency Visualization to Documentation","description":"Add visual package dependency diagram to CLAUDE.md using Mermaid for better onboarding and architecture understanding.\n\n## User Story\nAs a new developer joining the project, I want a visual package dependency diagram so I can quickly understand the layered architecture without reading all the code.\n\n## Value Proposition\n- Faster onboarding (visual learning vs reading code)\n- Validates layering rules (easy to spot violations)\n- Living documentation (Mermaid renders in GitHub)\n- Low maintenance (update when adding packages)\n\n## Background\n\n**Current state (twitch-tow-0jo):**\n- CLAUDE.md has excellent narrative documentation\n- No visual dependency diagram\n- Package structure must be inferred from imports\n\n**Architecture layers:**\n1. **Domain** (no dependencies) → interfaces + types\n2. **Infrastructure** (database, redis, crypto, twitch) → implement domain interfaces\n3. **Application** (app, sentiment, broadcast) → orchestrate domain\n4. **Server** (server) → HTTP handlers\n5. **Main** (cmd/server) → wiring\n\n## Tasks\n\n### 1. Create Mermaid dependency diagram\n\n**Add to CLAUDE.md (after Project Structure section):**\n\n```markdown\n## Package Dependency Graph\n\n```mermaid\ngraph TB\n    subgraph \"Entry Point\"\n        main[cmd/server/main.go]\n    end\n    \n    subgraph \"HTTP Layer\"\n        server[server]\n    end\n    \n    subgraph \"Application Layer\"\n        app[app]\n        sentiment[sentiment]\n        broadcast[broadcast]\n    end\n    \n    subgraph \"Infrastructure Layer\"\n        database[(database)]\n        redis[(redis)]\n        crypto[crypto]\n        twitch[twitch]\n        config[config]\n    end\n    \n    subgraph \"Domain Layer\"\n        domain[domain\u003cbr/\u003einterfaces + types]\n    end\n    \n    %% Dependencies\n    main --\u003e server\n    main --\u003e app\n    main --\u003e database\n    main --\u003e redis\n    main --\u003e config\n    \n    server --\u003e app\n    server --\u003e domain\n    \n    app --\u003e database\n    app --\u003e redis\n    app --\u003e twitch\n    app --\u003e sentiment\n    app --\u003e domain\n    \n    sentiment --\u003e domain\n    broadcast --\u003e domain\n    \n    database --\u003e domain\n    database --\u003e crypto\n    redis --\u003e domain\n    twitch --\u003e domain\n    \n    %% Styling\n    classDef domainStyle fill:#e1f5e1\n    classDef infraStyle fill:#e3f2fd\n    classDef appStyle fill:#fff9c4\n    classDef serverStyle fill:#ffccbc\n    classDef mainStyle fill:#f3e5f5\n    \n    class domain domainStyle\n    class database,redis,crypto,twitch,config infraStyle\n    class app,sentiment,broadcast appStyle\n    class server serverStyle\n    class main mainStyle\n```\n\n**Key principles visible in diagram:**\n- ✅ Domain has no dependencies (foundation layer)\n- ✅ Infrastructure depends only on domain\n- ✅ Application orchestrates infrastructure\n- ✅ Server depends on application (not infrastructure directly)\n- ✅ Main wires everything together\n\n**Dependency rules:**\n- ❌ Infrastructure NEVER imports other infrastructure (database ↛ redis)\n- ❌ Domain NEVER imports anything internal (pure interfaces)\n- ❌ Server NEVER imports infrastructure directly (goes through app)\n```\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Package Dependency Graph section)\n\n**Time estimate:** 30 minutes\n\n---\n\n### 2. Add dependency validation script\n\n**Create CI check to enforce layering:**\n\n```bash\n#\\!/bin/bash\n# scripts/check-deps.sh\n\nset -e\n\necho \"Checking package dependency rules...\"\n\n# Rule 1: domain package should have no internal imports\nif go list -f '{{.ImportPath}}: {{join .Imports \", \"}}' ./internal/domain | grep -q \"github.com/.*chatpulse/internal\"; then\n    echo \"❌ FAIL: domain package imports internal packages (should be pure interfaces)\"\n    exit 1\nfi\n\n# Rule 2: infrastructure packages should not import each other\nINFRA_PKGS=(\"database\" \"redis\" \"crypto\" \"twitch\")\nfor pkg in \"${INFRA_PKGS[@]}\"; do\n    for other in \"${INFRA_PKGS[@]}\"; do\n        if [ \"\" \\!= \"\" ]; then\n            if go list -f '{{.ImportPath}}: {{join .Imports \", \"}}' ./internal/ | grep -q \"internal/\"; then\n                echo \"❌ FAIL:  imports  (infrastructure should not cross-import)\"\n                exit 1\n            fi\n        fi\n    done\ndone\n\n# Rule 3: server should not import infrastructure directly\nif go list -f '{{.ImportPath}}: {{join .Imports \", \"}}' ./internal/server | grep -qE \"internal/(database|redis|crypto|twitch)\"; then\n    echo \"❌ FAIL: server imports infrastructure directly (should use app layer)\"\n    exit 1\nfi\n\necho \"✅ PASS: All dependency rules validated\"\n```\n\n**Add to CI workflow:**\n```yaml\n# .github/workflows/test.yml\n\n- name: Validate package dependencies\n  run: ./scripts/check-deps.sh\n```\n\n**Files to create:**\n- `scripts/check-deps.sh` (new script)\n\n**Files to modify:**\n- `.github/workflows/test.yml` (add dependency check step)\n\n**Time estimate:** 30 minutes\n\n---\n\n## Acceptance Criteria\n\n- ✅ Mermaid diagram renders correctly on GitHub\n- ✅ Diagram shows all major packages (domain, infra, app, server, main)\n- ✅ Dependency arrows match actual import relationships\n- ✅ Color coding distinguishes layers (domain=green, infra=blue, app=yellow, server=orange)\n- ✅ Dependency rules documented (what's allowed, what's forbidden)\n- ✅ (Optional) CI validates dependency rules\n\n## Files Changed\n\n**Modified:**\n- `CLAUDE.md` (add Package Dependency Graph section with Mermaid diagram)\n\n**Created (Optional):**\n- `scripts/check-deps.sh` (dependency validation script)\n\n**Modified (Optional):**\n- `.github/workflows/test.yml` (add dependency check)\n\n## Dependencies\n- None (documentation improvement)\n\n## Effort Estimate\n**Total: 1 hour** (60 minutes)\n- Mermaid diagram (30min): Create + validate + document rules\n- Validation script (30min): Optional, bash script + CI integration\n\n## Success Metrics\n- New developers reference diagram during onboarding\n- No layer violations detected by validation script\n- Diagram stays up-to-date (updated when packages added)","status":"open","priority":3,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":60,"created_at":"2026-02-12T17:49:06.542884+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:49.548379+01:00"}
{"id":"twitch-tow-e9m","title":"Discussion: Graceful degradation when Twitch EventSub is unavailable","description":"## Issue\nEventSub failures are fatal at startup and during operation. No graceful degradation or retry patterns for transient Twitch API failures.\n\n## Current State\n- EventSub setup failures cause os.Exit(1) (main.go:46-48)\n- Subscribe failures rollback session and return error (eventsub.go:111-114)\n- Unsubscribe failures are logged but not retried (eventsub.go:206-209)\n- No retry logic for Twitch API rate limits (429) or transient failures (5xx)\n- Webhook timestamp validation is defense-in-depth but no replay queue for missed events\n\n## Failure Modes\n1. **Startup failures**: Twitch API downtime prevents entire app from starting\n2. **New user failures**: Users cannot connect if Subscribe fails (session deleted)\n3. **Orphan subscriptions**: Unsubscribe failures leave stale Twitch subscriptions\n4. **Rate limiting**: High user churn could hit Twitch API rate limits with no backoff\n5. **Webhook delivery gaps**: If webhook delivery fails, votes are permanently lost\n\n## Risks\n- **Availability**: Twitch API instability cascades to full service outage\n- **User experience**: New streamers see \"failed to activate session\" with no context\n- **Cost**: Orphan subscriptions accumulate if cleanup fails repeatedly\n- **Data loss**: Missed webhook events = lost votes (no replay mechanism)\n\n## Suggestions\n1. Make EventSub optional at startup (defer to first user connection)\n2. Add retry with exponential backoff for Subscribe/Unsubscribe (e.g., 3 attempts)\n3. Implement Twitch API rate limit detection (429) with backoff\n4. Add dead letter queue for failed Unsubscribe attempts (retry later)\n5. Track EventSub subscription health (last webhook received timestamp)\n6. Add manual reconciliation endpoint to cleanup orphan subscriptions\n7. Consider webhook replay mechanism for missed events (if Twitch supports)\n8. Fallback mode: allow overlay to work without live vote processing (manual reset only)\n\n## Files\n- cmd/server/main.go:37-56 (initWebhooks)\n- internal/twitch/eventsub.go:64-112 (Setup)\n- internal/twitch/eventsub.go:145-193 (Subscribe)\n- internal/twitch/eventsub.go:195-218 (Unsubscribe)\n- internal/app/service.go:105-117 (EnsureSessionActive rollback)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:01.569588+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:49:18.346691+01:00","closed_at":"2026-02-12T17:49:18.346691+01:00","close_reason":"Superseded by implementation epic twitch-tow-dw8 (EventSub Graceful Degradation). Epic provides optional startup, retry with exponential backoff, rate limit handling, subscription health monitoring, manual reconciliation endpoint."}
{"id":"twitch-tow-euk","title":"Discussion: Context propagation inconsistency","description":"The codebase shows inconsistent context usage patterns:\n\n**1. Context.Background() vs request contexts:**\n- Broadcaster tick loop: Uses context.Background() with 2s timeout for Redis calls\n- CleanupOrphans: Uses context.Background() for Twitch unsubscribe (correct - fire-and-forget)\n- Server handlers: Properly propagate Echo request context\n- Main.go setup: Uses context.WithTimeout(Background(), 10s) for DB/Redis setup\n\n**2. No context cancellation propagation:**\n- Broadcaster.Stop() doesn't cancel in-flight Redis operations (they timeout independently)\n- App.Service cleanup goroutines use context.Background(), ignoring shutdown signal\n- WebSocket read pump blocks on conn.ReadMessage() with no context\n\n**3. Timeout values scattered:**\n- Redis timeout: 2s (broadcaster)\n- Command timeout: 5s (broadcaster register/unregister)\n- OAuth timeout: 10s (auth handlers)\n- App token timeout: 15s (EventSub manager)\n- Cleanup scan timeout: 30s (app service)\n\n**Issues:**\n1. Shutdown can take up to max(all timeouts) = 30s+ to complete all in-flight operations\n2. No central timeout configuration\n3. Can't cancel long-running operations on demand\n4. Tests can't speed up timeouts (hardcoded constants)\n\n**Options:**\nA. Inject timeout durations via config or constructor parameters\nB. Add context cancellation to Stop() methods\nC. Document the shutdown timing contract\nD. Keep current - argue that hardcoded timeouts are acceptable\n\n**Recommendation**: Document shutdown timing and consider making timeouts configurable for testing.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:30.1411+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:40:13.268181+01:00","closed_at":"2026-02-12T17:40:13.268181+01:00","close_reason":"Superseded by implementation epic twitch-tow-9c6 (Context Propagation and Configurable Timeouts). Epic provides shutdown context cancellation, configurable timeouts, request context propagation, testing strategy."}
{"id":"twitch-tow-eyl","title":"SOLUTION: Implement comprehensive observability (health checks + Prometheus metrics)","description":"SOLUTION: Comprehensive Observability Implementation\n\nThis solution addresses consensus beads:\n- twitch-tow-6hx (P1 - Scalability)\n- twitch-tow-c8q (P2 - Resilience)  \n- twitch-tow-bdb (P2 - Maintainability)\n\n## Implementation Plan\n\n### Phase 1: Health Checks (Week 1)\nPriority: CRITICAL - blocks production deployment\n\n**New endpoints:**\n```go\nGET /health/live   → 200 OK (always, process alive)\nGET /health/ready  → 200 OK if all dependencies healthy\n```\n\n**Readiness checks:**\n- Redis: PING command (timeout 1s)\n- PostgreSQL: SELECT 1 (timeout 1s)\n- Lua functions: Verify library loaded\n- Twitch EventSub: Check conduit exists (if configured)\n\n**Implementation:**\n```go\n// internal/server/handlers_health.go\nfunc (s *Server) handleLiveness(c echo.Context) error {\n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n\nfunc (s *Server) handleReadiness(c echo.Context) error {\n    checks := []check{\n        {name: \"redis\", fn: s.checkRedis},\n        {name: \"postgres\", fn: s.checkPostgres},\n    }\n    \n    for _, check := range checks {\n        if err := check.fn(ctx); err != nil {\n            return c.JSON(503, map[string]any{\n                \"status\": \"unhealthy\",\n                \"failed_check\": check.name,\n                \"error\": err.Error(),\n            })\n        }\n    }\n    return c.JSON(200, map[string]string{\"status\": \"ready\"})\n}\n```\n\n**Test strategy:**\n- Unit tests with mocked dependencies\n- Integration test that kills Redis, expects 503\n- Load balancer configuration example in docs\n\n---\n\n### Phase 2: Prometheus Metrics (Week 2-3)\nPriority: HIGH - needed for production monitoring\n\n**Dependency:** github.com/prometheus/client_golang\n\n**Metrics to implement:**\n\n**1. Redis metrics:**\n```go\nredis_operations_total{operation, status}          // counter\nredis_operation_duration_seconds{operation}        // histogram\nredis_connection_errors_total                      // counter\n```\n\n**2. Broadcaster metrics:**\n```go\nbroadcaster_active_sessions                        // gauge\nbroadcaster_connected_clients_total                // gauge\nbroadcaster_tick_duration_seconds                  // histogram\nbroadcaster_slow_clients_evicted_total             // counter\n```\n\n**3. WebSocket metrics:**\n```go\nwebsocket_connections_current                      // gauge\nwebsocket_connections_total{result}                // counter (success/error)\nwebsocket_message_send_duration_seconds            // histogram\n```\n\n**4. Vote processing:**\n```go\nvote_processing_total{result}                      // counter (applied/debounced/invalid)\nvote_processing_duration_seconds                   // histogram\n```\n\n**5. Database metrics:**\n```go\ndb_query_duration_seconds{query}                   // histogram\ndb_connections_current{state}                      // gauge (active/idle)\ndb_errors_total{query}                             // counter\n```\n\n**Implementation approach:**\n1. Create internal/metrics package with prometheus collectors\n2. Wrap Redis client with instrumentation\n3. Add metrics to Broadcaster tick loop\n4. Instrument vote processing pipeline\n5. Add middleware for HTTP request metrics\n\n**Endpoint:**\n```go\nGET /metrics → Prometheus text format\n```\n\n**Configuration:**\n- No authentication on /metrics (common practice)\n- Or add basic auth via middleware if needed\n- Document how to scrape in deployment guide\n\n---\n\n### Phase 3: Structured Logging Improvements (Week 4)\nPriority: MEDIUM - current logging works but could be better\n\n**Enhancements:**\n1. Request ID propagation via middleware\n2. Consistent log fields across components\n3. Performance: use slog groups for context\n\n**Middleware for request IDs:**\n```go\nfunc requestIDMiddleware(next echo.HandlerFunc) echo.HandlerFunc {\n    return func(c echo.Context) error {\n        reqID := uuid.New().String()\n        c.Set(\"request_id\", reqID)\n        c.Response().Header().Set(\"X-Request-ID\", reqID)\n        \n        logger := slog.With(\"request_id\", reqID)\n        c.Set(\"logger\", logger)\n        \n        return next(c)\n    }\n}\n```\n\n**Consistent field names:**\n```\nsession_uuid (not sessionUUID or session_id)\nuser_id (not userID or user)\nerror (always lowercase)\nduration_ms (not latency or elapsed)\n```\n\n---\n\n### Phase 4: Distributed Tracing (Future)\nPriority: LOW - defer until multi-service architecture\n\n**When to implement:**\n- Multiple services communicating\n- Need to trace requests across services\n- Performance debugging requires detailed traces\n\n**Approach:** OpenTelemetry with Jaeger/Tempo backend\n\n**Defer because:**\n- Single monolith doesn't need distributed tracing\n- Prometheus metrics + logs sufficient for now\n- Added complexity not justified yet\n\n---\n\n## Success Criteria\n\n**Phase 1 (Health Checks):**\n- ✅ Load balancer can route traffic to healthy instances\n- ✅ Unhealthy instances return 503 and are removed from pool\n- ✅ Health checks complete in under 1 second\n\n**Phase 2 (Metrics):**\n- ✅ Prometheus can scrape /metrics endpoint\n- ✅ Key metrics exported (15+ metrics covering Redis, WS, votes, DB)\n- ✅ Metrics used in Grafana dashboard (reference dashboard in docs)\n- ✅ Alerts configured for critical metrics (error rates, latencies)\n\n**Phase 3 (Logging):**\n- ✅ Request IDs in all logs\n- ✅ Can trace request flow through components\n- ✅ Consistent field names across codebase\n\n---\n\n## Trade-offs\n\n**Pros:**\n+ Production-ready monitoring\n+ Can detect issues before users report them\n+ Enables capacity planning\n+ Required for SLO tracking\n\n**Cons:**\n- Adds dependencies (prometheus/client_golang)\n- Metrics collection has CPU/memory cost (~1-2% overhead)\n- More code to maintain\n\n**Verdict:** Essential for production. Small overhead is acceptable.\n\n---\n\n## Implementation Effort\n\n**Phase 1:** 1 developer-week (health checks)\n**Phase 2:** 2 developer-weeks (metrics)\n**Phase 3:** 1 developer-week (logging)\n\n**Total:** 4 developer-weeks\n\n---\n\n## Open Questions for Team\n\n1. Should /metrics require authentication?\n2. What Prometheus retention period? (default: 15 days)\n3. Which metrics are most critical for alerting?\n4. Do we need custom Grafana dashboards or use generic Go app dashboard?\n\nVote: Use bd update to add your +1 or concerns\n","notes":"Vote: +1 from architect-resilience (Resilience Expert).\n\nSTRONG APPROVAL - This solution directly addresses my twitch-tow-c8q concerns about health checks and incident response visibility.\n\n**Why this is critical for resilience:**\n\n1. **Health Checks (Phase 1) - ESSENTIAL**\n   - Prevents traffic routing to unhealthy instances\n   - Enables graceful degradation at load balancer level\n   - Redis PING + Postgres SELECT 1 + Lua function verification covers all critical dependencies\n   - 1s timeout is appropriate (fail fast)\n\n2. **Metrics (Phase 2) - KEY RESILIENCE SIGNALS**\n   The proposed metrics catalog includes exactly what we need for incident response:\n   - redis_operation_duration_seconds → detect Redis slowdown before circuit breaker trips\n   - redis_connection_errors_total → alert on connection pool exhaustion\n   - websocket_connections_current → capacity planning (prevent hitting 10k limit)\n   - broadcaster_slow_clients_evicted_total → monitor eviction rate\n   - vote_processing_duration_seconds → detect vote pipeline degradation\n\n3. **Synergy with Circuit Breaker**\n   My twitch-tow-sb3 (Redis circuit breaker) and this observability work are complementary:\n   - Circuit breaker provides resilience mechanism\n   - Metrics provide visibility into when/why circuit opens\n   - Together: graceful degradation + observability\n\n**Answers to open questions:**\n\n**Q1: Should /metrics require authentication?**\n- **My vote: NO** - Standard practice to leave /metrics open\n- Rationale: Prometheus scraper config is simpler, metrics don't contain PII\n- Alternative: If security policy requires auth, use basic auth (but adds ops burden)\n\n**Q2: Prometheus retention period?**\n- **Suggestion: 15 days default, 90 days for critical metrics**\n- Rationale: 15 days sufficient for incident investigation, longer retention for capacity planning\n\n**Q3: Most critical metrics for alerting?**\nFrom resilience perspective, these should page on-call:\n- **redis_operation_duration_seconds{quantile=\"0.99\"} \u003e 0.1** (100ms p99 → Redis degraded)\n- **redis_connection_errors_total rate \u003e 5/min** (connection pool exhaustion)\n- **websocket_connections_current \u003e 8000** (80% of 10k capacity)\n- **broadcaster_tick_duration_seconds{quantile=\"0.95\"} \u003e 0.05** (50ms p95 → tick loop slow)\n- **vote_processing_duration_seconds{quantile=\"0.99\"} \u003e 0.5** (500ms p99 → vote pipeline slow)\n\n**Q4: Phased rollout?**\n- **My vote: Phase 1 immediately (health checks blocking), Phase 2 next sprint**\n- Rationale: Health checks are table stakes for production, metrics can follow\n\n**Minor suggestion:**\nConsider adding circuit breaker state metric in Phase 2:\n```\nredis_circuit_breaker_state{state=\"closed\"|\"open\"|\"half_open\"} // gauge (0 or 1)\n```\nThis enables alerting when circuit opens (incident in progress).\n\n**Consolidation recommendation:**\nMerge with maintainability's twitch-tow-1n1 (they suggested /version endpoint) - adopt eyl structure + add /version to Phase 1.\n\n**No concerns - ready to implement.**\nVote: +1 from Maintainability Architect\n\nRATIONALE: 4-phase approach is well-structured. Metric selection comprehensive. Deferred tracing is correct prioritization. Critical blocker for production.\n\nINTEGRATION: Merge with my ADR-016 (Epic 5). Your proposal = implementation, my ADR = rationale/alternatives/consequences.\n\nANSWERS:\n- Q1 (/metrics auth): NO - Prometheus needs unauthenticated. Use network controls.\n- Q2 (critical metrics): redis_operation_duration_seconds, websocket_connections_current, vote_processing_total{result=\"error\"}, db_query_duration_seconds, broadcaster_tick_duration_seconds\n- Q3 (dashboards): Start generic, add custom in Phase 3 (business metrics)\n\nRECOMMENDATIONS:\n- Add chatpulse_errors_total{type=\"domain|infrastructure|programming\"}\n- Add chatpulse_session_activation_duration_seconds\n- Document metric naming in ERROR_HANDLING.md\n- Document SLOs in ADR-016 (99.5% uptime, P95 vote \u003c500ms, P95 broadcast \u003c100ms)\n\nEFFORT: 4 dev-weeks reasonable","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:12:19.647169+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:56.981592+01:00","closed_at":"2026-02-12T17:56:56.981592+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics","labels":["consensus","observability","phase-2-solution"]}
{"id":"twitch-tow-f6g","title":"Discussion: Config is re-fetched on every Broadcaster tick","description":"The Broadcaster pulls values via GetCurrentValue which calls GetSessionConfig (reads config JSON from Redis). This happens 20 times per second per session.\n\nCurrent flow per tick per session:\n1. Engine.GetCurrentValue()\n2. SessionRepo.GetSessionConfig() - Redis HGET config_json\n3. json.Unmarshal(config)\n4. SentimentStore.GetSentiment() - Redis FCALL_RO\n\nCost breakdown:\n- 1 Redis HGET (config_json field)\n- 1 json.Unmarshal (~200-500 bytes)\n- 1 Redis FCALL_RO (Lua function call)\n\nAt scale:\n- 1K sessions: 20K config reads/sec + 20K unmarshal/sec\n- Config rarely changes (only on user save)\n- Same config read repeatedly every 50ms\n\nWhy this is wasteful:\n1. Config is read-heavy (20 reads/sec per session)\n2. Config is write-rare (maybe 1 update per hour)\n3. JSON unmarshaling is CPU-intensive (relative to needs)\n4. Redis bandwidth wasted on repeated config fetches\n\nCurrent caching: None\n- No in-memory config cache\n- No config change notifications\n- Every tick = full config read\n\nPotential optimizations:\n\nA. Local config cache per Broadcaster\n- Cache config in memory per session\n- TTL: 10 seconds (or indefinite)\n- Invalidate on config update\n- Reduces Redis calls by 95%\n\nB. Config snapshot in Engine\n- Pass ConfigSnapshot to Engine.GetCurrentValue\n- Caller (Broadcaster) maintains cache\n- Engine doesn't touch SessionRepo for config\n- Cleaner separation of concerns\n\nC. Config version tracking\n- Store config_version field in session hash\n- Cache config locally with version\n- Refetch only if version changed\n- Requires version increment on update\n\nD. Separate config from session\n- Store config:{userID} instead of in session hash\n- Multiple sessions share same config\n- Reduces duplication\n- Still need per-session cache for reads\n\nE. No caching (current)\n- Simple, always consistent\n- Works fine at low scale\n- Wasteful at high scale\n\nRecommendation:\nImplement local config cache with 10-second TTL (option A)\n\n```go\ntype configCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]*cacheEntry\n}\n\ntype cacheEntry struct {\n    config    domain.ConfigSnapshot\n    expiresAt time.Time\n}\n\nfunc (c *configCache) Get(sessionUUID) (*ConfigSnapshot, bool) {\n    // return cached if not expired\n}\n```\n\nBenefits:\n- Reduces Redis GET calls from 20/sec to 0.1/sec per session\n- Reduces JSON unmarshaling CPU by 99.5%\n- 10s staleness is acceptable (config changes are rare)\n- Simple to implement\n\nTrade-offs:\n- 10-second delay before config changes appear in overlay\n- Memory: ~200 bytes per cached config\n- At 1K sessions: ~200 KB memory (negligible)\n\nAlternative: Invalidate cache on config save\n- App.SaveConfig calls store.UpdateConfig\n- Broadcast invalidation signal to all Broadcasters\n- Requires pub/sub or shared cache invalidation key\n- More complex, not worth it for 10s delay\n\nPriority: P2 - significant CPU/bandwidth savings at scale\n","notes":"RESOLVED: This config re-fetch concern is addressed by the config caching solution (same root cause as twitch-tow-fc8).\n\n**Implemented in:**\n- **twitch-tow-h6x** Phase 2: Local Config Cache (week 2)\n- **twitch-tow-4c4** (Epic: Local Config Cache implementation details)\n\n**How the solution addresses this:**\nImplements exactly the recommendation from this discussion (option A):\n\n```go\ntype ConfigCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]*cacheEntry\n    ttl     time.Duration\n}\n\ntype cacheEntry struct {\n    config    domain.ConfigSnapshot\n    expiresAt time.Time\n}\n```\n\n**Benefits achieved:**\n- ✅ Reduces Redis GET calls from 20/sec to 0.1/sec per session (99.5% reduction)\n- ✅ Reduces JSON unmarshaling CPU by 99.5%\n- ✅ 10-second staleness acceptable (config changes are rare)\n- ✅ Memory: ~200 bytes per cached config × 1K sessions = ~200 KB (negligible)\n\n**Cache invalidation strategy:**\n- TTL-based: 10 seconds (simple, no pub/sub complexity)\n- Explicit invalidation on config save: `engine.InvalidateConfigCache(overlayUUID)`\n- Periodic eviction timer: 1-minute interval to prevent unbounded growth\n\nThe solution follows the recommendation from this discussion exactly. The alternative options (config version tracking, separate config storage) were considered but the simple TTL-based cache was chosen for implementation simplicity while achieving 99%+ hit rate.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:08:19.986411+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:36:19.548493+01:00","closed_at":"2026-02-12T17:36:19.548496+01:00"}
{"id":"twitch-tow-fc8","title":"Discussion: Broadcaster tick interval and Redis call frequency","description":"The Broadcaster pulls sentiment values from Redis every 50ms (tickInterval) by calling Engine.GetCurrentValue() for each active session. This creates a constant Redis query load.\n\n**Current implementation:**\n- Tick: 50ms (20 ticks/second)\n- Each tick: O(N) where N = local active sessions\n- Per session: 1 Redis HGET (2 fields: value, last_update) + Lua function call\n- Timeout: 2 seconds per session query (redisTimeout)\n\n**Load calculation examples:**\n- 10 local sessions: 200 Redis calls/sec\n- 100 local sessions: 2,000 Redis calls/sec  \n- 1,000 local sessions: 20,000 Redis calls/sec\n\n**Scalability implications:**\n1. Redis call rate scales with session count per instance\n2. 50ms tick = up to 50ms broadcast latency for vote changes\n3. Each tick queries ALL active sessions (no change detection)\n4. Lua function reads are marked 'no-writes' so they use FCALL_RO ✓\n5. Pull-based design = no Redis pub/sub overhead ✓\n\n**Trade-offs:**\n**Pros:**\n- Simple, stateless, no synchronization needed\n- Redis timeout prevents one slow session blocking others\n- Read-only Lua function is efficient\n- Works across multiple instances without coordination\n\n**Cons:**\n- Constant polling even if values haven't changed\n- Scales linearly with session count\n- Cannot reduce latency below tick interval\n- Higher Redis CPU usage compared to push model\n\n**Potential optimizations:**\n1. Increase tick interval (100ms = half the load, barely noticeable)\n2. Use Redis MGET to batch session reads (N calls → 1 call)\n3. Implement change detection (only broadcast on value change)\n4. Add adaptive tick rate (slow down when sessions idle)\n5. Return last_update from Lua function to detect stale sessions\n\n**Current bottleneck threshold:**\n- Redis can handle 100K+ ops/sec easily\n- Becomes bottleneck at ~5,000 local sessions per instance\n- At that scale, need horizontal scaling anyway\n\nPriority: P2 (medium) - fine for current scale, batching recommended above 1K sessions/instance","notes":"RESOLVED: This polling overhead concern is addressed by the config caching solution.\n\n**Implemented in:**\n- **twitch-tow-h6x** Phase 2: Local Config Cache (week 2)\n- **twitch-tow-4c4** (Epic: Local Config Cache implementation details)\n\n**How the solution addresses this:**\nThe config re-fetch problem identified in this discussion is solved by implementing a local in-memory cache:\n\n**Impact:**\n- Before: 1K sessions × 20 calls/sec = 20K config reads/sec\n- After: 1K sessions × 0.1 calls/sec = 100 config reads/sec  \n- **Reduction: 99.5% fewer Redis calls** (matches the \"batching recommended\" threshold in this discussion)\n\n**Implementation:**\n- Thread-safe ConfigCache with RWMutex\n- 10-second TTL (acceptable staleness for config changes)\n- Explicit invalidation on config save in dashboard\n- Periodic eviction to prevent memory leak\n- Memory overhead: ~200 bytes per config × 1K sessions = 200 KB\n\n**Trade-offs accepted:**\n- 10-second delay for config updates to appear in overlay (acceptable - config changes are rare)\n- 200 KB memory overhead (negligible)\n\nThe ticker interval remains 50ms as-is (not increased) because the config cache eliminates the primary overhead. No batching needed since cache hit rate \u003e99%.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:03.723632+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:36:08.257177+01:00","closed_at":"2026-02-12T17:36:08.257181+01:00"}
{"id":"twitch-tow-fgm","title":"Idea: Implement circuit breaker pattern for Redis operations","description":"## Proposal\nAdd circuit breaker pattern to Redis operations to prevent cascading failures when Redis is degraded or unavailable.\n\n## Implementation Approach\nUse sony/gobreaker or similar library to wrap Redis client operations:\n\n```go\ntype CircuitBreakerRedisClient struct {\n    client  *redis.Client\n    breaker *gobreaker.CircuitBreaker\n}\n\nfunc (c *CircuitBreakerRedisClient) Get(ctx context.Context, key string) (*redis.StringCmd, error) {\n    result, err := c.breaker.Execute(func() (interface{}, error) {\n        return c.client.Get(ctx, key).Result()\n    })\n    if err != nil {\n        return nil, err\n    }\n    return result.(*redis.StringCmd), nil\n}\n```\n\n## Circuit Breaker Configuration\n- **Failure threshold**: 5 consecutive failures\n- **Timeout**: 30 seconds (half-open after 30s)\n- **Success threshold (half-open)**: 2 consecutive successes to close\n- **Max requests (half-open)**: 1 (only allow 1 test request)\n\n## Fallback Behavior\nWhen circuit is open:\n1. **GetCurrentValue**: Return 0 (neutral value) or last known value (if cached)\n2. **ApplyVote**: Drop vote, increment metric (dropped_votes_total)\n3. **EnsureSessionActive**: Return error, prevent new WebSocket connections\n4. **UpdateConfig**: Return error, prevent config saves\n\n## Metrics\n- circuit_breaker_state (closed=0, open=1, half_open=2)\n- circuit_breaker_failures_total\n- circuit_breaker_successes_total\n- circuit_breaker_timeouts_total\n\n## Trade-offs\n**Pros:**\n- Fail fast when Redis is down (don't wait for timeout)\n- Prevent cascading failures (give Redis time to recover)\n- Improved user experience (error message vs. hang)\n\n**Cons:**\n- Added complexity (one more dependency)\n- False positives (transient blip could open circuit unnecessarily)\n- Need to tune thresholds per environment (dev vs. prod)\n\n## Related Issues\n- twitch-tow-9yg (Redis connection pooling)\n- twitch-tow-mxs (Broadcaster timeout handling)\n\n## Files to Modify\n- internal/redis/client.go (wrap client in circuit breaker)\n- internal/sentiment/engine.go (handle circuit open errors)\n- internal/broadcast/broadcaster.go (fallback on circuit open)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:07:06.071437+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:35:22.223637+01:00","closed_at":"2026-02-12T17:35:22.223637+01:00","close_reason":"Superseded by implementation epic twitch-tow-sb3 (Redis Circuit Breaker). Full PRD with task breakdown, code examples, acceptance criteria in epic. Consolidated into twitch-tow-h6x solution."}
{"id":"twitch-tow-g53","title":"Discussion: Template caching strategy needs documentation","description":"The Server struct caches parsed templates at startup (loginTemplate, dashboardTemplate, overlayTemplate), which is good for performance. However:\n\n**Concerns**:\n1. **No reload mechanism**: Template changes require full app restart (acceptable for production, painful for development)\n2. **Template errors at startup**: If templates fail to parse, app exits in NewServer (fail-fast is good, but no graceful degradation)\n3. **No hot-reload for dev**: The codebase doesn't distinguish dev vs prod behavior for templates\n4. **Template location hardcoded**: 'web/templates/' paths are hardcoded strings, no central constant\n\n**Current behavior**: Good for production (pre-parse once, fast runtime), but suboptimal DX.\n\n**Options**:\nA. Add dev mode template reloading (check APP_ENV, parse on each request if development)\nB. Keep current - argue that restart overhead is acceptable for template changes\nC. Add template path constants for better refactoring\nD. Document the trade-off explicitly\n\n**Note**: This is a DX concern, not a bug. Current approach is valid for production workloads.","notes":"RESOLVED: Merged into Epic 6 (twitch-tow-8dx) - Code Quality Improvements. Template caching trade-off documented in Task 5. Current approach (parse at startup) is appropriate for production.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:38.407452+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:48.93591+01:00","closed_at":"2026-02-12T17:37:48.935913+01:00"}
{"id":"twitch-tow-gse","title":"Fix whitespace validation inconsistency","description":"**Low Priority (Data Validation)**\n\nLocation: internal/server/handlers_dashboard.go lines 19-48\n\nIssue: Validation checks TrimSpace() for emptiness but uses raw len() for max length. User can submit 500 spaces.\n\nImpact: Users can submit invalid whitespace-only triggers.\n\nFix:\n- Trim first, then validate length and emptiness\n- Or: Check both raw and trimmed lengths","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:25.631406+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:37:21.268632+01:00","closed_at":"2026-02-12T16:37:21.268632+01:00","close_reason":"Closed","comments":[{"id":1,"issue_id":"twitch-tow-gse","author":"Patrick Scheid","text":"Enhanced fix: Added TrimSpace() in handleSaveConfig handler (line 104-107) to trim form values BEFORE validation and storage. This ensures trimmed values are actually saved to the database, not just validated. Added test case TestValidateConfig_WhitespaceOnlyTrigger to verify 500-space edge case is rejected.","created_at":"2026-02-12T15:40:00Z"}]}
{"id":"twitch-tow-h1k","title":"Discussion: Orphan cleanup timing and edge cases","description":"The orphan cleanup system (app/service.go) runs on a 30s ticker with specific timing constraints:\n\nConfiguration:\n- orphanMaxAge = 30s (session must be disconnected this long)\n- cleanupInterval = 30s (ticker frequency)\n- cleanupScanTimeout = 30s (max scan duration)\n\nCleanup flow:\n1. Every 30s CleanupOrphans() runs\n2. Scans Redis for sessions with last_disconnect \u003e 30s ago\n3. Deletes session from Redis\n4. Spawns background goroutine to unsubscribe from Twitch\n5. Background goroutine uses context.Background() (fire-and-forget)\n\nEdge case analysis:\n\nCASE 1 - Session flip-flops:\n- T=0: Client connects, session activated\n- T=10: Client disconnects, last_disconnect set\n- T=35: Cleanup scanner sees session (disconnected for 25s) - NOT cleaned (\u003c 30s)\n- T=40: Client reconnects, last_disconnect cleared to 0\n- T=65: Cleanup scanner sees session with last_disconnect=0 - NOT cleaned (active)\nResult: Correct behavior, no premature cleanup\n\nCASE 2 - Race between cleanup and reconnect:\n- T=0: Client disconnects, last_disconnect set\n- T=31: Cleanup scanner starts, finds session (disconnected 31s)\n- T=31.5: Client reconnects (while cleanup is processing)\n- T=32: Cleanup calls DeleteSession (deletes active session!)\nResult: Session deleted while client thinks it is connected. Next broadcast will fail. Client will see reconnecting status.\n\nCASE 3 - Slow cleanup scan:\n- T=0: 1000 sessions to scan\n- T=0: Cleanup starts with 30s timeout\n- T=30: Context cancelled after scanning 500 sessions\n- T=30: Returns orphans list with 50 sessions\n- T=31: Deletes 50 sessions, spawns background unsubscribe\n- T=60: Next cleanup starts (missed 500 sessions from previous scan)\nResult: Eventual consistency - missed sessions will be cleaned in next iteration\n\nCASE 2 is the critical race condition. Current fix:\n- IncrRefCount/DecrRefCount provide instance-level ref counting\n- Session only marked disconnected when ref_count reaches 0\n- But cleanup uses MarkDisconnected time, not ref count check\n\nPotential fix:\nAdd ref count check in DeleteSession before deleting. If ref_count \u003e 0 skip deletion.\n\nRecommendation: Document CASE 2 race condition. Add ref count validation to DeleteSession if this becomes a real issue. Monitor for disconnected session errors in logs.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:08:55.264758+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:36:22.910586+01:00","closed_at":"2026-02-12T17:36:22.910586+01:00","close_reason":"Superseded by implementation epic twitch-tow-bo6 (Robust Orphan Cleanup). Epic provides full solution with ref count validation, idempotent unsubscribe, metrics, testing strategy. Addresses CASE 2 race condition."}
{"id":"twitch-tow-h6x","title":"CONSOLIDATED: Redis Resilience - Circuit Breaker + Config Cache + Sentinel + Replicas","description":"# CONSOLIDATED SOLUTION: Redis Resilience Architecture\n\nThis consolidates three aligned proposals:\n- **twitch-tow-ojd** (Scalability - 4-phase comprehensive plan)\n- **twitch-tow-b9h** (Resilience - superior circuit breaker configuration)\n- **twitch-tow-hdl** (Maintainability - aligned, config cache emphasis)\n\n**Consensus:** All three architects voted +1. This merged solution adopts the best circuit breaker config from b9h and config cache strategy from hdl.\n\n---\n\n## Phase 1: Circuit Breaker Pattern (Week 1)\n**Priority: HIGH** - prevents Redis failures from cascading\n\n### Library \u0026 Configuration\n**Use:** `github.com/sony/gobreaker` v1.0.0\n\n**Configuration** *(adopted from twitch-tow-b9h - superior to consecutive errors)*:\n```go\nsettings := gobreaker.Settings{\n    Name:        \"redis\",\n    MaxRequests: 1,  // half-open: allow 1 test request\n    Interval:    10 * time.Second,  // sliding window for failure tracking\n    Timeout:     30 * time.Second,  // time in open state before half-open\n    ReadyToTrip: func(counts gobreaker.Counts) bool {\n        // Open circuit if 60% of requests fail over 10s window\n        failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)\n        return counts.Requests \u003e= 5 \u0026\u0026 failureRatio \u003e= 0.6\n    },\n    OnStateChange: func(name string, from, to gobreaker.State) {\n        slog.Warn(\"Circuit breaker state changed\",\n            \"component\", name,\n            \"from\", from.String(),\n            \"to\", to.String(),\n        )\n        metrics.CircuitBreakerStateChanges.WithLabelValues(name, to.String()).Inc()\n    },\n}\n```\n\n**Why this config is better:**\n- **Failure ratio (60%)** \u003e consecutive errors: More robust under variable load\n- **10s sliding window:** Fast detection but not too sensitive\n- **30s timeout:** Gives Redis time to recover\n- **1 max request in half-open:** Conservative test\n\n### Implementation\n```go\ntype CircuitBreakerClient struct {\n    client *redis.Client\n    cb     *gobreaker.CircuitBreaker\n    \n    // For fallback behavior\n    lastKnownValues map[string]cachedValue\n    mu              sync.RWMutex\n}\n\nfunc (c *CircuitBreakerClient) Get(ctx context.Context, key string) (string, error) {\n    result, err := c.cb.Execute(func() (interface{}, error) {\n        return c.client.Get(ctx, key).Result()\n    })\n    \n    if err == gobreaker.ErrOpenState {\n        // Circuit open, try fallback\n        if cached := c.getCached(key); cached != \"\" {\n            slog.Debug(\"Circuit open, returning cached value\", \"key\", key)\n            return cached, nil\n        }\n        return \"\", fmt.Errorf(\"redis unavailable and no cached value: %w\", err)\n    }\n    \n    if err == nil {\n        // Success, cache the value\n        c.setCached(key, result.(string))\n    }\n    \n    return result.(string), err\n}\n```\n\n### Graceful Degradation Strategy\nDifferent components handle open circuit differently:\n\n1. **Broadcaster (tick loop):** Return last known value (stale but acceptable, \u003c5 min)\n2. **Vote processing:** Log error, return 0 (votes are not critical)\n3. **Session activation:** Return error to client (trigger retry logic)\n4. **Config reads:** Return cached config (Phase 2 cache prevents this path)\n\n### Implementation Files\n- `internal/redis/circuit_breaker.go` (NEW - 300 lines)\n- `internal/redis/client.go` (wrap client, 15 lines)\n- `internal/redis/sentiment_store.go` (handle ErrOpenState, 10 lines)\n- `internal/metrics/metrics.go` (add circuit breaker metrics)\n\n### Testing\n- Unit tests with mock failing Redis (400 lines)\n- Integration test with real Redis failures (200 lines)\n- Chaos testing guide (docs)\n\n### Effort: 5 developer-days\n\n---\n\n## Phase 2: Local Config Cache (Week 2)\n**Priority: MEDIUM** - reduces Redis load by 95%\n\n*(Emphasized in twitch-tow-hdl maintainability proposal)*\n\n### Goal\nCache config in memory, reduce fetches from 20/sec to 0.1/sec per session\n\n### Implementation\n```go\ntype ConfigCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]*cacheEntry\n    ttl     time.Duration\n    clock   clockwork.Clock\n}\n\ntype cacheEntry struct {\n    config    domain.ConfigSnapshot\n    expiresAt time.Time\n}\n\nfunc (c *ConfigCache) Get(sessionUUID uuid.UUID) (*ConfigSnapshot, bool) {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[sessionUUID]\n    if !ok || c.clock.Now().After(entry.expiresAt) {\n        return nil, false\n    }\n    return \u0026entry.config, true\n}\n```\n\n**TTL:** 10 seconds (acceptable staleness)\n\n**Cache invalidation:**\n- TTL-based (simple, 10s delay for updates)\n- Explicit invalidation on config save in dashboard\n\n**Impact:**\n- Before: 1K sessions × 20 calls/sec = 20K config reads/sec\n- After: 1K sessions × 0.1 calls/sec = 100 config reads/sec\n- **Reduction: 99.5% fewer Redis calls**\n\n**Memory overhead:**\n- ~200 bytes per cached config\n- 1K sessions = 200 KB (negligible)\n\n### Implementation Files\n- `internal/sentiment/config_cache.go` (NEW - 200 lines)\n- `internal/sentiment/engine.go` (integrate cache, 30 lines)\n- `internal/app/service.go` (invalidate on save, 5 lines)\n- `cmd/server/main.go` (create cache, eviction timer, 10 lines)\n\n### Testing\n- Unit tests with fake clock (300 lines)\n- Integration test with broadcaster (150 lines)\n- Benchmark memory usage (100 lines)\n\n### Effort: 5 developer-days\n\n---\n\n## Phase 3: Redis Sentinel for HA (Week 3-4)\n**Priority: HIGH** - eliminates SPOF\n\n### Architecture\n```\n[App Instance 1] ──┐\n[App Instance 2] ──┼─→ [Sentinel 1] ─┐\n[App Instance N] ──┘   [Sentinel 2] ─┼─→ [Redis Master]\n                       [Sentinel 3] ─┘   [Redis Replica 1]\n                                          [Redis Replica 2]\n```\n\n### Setup\n1. Deploy 3 Redis Sentinel instances (HA quorum)\n2. Configure 1 master + 2 replicas\n3. Sentinels monitor master health\n4. Auto-failover on master failure (30s typical)\n\n### Application Changes\n```go\n// Instead of:\nopts, _ := redis.ParseURL(redisURL)\nclient := redis.NewClient(opts)\n\n// Use:\nclient := redis.NewFailoverClient(\u0026redis.FailoverOptions{\n    MasterName:    \"chatpulse-master\",\n    SentinelAddrs: []string{\n        \"sentinel1:26379\",\n        \"sentinel2:26379\",\n        \"sentinel3:26379\",\n    },\n    Password: cfg.RedisPassword,\n})\n```\n\n### Failover Behavior\n1. Master crashes\n2. Sentinels detect failure (10s)\n3. Quorum votes on new master\n4. Replica promoted to master (20s)\n5. Clients auto-reconnect to new master\n6. **Total downtime: 30-60 seconds**\n\n### Implementation Files\n- `internal/redis/client.go` (add Sentinel support, 30 lines)\n- `internal/config/config.go` (add Sentinel config fields, 10 lines)\n- `docs/deployment/redis-sentinel.md` (NEW - runbook)\n- `docker-compose.sentinel.yml` (NEW - local testing setup)\n\n### Testing\n- Integration test with Sentinel failover (docker-compose)\n- Chaos testing guide (kill master, verify recovery)\n\n### Effort: 10 developer-days (includes infrastructure setup)\n\n---\n\n## Phase 4: Read Replicas for Scale (Week 5-6)\n**Priority: MEDIUM** - scale reads beyond 200K ops/sec\n\n### Goal\nRoute read-only operations to replicas\n\n**Read operations (95% of load):**\n- Broadcaster ticks: GetSentiment (Lua FCALL_RO)\n- Config reads (if not cached)\n\n**Write operations (5% of load):**\n- Vote processing: ApplyVote (Lua FCALL)\n- Session lifecycle: ActivateSession, MarkDisconnected\n- Ref counting: INCR/DECR\n\n### Implementation\n```go\ntype SplitRedisClient struct {\n    master   *redis.Client  // writes + consistent reads\n    replicas []*redis.Client // read-only operations\n}\n\nfunc (s *SplitRedisClient) GetSentiment(ctx, sessionUUID) (float64, error) {\n    // Round-robin across replicas\n    replica := s.replicas[atomic.AddUint64(\u0026s.counter, 1) % len(s.replicas)]\n    return replica.FCallRO(ctx, \"get_decayed_value\", ...)\n}\n\nfunc (s *SplitRedisClient) ApplyVote(ctx, sessionUUID, delta) (float64, error) {\n    // Always use master for writes\n    return s.master.FCall(ctx, \"apply_vote\", ...)\n}\n```\n\n### Scaling Capacity\n- 1 master: 100K write ops/sec\n- 2 replicas: 200K read ops/sec each = 400K total read capacity\n- **Combined: 500K ops/sec (5x improvement)**\n\n### Replication Lag\n- Typical: 50-200ms\n- Max acceptable: 1 second\n- **Impact on overlay:** Acceptable (sentiment is not real-time critical)\n\n### Implementation Files\n- `internal/redis/split_client.go` (NEW - 200 lines)\n- `internal/redis/client.go` (create split client, 40 lines)\n- `internal/config/config.go` (add replica URLs config)\n\n### Testing\n- Integration test with replication lag simulation\n- Performance test measuring throughput improvement\n\n### Effort: 10 developer-days\n\n---\n\n## Total Implementation Timeline\n\n**Week 1:** Circuit breaker pattern (5 days)\n**Week 2:** Config caching (5 days)\n**Week 3-4:** Redis Sentinel deployment (10 days)\n**Week 5-6:** Read replica routing (10 days)\n\n**Total:** 30 developer-days (6 weeks)\n\nCan be parallelized in some areas.\n\n---\n\n## Success Criteria\n\n**Circuit Breaker:**\n- ✅ Redis failure doesn't crash application\n- ✅ Circuit opens after 60% failures over 10s window\n- ✅ Automatic recovery in 30s\n\n**Config Caching:**\n- ✅ Redis config reads reduced by 99%+\n- ✅ 10-second staleness acceptable\n- ✅ Memory overhead under 500 KB\n\n**Sentinel HA:**\n- ✅ Master failure auto-recovers in \u003c60s\n- ✅ Zero manual intervention required\n- ✅ No data loss (replication)\n\n**Read Replicas:**\n- ✅ Read capacity scaled 5x+ (500K ops/sec)\n- ✅ Replication lag under 200ms p99\n- ✅ Overlay staleness acceptable (\u003c1s)\n\n---\n\n## Cost Analysis\n\n**Infrastructure costs (AWS example):**\n- Current: 1 Redis (r6g.large) = $150/month\n- With Sentinel: 1 master + 2 replicas + 3 sentinels = $450/month\n- **Increase: $300/month**\n\n**Engineering costs:**\n- 30 developer-days implementation\n\n**ROI:**\n- Eliminates SPOF (prevents outages)\n- Enables horizontal scaling (10x capacity)\n- Reduces incident response time\n\n**Verdict:** Essential investment for production scale\n\n---\n\n## Dependencies\n**Adds:**\n- `github.com/sony/gobreaker` v1.0.0\n\n**Requires:**\n- Redis Sentinel infrastructure (Phase 3)\n- Redis replication setup (Phase 4)\n\n---\n\n## Supersedes\nThis consolidated solution replaces:\n- twitch-tow-ojd (scalability)\n- twitch-tow-b9h (resilience - circuit breaker config adopted)\n- twitch-tow-hdl (maintainability - config cache emphasis adopted)\n\nAll three architects voted +1. Merging into single authoritative plan with best-of-breed circuit breaker configuration from b9h.","status":"open","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:33:32.575601+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:33:32.575601+01:00","labels":["phase-3-consolidated"]}
{"id":"twitch-tow-hdl","title":"Solution: Redis resilience strategy","description":"SOLUTION PROPOSAL for Redis resilience concerns (twitch-tow-usj P1, plus 9 related P2 beads)\n\n## Problem Summary\nRedis is a single point of failure and bottleneck. Multiple architects identified concerns: SPOF, connection pooling, circuit breakers, Lua function failures, SCAN operations.\n\n## Proposed Solution: Layered Resilience Approach\n\n### Tier 1 - Immediate Wins (Week 1)\n1. Connection pool configuration\n   - Set explicit pool size limits (default is unlimited)\n   - Add connection health checks\n   - Configure timeouts (connect, read, write)\n   \n2. Circuit breaker pattern\n   - Use gobreaker or similar library\n   - Wrap Redis operations with circuit breaker\n   - Fail fast when Redis is down (avoid cascading failures)\n   - Graceful degradation: serve stale data or return cached values\n\n3. Better error handling\n   - Distinguish transient vs permanent failures\n   - Retry with exponential backoff for transient errors\n   - Log Redis errors with structured context\n\n### Tier 2 - Architectural Improvements (Week 2-3)\n1. Redis Sentinel for high availability\n   - Deploy 3-node Sentinel cluster\n   - Automatic failover (sub-second RTO)\n   - Sentinel handles master election\n   - go-redis client has native Sentinel support\n\n2. Read replicas for scaling\n   - Route read-only operations to replicas\n   - Writes still go to master\n   - Reduces load on master (broadcaster ticks are 90% reads)\n\n3. Lua function resilience\n   - Add error handling in Lua scripts\n   - Return error codes instead of crashing\n   - Test failure modes explicitly\n\n### Tier 3 - Future Scaling (Month 2+)\n1. Redis Cluster for horizontal scaling\n   - Shard sessions across multiple Redis nodes\n   - Requires key distribution strategy (hash slot based on overlay_uuid)\n   - More complex but handles 100k+ sessions\n\n2. Write-through cache for session data\n   - PostgreSQL as source of truth for session config\n   - Redis as cache layer\n   - Rebuild Redis state from PostgreSQL on failure\n\n## Recommended Immediate Actions\n\nPriority 1: Circuit breaker + connection pool config (1 week)\n- Prevents cascading failures\n- Low complexity, high impact\n- No architecture change required\n\nPriority 2: Redis Sentinel (2 weeks)\n- Solves SPOF concern\n- Industry-standard HA solution\n- Compatible with current architecture\n\nPriority 3: Read replicas (1 week after Sentinel)\n- Reduces load on master\n- Scales read-heavy workload (broadcaster ticks)\n\n## Trade-offs\n\nCircuit Breaker:\n- Pros: Fast failure, prevents cascades, simple\n- Cons: Need fallback strategy (serve stale data or return errors)\n\nRedis Sentinel:\n- Pros: Proven HA solution, automatic failover, minimal code changes\n- Cons: Operational complexity (3 processes to monitor), network split-brain scenarios\n\nRedis Cluster:\n- Pros: True horizontal scaling\n- Cons: High complexity, requires key redesign, connection pool changes\n\n## Estimated Effort\n- Circuit breaker: 8 hours\n- Connection pool tuning: 4 hours\n- Sentinel deployment: 16 hours (infra + testing)\n- Read replicas: 8 hours\n- Cluster migration: 80+ hours (major refactor)\n\n## Vote\n+1 from Maintainability architect for Tier 1 + Tier 2 (Circuit breaker + Sentinel). This balances quick wins with long-term reliability. Tier 3 can wait until we hit scale limits.","notes":"Vote: +1 from architect-scalability\n\nRATIONALE: Good 3-tier structure. Connection pool configuration is important addition. Emphasis on explicit pool sizes prevents default surprises.\n\nPROPOSAL: Merge into twitch-tow-ojd. I'll add Tier 1 connection pool config to my Phase 1. Your layered approach (Immediate/Architectural/Future) aligns with my 4-phase timeline.\n\nAGREEMENT: Defer Redis Cluster to Tier 3 (not needed until 500K+ ops/sec).","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:11:54.477847+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.12759+01:00","closed_at":"2026-02-12T17:56:57.12759+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-heu","title":"Discussion: Orphan cleanup robustness and error recovery","description":"## Issue\nThe orphan cleanup process (CleanupOrphans) runs every 30 seconds but lacks retry logic, error budgeting, and partial failure handling. Redis SCAN could timeout on large keyspaces.\n\n## Current State\n- CleanupOrphans runs on 30s ticker unconditionally (app/service.go:217-230)\n- SCAN operation has 30s timeout but no pagination limits (session_repository.go:178-208)\n- SCAN errors abort entire cleanup (returns early, line 193)\n- DeleteSession errors are logged but don't halt cleanup (line 194)\n- Twitch Unsubscribe runs in background goroutine (no timeout, no result tracking)\n- Background unsubscribe errors are logged but not retried (line 207-209)\n\n## Failure Modes\n1. **SCAN timeout**: 30s timeout could fire on \u003e10k keys (100 keys/batch * ~300ms/batch = 30s)\n2. **Partial cleanup**: Early SCAN error leaves some orphans unprocessed\n3. **Redis unavailability**: Cleanup fails, orphans accumulate, retry in 30s (no backoff)\n4. **Twitch API failures**: Unsubscribe failures leave dangling subscriptions on Twitch\n5. **No failure budget**: Cleanup runs every 30s regardless of consecutive failures\n6. **Goroutine leak risk**: Background unsubscribe goroutines could leak on panic\n\n## Risks\n- **Memory leak**: Failed cleanups allow orphan sessions to accumulate in Redis\n- **Cost leak**: Failed unsubscribes accumulate on Twitch (quota/billing impact)\n- **Redis memory pressure**: Large orphan accumulation could trigger eviction\n- **Twitch quota**: Orphan subscriptions count against EventSub limits (10k per app)\n\n## Suggestions\n1. Add failure budget (skip cleanup if last N attempts failed, exponential backoff)\n2. Paginate SCAN with cursor tracking (checkpoint progress, resume on next run)\n3. Add max keys per cleanup run (e.g., 1000) to bound execution time\n4. Track cleanup metrics (orphans found, deleted, failed, scan duration)\n5. Add separate retry queue for failed Unsubscribe calls (persistent, with backoff)\n6. Implement graceful degradation (continue cleanup even if Unsubscribe fails)\n7. Add manual cleanup endpoint for admin intervention\n8. Log orphan age distribution (helps tune orphanMaxAge threshold)\n\n## Files\n- internal/app/service.go:179-215 (CleanupOrphans)\n- internal/redis/session_repository.go:178-236 (ListOrphans + checkOrphan)\n- internal/app/service.go:217-230 (cleanup timer)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:06.378854+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:44:51.255954+01:00","closed_at":"2026-02-12T17:44:51.255954+01:00","close_reason":"Superseded by implementation epic twitch-tow-9kw (Orphan Cleanup Robustness). Epic provides failure budget with exponential backoff, SCAN pagination (1000 keys/run), unsubscribe retry queue, metrics, manual cleanup endpoint."}
{"id":"twitch-tow-hpr","title":"EPIC: Redis Memory Monitoring and Defensive TTLs for Session Keys","description":"## User Story\nAs an operator managing Redis memory usage, I want defensive TTLs on session keys and memory monitoring so orphaned sessions don't leak memory indefinitely and I have visibility into Redis memory consumption.\n\n## Problem Statement\n\nCurrent Redis key schema has excellent memory efficiency (\u003c10 MB for 10K sessions) but lacks defensive safeguards.\n\n**Current state:**\n- Session keys have NO TTL (rely on orphan cleanup)\n- Debounce keys have 1s TTL (good) ✓\n- Memory usage: ~400-700 bytes per session\n- Config JSON duplicated per session (could be normalized)\n\n**Risks:**\n1. If orphan cleanup fails, sessions leak indefinitely\n2. No Redis memory metrics exported\n3. No alerts on high memory usage\n4. No visibility into key distribution\n\n## Solution: Defensive TTLs + Monitoring\n\n### Implementation Tasks\n\n#### Task 1: Add defensive 24h TTL to session keys\n**File:** `internal/redis/session_repository.go`\n\n```go\nfunc (r *SessionRepo) ActivateSession(ctx, sessionUUID, broadcasterUserID, config) error {\n    pipe := r.client.Pipeline()\n    \n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    configJSON, _ := json.Marshal(config)\n    \n    pipe.HSet(ctx, key, map[string]interface{}{\n        \"value\":              0,\n        \"broadcaster_user_id\": broadcasterUserID,\n        \"config_json\":        configJSON,\n        \"last_update\":        r.clock.Now().UnixMilli(),\n        // last_disconnect is NOT set (session is active)\n    })\n    \n    // NEW: Add defensive 24h TTL\n    // If orphan cleanup fails, this prevents indefinite memory leak\n    pipe.Expire(ctx, key, 24*time.Hour)\n    \n    // Broadcaster mapping\n    broadcasterKey := fmt.Sprintf(\"broadcaster:%s\", broadcasterUserID)\n    pipe.Set(ctx, broadcasterKey, sessionUUID.String(), 0)\n    \n    // ... rest of activation\n    \n    _, err := pipe.Exec(ctx)\n    return err\n}\n```\n\n**Rationale:**\n- 24 hours is much longer than typical session lifetime (minutes to hours)\n- Active sessions get TTL refreshed on every vote or tick\n- Orphaned sessions expire after 24h (worst case)\n- Prevents indefinite memory leak if cleanup fails\n\n#### Task 2: Refresh TTL on session updates\n**File:** `internal/redis/sentiment_store.go`\n\n```go\nfunc (s *SentimentStore) ApplyVote(ctx, sessionUUID, delta, now) (float64, error) {\n    key := fmt.Sprintf(\"session:%s\", sessionUUID)\n    \n    result := s.client.FCall(ctx, \"apply_vote\",\n        []string{key},\n        now.UnixMilli(), delta,\n    )\n    \n    if err := result.Err(); err != nil {\n        return 0, err\n    }\n    \n    // NEW: Refresh TTL on vote (session is active)\n    s.client.Expire(ctx, key, 24*time.Hour)\n    \n    value, _ := result.Float64()\n    return value, nil\n}\n```\n\n**Alternative:** Refresh TTL in Lua script (more efficient)\n```lua\n-- In apply_vote function\nredis.call('EXPIRE', KEYS[1], 86400)  -- 24h in seconds\n```\n\n#### Task 3: Add Redis memory metrics\n**File:** `cmd/server/main.go`\n\n```go\nfunc main() {\n    // ... existing setup\n    \n    // Start Redis memory stats exporter\n    go exportRedisStats(ctx, redisClient, 30*time.Second)\n    \n    // ... rest of main\n}\n\nfunc exportRedisStats(ctx context.Context, rdb *redis.Client, interval time.Duration) {\n    ticker := time.NewTicker(interval)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ctx.Done():\n            return\n        case \u003c-ticker.C:\n            // Get Redis INFO memory\n            info, err := rdb.Info(ctx, \"memory\").Result()\n            if err != nil {\n                slog.Error(\"Failed to get Redis memory info\", \"error\", err)\n                continue\n            }\n            \n            // Parse INFO output\n            memStats := parseRedisInfo(info)\n            \n            metrics.RedisMemoryUsedBytes.Set(float64(memStats.UsedMemory))\n            metrics.RedisMemoryMaxBytes.Set(float64(memStats.MaxMemory))\n            metrics.RedisMemoryFragmentationRatio.Set(memStats.FragmentationRatio)\n            metrics.RedisKeysTotal.Set(float64(memStats.TotalKeys))\n            metrics.RedisKeysExpiring.Set(float64(memStats.ExpiringKeys))\n        }\n    }\n}\n\ntype redisMemStats struct {\n    UsedMemory          int64\n    MaxMemory           int64\n    FragmentationRatio  float64\n    TotalKeys           int64\n    ExpiringKeys        int64\n}\n\nfunc parseRedisInfo(info string) redisMemStats {\n    stats := redisMemStats{}\n    \n    for _, line := range strings.Split(info, \"\\r\\n\") {\n        if strings.HasPrefix(line, \"used_memory:\") {\n            fmt.Sscanf(line, \"used_memory:%d\", \u0026stats.UsedMemory)\n        } else if strings.HasPrefix(line, \"maxmemory:\") {\n            fmt.Sscanf(line, \"maxmemory:%d\", \u0026stats.MaxMemory)\n        } else if strings.HasPrefix(line, \"mem_fragmentation_ratio:\") {\n            fmt.Sscanf(line, \"mem_fragmentation_ratio:%f\", \u0026stats.FragmentationRatio)\n        }\n    }\n    \n    // Get key counts\n    // ... parse db0:keys=N,expires=M\n    \n    return stats\n}\n```\n\n#### Task 4: Add Redis metrics definitions\n**File:** `internal/metrics/metrics.go`\n\n```go\nRedisMemoryUsedBytes = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"redis_memory_used_bytes\",\n        Help: \"Redis memory usage in bytes\",\n    },\n)\n\nRedisMemoryMaxBytes = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"redis_memory_max_bytes\",\n        Help: \"Redis max memory configured in bytes\",\n    },\n)\n\nRedisMemoryFragmentationRatio = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"redis_memory_fragmentation_ratio\",\n        Help: \"Redis memory fragmentation ratio\",\n    },\n)\n\nRedisKeysTotal = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"redis_keys_total\",\n        Help: \"Total number of keys in Redis\",\n    },\n)\n\nRedisKeysExpiring = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"redis_keys_expiring\",\n        Help: \"Number of keys with TTL set\",\n    },\n)\n```\n\n#### Task 5: Add memory usage breakdown endpoint\n**File:** `internal/server/handlers_api.go`\n\n```go\n// Debug endpoint for Redis memory analysis\nfunc (s *Server) handleRedisMemoryAnalysis(c echo.Context) error {\n    if !s.isAuthenticated(c) {\n        return c.JSON(401, map[string]string{\"error\": \"unauthorized\"})\n    }\n    \n    ctx := c.Request().Context()\n    \n    // Count keys by prefix\n    analysis := map[string]interface{}{\n        \"sessions\":    countKeys(ctx, s.redis, \"session:*\"),\n        \"ref_counts\":  countKeys(ctx, s.redis, \"ref_count:*\"),\n        \"broadcasters\": countKeys(ctx, s.redis, \"broadcaster:*\"),\n        \"debounce\":    countKeys(ctx, s.redis, \"debounce:*\"),\n        \"config_cache\": countKeys(ctx, s.redis, \"config_cache:*\"),\n        \"disconnected\": s.redis.ZCard(ctx, \"disconnected_sessions\").Val(),\n        \"total_memory\": getMemoryUsage(ctx, s.redis),\n    }\n    \n    return c.JSON(200, analysis)\n}\n\nfunc countKeys(ctx context.Context, rdb *redis.Client, pattern string) int64 {\n    var count int64\n    iter := rdb.Scan(ctx, 0, pattern, 0).Iterator()\n    for iter.Next(ctx) {\n        count++\n    }\n    return count\n}\n```\n\n#### Task 6: Document TTL strategy\n**File:** `docs/architecture/redis-memory-management.md` (NEW)\n\n```markdown\n# Redis Memory Management\n\n## Key TTL Strategy\n\n### Session Keys (24h defensive TTL)\n- **Purpose:** Prevent indefinite memory leak if orphan cleanup fails\n- **Refresh:** Every vote application and broadcaster tick\n- **Rationale:** Active sessions constantly refresh, orphans expire\n\n### Debounce Keys (1s TTL)\n- **Purpose:** Temporary rate limiting\n- **Auto-expire:** No manual cleanup needed\n\n### Ref Count Keys (no TTL)\n- **Cleanup:** Deleted with session in DeleteSession()\n- **Risk:** If DeleteSession fails, key leaks\n- **Mitigation:** Consider adding 24h TTL\n\n### Config Cache Keys (24h TTL)\n- **Purpose:** Cold start optimization\n- **Invalidation:** Explicit on config save + TTL fallback\n\n## Memory Monitoring\n\n### Key Metrics\n- `redis_memory_used_bytes`: Total memory usage\n- `redis_keys_total`: Total key count\n- `redis_keys_expiring`: Keys with TTL set\n\n### Alerts\n- Memory usage \u003e80% of max\n- Fragmentation ratio \u003e1.5\n- Total keys \u003e100K (investigate)\n\n## Capacity Planning\n\n### Memory per Session\n- Session hash: ~200 bytes\n- Ref count: ~50 bytes\n- Broadcaster mapping: ~50 bytes\n- Config cache: ~200 bytes (optional)\n- **Total: ~500 bytes per session**\n\n### At Scale\n- 10K sessions = 5 MB\n- 100K sessions = 50 MB\n- 1M sessions = 500 MB\n\n### Redis Sizing\n- Small (\u003c10K sessions): 256 MB Redis\n- Medium (\u003c100K sessions): 2 GB Redis  \n- Large (\u003c1M sessions): 8 GB Redis\n```\n\n#### Task 7: Add alert rules\n**File:** `docs/deployment/prometheus-alerts.md`\n\n```yaml\ngroups:\n- name: redis_memory\n  rules:\n  - alert: RedisMemoryHigh\n    expr: redis_memory_used_bytes / redis_memory_max_bytes \u003e 0.8\n    for: 5m\n    annotations:\n      summary: \"Redis memory usage \u003e80%\"\n      \n  - alert: RedisMemoryFragmentation\n    expr: redis_memory_fragmentation_ratio \u003e 1.5\n    for: 10m\n    annotations:\n      summary: \"Redis memory fragmentation high\"\n      \n  - alert: RedisKeyCountHigh\n    expr: redis_keys_total \u003e 100000\n    for: 5m\n    annotations:\n      summary: \"Redis key count \u003e100K, investigate\"\n```\n\n#### Task 8: Unit tests\n**File:** `internal/redis/session_repository_ttl_test.go` (NEW)\n\nTest scenarios:\n1. ActivateSession sets 24h TTL on session key\n2. ApplyVote refreshes TTL\n3. TTL expires after 24h if no activity (use fake clock)\n4. Orphaned session is cleaned up by TTL\n\n#### Task 9: Integration test\n**File:** `internal/redis/memory_monitoring_integration_test.go` (NEW)\n\nTest with real Redis:\n1. Create 100 sessions\n2. Verify memory metrics exported\n3. Verify key counts accurate\n4. Test TTL expiration\n\n#### Task 10: Update CLAUDE.md\nDocument TTL strategy in ## Redis Architecture section.\n\n## Acceptance Criteria\n\n- ✅ Session keys have 24h defensive TTL\n- ✅ TTL refreshed on every vote and tick\n- ✅ Redis memory metrics exported every 30s\n- ✅ Memory breakdown endpoint for debugging\n- ✅ Documentation covers TTL strategy and capacity planning\n- ✅ Alert rules for memory \u003e80% and fragmentation \u003e1.5\n- ✅ Unit tests verify TTL behavior\n- ✅ Integration test validates memory monitoring\n\n## Files Created/Modified\n\n**New files:**\n- `internal/redis/session_repository_ttl_test.go` (200 lines)\n- `internal/redis/memory_monitoring_integration_test.go` (150 lines)\n- `docs/architecture/redis-memory-management.md` (300 lines)\n\n**Modified files:**\n- `internal/redis/session_repository.go` (add TTL to ActivateSession, 5 lines)\n- `internal/redis/sentiment_store.go` (refresh TTL in ApplyVote, 3 lines)\n- `cmd/server/main.go` (export Redis stats, 40 lines)\n- `internal/metrics/metrics.go` (add 5 Redis memory metrics, 40 lines)\n- `internal/server/handlers_api.go` (add memory analysis endpoint, 30 lines)\n- `CLAUDE.md` (document TTL strategy)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock Redis, verify TTL commands sent\n- Use fake clock to test expiration\n\n**Integration tests:**\n- Real Redis with testcontainers\n- Create sessions, verify TTL set\n- Advance time, verify TTL refresh\n- Wait 24h, verify expiration\n\n**Manual testing:**\n- Deploy to staging\n- Monitor memory metrics in Grafana\n- Create load, verify metrics accuracy\n\n## Dependencies\n- Requires Prometheus metrics (twitch-tow-kgj)\n- Complements orphan cleanup (existing)\n\n## Success Metrics\n- Zero indefinite memory leaks\n- Memory usage predictable (500 bytes/session)\n- Metrics provide visibility into Redis health\n- Alerts catch memory issues before impact\n\n## Effort Estimate\n**2 developer-days**\n\nBreakdown:\n- TTL implementation: 0.5 day\n- Memory metrics: 1 day\n- Testing: 0.5 day\n\n## Trade-offs\n\n**Pros:**\n+ Prevents indefinite memory leaks\n+ Visibility into Redis memory\n+ Capacity planning data\n+ Low implementation cost\n\n**Cons:**\n- 24h TTL means orphaned sessions take up to 24h to expire (vs 30s with cleanup)\n- Memory metrics add Redis INFO calls (lightweight)\n\n**Verdict:** Worth it for defensive protection\n\n## Risks \u0026 Mitigation\n- **Risk:** TTL refresh fails, active session expires\n  - **Mitigation:** 24h is very long, votes/ticks happen frequently\n  - **Mitigation:** Monitor expired keys metric\n- **Risk:** Memory metrics polling impacts Redis\n  - **Mitigation:** INFO command is lightweight, 30s interval is conservative","status":"open","priority":3,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:50:55.430196+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:49.182868+01:00"}
{"id":"twitch-tow-iaf","title":"Add tests for Redis Lua function error handling","description":"**Medium Priority (Testing)**\n\nLocation: internal/redis/sentiment_store_integration_test.go\n\nMissing: malformed Redis responses, ParseFloat failures, Lua function returning unexpected types, Redis connection failures during FCALL, error propagation.\n\nImpact: No verification that error handling works correctly. ParseFloat bug wouldn't be caught.\n\nFix:\n- Add test with mocked Redis client returning malformed data\n- Test ParseFloat error path explicitly\n- Verify error messages are clear and actionable","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:17.202896+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:39:20.165655+01:00","closed_at":"2026-02-12T16:39:20.165655+01:00","close_reason":"Redis Lua function error handling tests already added\n\nThese tests were added as part of fixing twitch-tow-wsw (ParseFloat error handling):\n\n1. TestApplyVote_InvalidRedisValue (lines 134-153)\n   - Corrupts Redis value with non-numeric data ('not-a-number')\n   - Verifies ApplyVote returns error when Lua function returns invalid float\n   - Asserts error message contains 'returned invalid float value'\n\n2. TestGetSentiment_InvalidRedisValue (lines 155-171)\n   - Corrupts Redis value with non-numeric data ('corrupted')\n   - Verifies GetSentiment returns error when Lua function returns invalid float\n   - Asserts error message contains 'returned invalid float value'\n\nBoth tests:\n- Use real Redis (integration tests via testcontainers)\n- Test the full error path: corrupted data → Lua function → ParseFloat failure → error propagation\n- Verify error messages are clear and actionable (include the invalid value)\n- Cover the exact scenario mentioned in the issue description\n\nThe ParseFloat bug mentioned would now be caught by these tests.\n\nFiles: internal/redis/sentiment_store_integration_test.go (already modified in twitch-tow-wsw)"}
{"id":"twitch-tow-jgs","title":"Discussion: Missing package-level documentation","description":"The codebase lacks package-level documentation comments:\n\n**Current state:**\n- Most packages have NO package-level doc comment\n- Individual types/functions are documented inconsistently\n- No godoc-friendly documentation\n\n**Package doc status (checked 11 packages):**\n- domain/: No package comment\n- app/: No package comment\n- broadcast/: No package comment  \n- config/: No package comment\n- crypto/: No package comment\n- database/: No package comment\n- redis/: No package comment\n- sentiment/: No package comment\n- server/: No package comment\n- twitch/: No package comment\n- logging/: No package comment\n\n**Impact:**\n1. godoc output is sparse - just shows types without context\n2. New contributors get no high-level overview from code\n3. CLAUDE.md contains excellent docs but not in godoc format\n4. IDE hover help shows no package-level info\n\n**Good documentation exists in:**\n- CLAUDE.md (comprehensive but external)\n- Code comments on complex functions (Lua scripts, actor pattern)\n- Interface method comments (inconsistent)\n\n**Options:**\nA. **Add package comments**: One-paragraph summary for each package\nB. **Generate from CLAUDE.md**: Extract relevant sections to package docs\nC. **Keep external**: Argue that CLAUDE.md is sufficient, godoc optional\nD. **Minimal approach**: Add package comments only for domain/ and app/\n\n**Recommendation**: Add minimal package-level comments (1-2 lines) to aid godoc navigation. Full architecture docs can stay in CLAUDE.md.","notes":"RESOLVED: Merged into Epic 6 (twitch-tow-8dx) - Code Quality Improvements. Package-level documentation implemented in Task 2 of epic (10 doc.go files).","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:13.987861+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:37:44.659186+01:00","closed_at":"2026-02-12T17:37:44.659189+01:00"}
{"id":"twitch-tow-k24","title":"EPIC: Optimize Broadcaster Performance - Parallel Redis Calls with Worker Pool","description":"## Epic Overview\nOptimize Broadcaster tick loop to handle 1,000+ sessions per instance by batching Redis calls and parallelizing session processing.\n\n## Parent Discussion\ntwitch-tow-bwp (Broadcaster actor single-goroutine bottleneck)\n\n## User Story\nAs an operator scaling to 1,000+ sessions per instance, I need the Broadcaster tick loop to efficiently query Redis for multiple sessions so tick latency remains under 50ms and doesn't degrade with session count.\n\n## Problem Analysis\n\n**Current bottleneck:**\n- Single goroutine processes sessions sequentially\n- Each session requires 1 Redis call (FCALL_RO get_decayed_value)\n- 1,000 sessions × 1ms Redis latency = 1 second per tick (unacceptable)\n- 2s timeout per session means worst case 2,000 seconds if all timeout\n\n**Current protection:**\n- ✅ 2s timeout per Redis call\n- ✅ 50ms tick interval (20 ticks/sec)\n- ✅ Non-blocking send to slow clients\n- ✅ Context cancellation checks\n\n**Scalability limit:**\n- Works fine for \u003c100 sessions (100ms tick latency)\n- Degrades at 500+ sessions (500ms+ tick latency)\n- Unacceptable at 1,000+ sessions (1s+ tick latency)\n\n## Solution: Batched Redis Queries\n\n### Option A: MGET Pattern (NOT applicable)\n- MGET only works for simple GET operations\n- Our Lua function (get_decayed_value) requires FCALL_RO\n- Cannot batch Lua function calls natively\n\n### Option B: Parallel Worker Pool ✅ (RECOMMENDED)\nExecute Redis calls in parallel with worker pool:\n\n```go\ntype sessionResult struct {\n    sessionUUID uuid.UUID\n    update      domain.SessionUpdate\n    err         error\n}\n\nfunc (b *Broadcaster) broadcastTick() {\n    sessions := b.getActiveSessions()\n    \n    // Create buffered results channel\n    results := make(chan sessionResult, len(sessions))\n    \n    // Use worker pool (10 goroutines)\n    var wg sync.WaitGroup\n    semaphore := make(chan struct{}, 10)  // Limit concurrency\n    \n    for _, sessionUUID := range sessions {\n        wg.Add(1)\n        go func(uuid uuid.UUID) {\n            defer wg.Done()\n            \n            semaphore \u003c- struct{}{}  // Acquire\n            defer func() { \u003c-semaphore }()  // Release\n            \n            update, err := b.engine.GetCurrentValue(ctx, uuid)\n            results \u003c- sessionResult{uuid, update, err}\n        }(sessionUUID)\n    }\n    \n    // Wait for all results\n    go func() {\n        wg.Wait()\n        close(results)\n    }()\n    \n    // Collect results and broadcast\n    for result := range results {\n        if result.err != nil {\n            slog.Warn(\"Failed to get session value\", \"session_uuid\", result.sessionUUID, \"error\", result.err)\n            continue\n        }\n        \n        b.broadcastToSession(result.sessionUUID, result.update)\n    }\n}\n```\n\n**Benefits:**\n- 10 parallel Redis calls instead of 1 sequential\n- 1,000 sessions / 10 workers = 100 sessions per worker\n- 100 sessions × 1ms = 100ms per worker\n- Total tick time: 100ms (10x improvement)\n\n**Trade-offs:**\n- More goroutines (10 per tick)\n- More complex code\n- Needs Redis connection pool tuning\n\n### Option C: Batching Lua Script (Complex)\nCreate new Lua function that accepts multiple session UUIDs:\n```lua\nfunction batch_get_decayed_values(keys, args)\n    local results = {}\n    for i, key in ipairs(keys) do\n        -- Call get_decayed_value for each session\n        results[i] = get_decayed_value({key}, {args[1]})\n    end\n    return results\nend\n```\n\n**Benefits:**\n- Single Redis round-trip\n- Lower network overhead\n\n**Trade-offs:**\n- More complex Lua code\n- Harder to debug\n- Lua script timeout applies to all sessions\n- Large batches could timeout (2s limit)\n\n**Verdict:** Option B (worker pool) is simpler and more robust\n\n## Implementation Tasks\n\n### Task 1: Add worker pool to Broadcaster\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\ntype Broadcaster struct {\n    // ... existing fields\n    \n    // NEW: Worker pool configuration\n    numWorkers       int  // default: 10\n    maxConcurrentOps int  // default: 20\n}\n\nfunc NewBroadcaster(engine, onFirstClient, onSessionEmpty, clock, numWorkers) *Broadcaster {\n    if numWorkers \u003c= 0 {\n        numWorkers = 10  // default\n    }\n    \n    return \u0026Broadcaster{\n        // ... existing init\n        numWorkers: numWorkers,\n    }\n}\n```\n\n### Task 2: Implement parallel broadcastTick\n**File:** `internal/broadcast/broadcaster.go`\n\nReplace serial loop with parallel execution (see Option B code above).\n\nAdd metrics:\n```go\nmetrics.BroadcasterTickConcurrency.Set(float64(len(sessions)))\nmetrics.BroadcasterWorkerPoolDepth.Observe(float64(len(semaphore)))\n```\n\n### Task 3: Configure worker pool size\n**File:** `internal/config/config.go`\n\n```go\ntype Config struct {\n    // ... existing fields\n    \n    BroadcasterWorkers int `env:\"BROADCASTER_WORKERS\" default:\"10\"`\n}\n```\n\n**File:** `.env.example`\n```bash\n# Broadcaster worker pool for parallel Redis calls\nBROADCASTER_WORKERS=10  # Number of concurrent Redis operations per tick\n```\n\n### Task 4: Tune Redis connection pool\n**File:** `internal/redis/client.go`\n\nEnsure Redis pool can handle concurrent operations:\n```go\nopts.PoolSize = 20  // Must be \u003e= BROADCASTER_WORKERS * num_instances\nopts.MinIdleConns = 5\nopts.PoolTimeout = 3 * time.Second\n```\n\n### Task 5: Add metrics for tick performance\n**File:** `internal/metrics/metrics.go`\n\n```go\nBroadcasterTickSessions = promauto.NewHistogram(\n    prometheus.HistogramOpts{\n        Name: \"broadcaster_tick_sessions\",\n        Help: \"Number of sessions processed per tick\",\n        Buckets: []float64{1, 10, 50, 100, 500, 1000, 5000},\n    },\n)\n\nBroadcasterParallelismUtilization = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"broadcaster_parallelism_utilization\",\n        Help: \"Percentage of worker pool in use\",\n    },\n)\n```\n\n### Task 6: Unit tests\n**File:** `internal/broadcast/broadcaster_parallel_test.go` (NEW)\n\nTest scenarios:\n1. **10 sessions**: Verify all processed in single tick\n2. **100 sessions**: Verify parallelism improves latency\n3. **1,000 sessions**: Verify tick completes in \u003c200ms\n4. **Redis timeout**: Verify one slow session doesn't block others\n5. **Worker pool saturation**: Verify semaphore limits concurrency\n\nUse fake clock and mock engine to control timing.\n\n### Task 7: Integration test with real Redis\n**File:** `internal/broadcast/broadcaster_parallel_integration_test.go` (NEW)\n\nLoad test:\n1. Create 1,000 sessions in testcontainers Redis\n2. Start Broadcaster with 10 workers\n3. Measure tick duration for 10 ticks\n4. Assert p99 tick latency \u003c200ms\n\n### Task 8: Documentation\n**File:** `docs/architecture/broadcaster-scaling.md` (NEW)\n\nDocument:\n- Worker pool architecture\n- Tuning BROADCASTER_WORKERS parameter\n- Redis pool sizing recommendations\n- Expected tick latency by session count\n- Monitoring tick performance metrics\n\n## Acceptance Criteria\n\n- ✅ Broadcaster processes 1,000 sessions in \u003c200ms per tick (p99)\n- ✅ Worker pool limits concurrent Redis operations to configured value\n- ✅ One slow/failed session doesn't block others\n- ✅ Metrics track parallelism utilization\n- ✅ Unit tests verify parallel behavior with mocks\n- ✅ Integration test with 1,000 real sessions passes\n- ✅ Redis connection pool sized appropriately\n- ✅ Documentation covers scaling guidelines\n\n## Files Created/Modified\n\n**New files:**\n- `internal/broadcast/broadcaster_parallel_test.go` (300 lines)\n- `internal/broadcast/broadcaster_parallel_integration_test.go` (200 lines)\n- `docs/architecture/broadcaster-scaling.md` (300 lines)\n\n**Modified files:**\n- `internal/broadcast/broadcaster.go` (refactor broadcastTick, add 80 lines)\n- `internal/config/config.go` (add BROADCASTER_WORKERS field)\n- `internal/redis/client.go` (tune connection pool settings, 10 lines)\n- `internal/metrics/metrics.go` (add tick parallelism metrics, 20 lines)\n- `.env.example` (document BROADCASTER_WORKERS)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock engine with controlled latency\n- Verify parallel execution (multiple sessions queried concurrently)\n- Test worker pool semaphore limits\n- Test error handling (one session fails, others succeed)\n\n**Integration tests:**\n- Real Redis with 1,000 sessions\n- Measure actual tick latency\n- Verify no Redis connection exhaustion\n- Test with different worker pool sizes (5, 10, 20)\n\n**Load tests:**\n- Benchmark tick latency by session count (100, 500, 1000, 5000)\n- Compare serial vs parallel performance\n- Measure Redis CPU usage\n\n## Dependencies\n- Requires prometheus metrics (twitch-tow-kgj)\n- Complements config cache (twitch-tow-4c4) for maximum performance\n\n## Success Metrics\n- 10x improvement in tick latency at 1,000 sessions\n- Linear scaling: doubling workers halves latency\n- No degradation in client fan-out performance\n- Redis connection pool stable under load\n\n## Effort Estimate\n**5 developer-days** (1 week)\n\nBreakdown:\n- Worker pool implementation: 2 days\n- Unit tests: 1 day\n- Integration + load tests: 1 day\n- Documentation: 1 day\n\n## Risk Mitigation\n- **Risk:** Worker pool adds too many goroutines\n  - **Mitigation:** Semaphore limits concurrency, configurable worker count\n- **Risk:** Redis connection pool exhausted\n  - **Mitigation:** Pool size \u003e= workers × instances, monitor pool metrics\n- **Risk:** Complex code harder to debug\n  - **Mitigation:** Comprehensive tests, metrics for parallelism utilization","status":"open","priority":3,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:39:56.516997+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:52.508922+01:00"}
{"id":"twitch-tow-kgj","title":"EPIC: Phase 2 Prometheus Metrics - Implement Comprehensive Application Metrics","description":"## Epic Overview\nImplement production-ready Prometheus metrics covering all critical components: Redis operations, WebSocket lifecycle, vote processing, broadcaster performance, and database queries.\n\n## User Story\nAs an operator, I need detailed metrics exported in Prometheus format so I can monitor application health, detect performance issues, configure alerts, and perform capacity planning.\n\n## Parent Solution\ntwitch-tow-eyl (Comprehensive Observability)\n\n## Dependencies\n- twitch-tow-682 (Phase 1 Health Checks) - should be complete first\n\n## Technical Requirements\n\n### Metrics Categories\n\n#### 1. Redis Operations Metrics\n```go\nredis_operations_total{operation, status} // counter\nredis_operation_duration_seconds{operation} // histogram\nredis_connection_errors_total // counter\nredis_pool_connections_current{state} // gauge (active/idle)\n```\n\nOperations to track: Get, Set, FCall, FCallRO, Incr, Decr, Expire, HGet, HSet\n\n#### 2. Broadcaster Metrics  \n```go\nbroadcaster_active_sessions // gauge\nbroadcaster_connected_clients_total // gauge\nbroadcaster_tick_duration_seconds // histogram\nbroadcaster_slow_clients_evicted_total // counter\nbroadcaster_broadcast_duration_seconds{session} // histogram\n```\n\n#### 3. WebSocket Metrics\n```go\nwebsocket_connections_current // gauge\nwebsocket_connections_total{result} // counter (success/error/rejected)\nwebsocket_message_send_duration_seconds // histogram\nwebsocket_connection_duration_seconds // histogram\nwebsocket_ping_failures_total // counter\n```\n\n#### 4. Vote Processing Metrics\n```go\nvote_processing_total{result} // counter (applied/debounced/invalid/no_session)\nvote_processing_duration_seconds // histogram\nvote_trigger_matches_total{trigger_type} // counter (for/against)\n```\n\n#### 5. Database Metrics\n```go\ndb_query_duration_seconds{query} // histogram\ndb_connections_current{state} // gauge (active/idle)\ndb_errors_total{query} // counter\n```\n\n#### 6. HTTP Request Metrics (Echo middleware)\n```go\nhttp_requests_total{method, path, status} // counter\nhttp_request_duration_seconds{method, path} // histogram\n```\n\n### Total Metrics Count\n- 6 categories\n- 22 unique metrics\n- 50+ time series with labels\n\n## Implementation Tasks\n\n### Task 1: Create metrics package\n**File:** `internal/metrics/metrics.go` (NEW)\n```go\npackage metrics\n\nimport (\n    \"github.com/prometheus/client_golang/prometheus\"\n    \"github.com/prometheus/client_golang/prometheus/promauto\"\n)\n\nvar (\n    // Redis metrics\n    RedisOpsTotal = promauto.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"redis_operations_total\",\n            Help: \"Total Redis operations by operation and status\",\n        },\n        []string{\"operation\", \"status\"},\n    )\n    \n    RedisOpDuration = promauto.NewHistogramVec(\n        prometheus.HistogramOpts{\n            Name: \"redis_operation_duration_seconds\",\n            Help: \"Redis operation duration in seconds\",\n            Buckets: []float64{.001, .005, .01, .025, .05, .1, .25, .5, 1},\n        },\n        []string{\"operation\"},\n    )\n    \n    // ... define all 22 metrics\n)\n```\n\n### Task 2: Instrument Redis client\n**File:** `internal/redis/instrumented_client.go` (NEW)\n```go\ntype InstrumentedClient struct {\n    client *redis.Client\n}\n\nfunc (c *InstrumentedClient) Get(ctx context.Context, key string) *redis.StringCmd {\n    start := time.Now()\n    result := c.client.Get(ctx, key)\n    \n    status := \"success\"\n    if result.Err() != nil {\n        status = \"error\"\n        metrics.RedisOpsTotal.WithLabelValues(\"get\", \"error\").Inc()\n    } else {\n        metrics.RedisOpsTotal.WithLabelValues(\"get\", \"success\").Inc()\n    }\n    \n    metrics.RedisOpDuration.WithLabelValues(\"get\").Observe(time.Since(start).Seconds())\n    return result\n}\n\n// Wrap all Redis methods: FCall, FCallRO, HGet, HSet, Incr, Decr, Expire\n```\n\n**Alternative approach:** Use Redis hooks\n```go\nclient.AddHook(\u0026MetricsHook{})\n\ntype MetricsHook struct{}\n\nfunc (h *MetricsHook) DialHook(next redis.DialHook) redis.DialHook {\n    return next\n}\n\nfunc (h *MetricsHook) ProcessHook(next redis.ProcessHook) redis.ProcessHook {\n    return func(ctx context.Context, cmd redis.Cmder) error {\n        start := time.Now()\n        err := next(ctx, cmd)\n        \n        status := \"success\"\n        if err != nil {\n            status = \"error\"\n        }\n        \n        metrics.RedisOpsTotal.WithLabelValues(cmd.Name(), status).Inc()\n        metrics.RedisOpDuration.WithLabelValues(cmd.Name()).Observe(time.Since(start).Seconds())\n        \n        return err\n    }\n}\n```\n**Recommendation:** Use hooks (cleaner, automatic coverage)\n\n### Task 3: Instrument Broadcaster\n**File:** `internal/broadcast/broadcaster.go`\nAdd metrics collection to:\n- `run()` tick loop: measure tick duration\n- `Register()`: increment active clients gauge\n- `Unregister()`: decrement active clients gauge, record connection duration\n- Slow client eviction: increment counter\n\n```go\nfunc (b *Broadcaster) run() {\n    ticker := time.NewTicker(50 * time.Millisecond)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ticker.C:\n            start := time.Now()\n            b.broadcastTick()\n            metrics.BroadcasterTickDuration.Observe(time.Since(start).Seconds())\n            metrics.BroadcasterActiveSessions.Set(float64(len(b.activeClients)))\n```\n\n### Task 4: Instrument WebSocket handlers\n**File:** `internal/server/handlers_overlay.go`\nTrack:\n- Connection duration (defer statement with start time)\n- Send duration (in clientWriter)\n- Ping/pong failures\n\n```go\nfunc (s *Server) handleWebSocket(c echo.Context) error {\n    start := time.Now()\n    defer func() {\n        metrics.WebSocketConnectionDuration.Observe(time.Since(start).Seconds())\n        metrics.WebSocketConnectionsCurrent.Dec()\n    }()\n    \n    metrics.WebSocketConnectionsCurrent.Inc()\n    metrics.WebSocketConnectionsTotal.WithLabelValues(\"success\").Inc()\n    \n    // ... existing logic\n}\n```\n\n### Task 5: Instrument vote processing\n**File:** `internal/sentiment/engine.go`\nAdd metrics to `ProcessVote()`:\n```go\nfunc (e *Engine) ProcessVote(ctx, broadcasterUserID, chatterUserID, message) (float64, bool) {\n    start := time.Now()\n    defer func() {\n        metrics.VoteProcessingDuration.Observe(time.Since(start).Seconds())\n    }()\n    \n    // ... existing logic\n    \n    if delta != 0 {\n        metrics.VoteTriggerMatches.WithLabelValues(triggerType).Inc()\n    }\n    \n    if !allowed {\n        metrics.VoteProcessingTotal.WithLabelValues(\"debounced\").Inc()\n        return 0, false\n    }\n    \n    metrics.VoteProcessingTotal.WithLabelValues(\"applied\").Inc()\n    return newValue, true\n}\n```\n\n### Task 6: Instrument database queries\n**File:** `internal/database/instrumented_pool.go` (NEW)\nWrap pgxpool with metrics:\n```go\ntype InstrumentedPool struct {\n    pool *pgxpool.Pool\n}\n\nfunc (p *InstrumentedPool) Query(ctx, sql, args...) (pgx.Rows, error) {\n    start := time.Now()\n    rows, err := p.pool.Query(ctx, sql, args...)\n    \n    queryName := extractQueryName(sql) // \"GetUserByID\", \"UpsertUser\"\n    status := \"success\"\n    if err != nil {\n        status = \"error\"\n        metrics.DBErrorsTotal.WithLabelValues(queryName).Inc()\n    }\n    \n    metrics.DBQueryDuration.WithLabelValues(queryName).Observe(time.Since(start).Seconds())\n    return rows, err\n}\n```\n\n**Alternative:** Use pgx query tracer (built-in)\n```go\nconfig.ConnConfig.Tracer = \u0026MetricsTracer{}\n```\n\n### Task 7: Add HTTP middleware\n**File:** `internal/server/server.go`\nAdd Prometheus middleware from Echo community:\n```go\nimport \"github.com/labstack/echo-contrib/echoprometheus\"\n\ne.Use(echoprometheus.NewMiddleware(\"chatpulse\"))\n```\n\nThis automatically adds:\n- `http_requests_total{method, path, status}`\n- `http_request_duration_seconds{method, path}`\n\n### Task 8: Add /metrics endpoint\n**File:** `internal/server/routes.go`\n```go\nimport \"github.com/prometheus/client_golang/prometheus/promhttp\"\n\ne.GET(\"/metrics\", echo.WrapHandler(promhttp.Handler()))\n```\n\nNo authentication required (standard practice, or add basic auth if needed)\n\n### Task 9: Unit tests for metrics\n**File:** `internal/metrics/metrics_test.go`\nTest that:\n- All metrics are registered without conflicts\n- Label cardinality is reasonable\n- Histogram buckets are appropriate\n\n### Task 10: Integration test for metrics export\n**File:** `internal/server/handlers_metrics_integration_test.go`\nTest scenario:\n1. Start server\n2. Make WebSocket connection\n3. Process vote via webhook\n4. Query /metrics endpoint\n5. Verify expected metrics are present:\n   - `websocket_connections_current \u003e 0`\n   - `vote_processing_total{result=\"applied\"} \u003e 0`\n   - `redis_operations_total \u003e 0`\n\n### Task 11: Documentation\n**File:** `docs/deployment/prometheus-metrics.md` (NEW)\nDocument:\n- All 22 metrics with descriptions\n- Recommended alert rules\n- Example Prometheus scrape config\n- Grafana dashboard JSON (optional)\n- Capacity planning guidelines using metrics\n\nExample alert rules:\n```yaml\ngroups:\n- name: chatpulse\n  rules:\n  - alert: HighErrorRate\n    expr: rate(redis_operations_total{status=\"error\"}[5m]) \u003e 0.05\n    for: 5m\n    annotations:\n      summary: \"High Redis error rate\"\n  \n  - alert: SlowBroadcasterTicks\n    expr: histogram_quantile(0.99, broadcaster_tick_duration_seconds) \u003e 0.1\n    for: 5m\n    annotations:\n      summary: \"Broadcaster ticks taking \u003e100ms\"\n```\n\n### Task 12: Update CLAUDE.md\nAdd /metrics endpoint to ## Key Routes section\n\n## Acceptance Criteria\n\n- ✅ All 22 metrics implemented across 6 categories\n- ✅ GET /metrics returns Prometheus text format\n- ✅ Metrics have appropriate labels (not too high cardinality)\n- ✅ Histogram buckets cover expected ranges\n- ✅ Integration test verifies metrics are updated\n- ✅ Documentation includes alert examples\n- ✅ No performance regression (metrics collection \u003c2% overhead)\n\n## Files Created/Modified\n\n**New files:**\n- `internal/metrics/metrics.go` (300 lines - all metric definitions)\n- `internal/redis/instrumented_client.go` (200 lines - or use hooks)\n- `internal/database/instrumented_pool.go` (150 lines - or use tracer)\n- `internal/metrics/metrics_test.go` (100 lines)\n- `internal/server/handlers_metrics_integration_test.go` (150 lines)\n- `docs/deployment/prometheus-metrics.md` (500 lines comprehensive docs)\n\n**Modified files:**\n- `internal/broadcast/broadcaster.go` (add 10 lines for metrics)\n- `internal/server/handlers_overlay.go` (add 15 lines for WS metrics)\n- `internal/sentiment/engine.go` (add 20 lines for vote metrics)\n- `internal/server/routes.go` (add /metrics endpoint)\n- `internal/server/server.go` (add Echo Prometheus middleware)\n- `go.mod` (add prometheus/client_golang, echo-contrib/echoprometheus)\n- `CLAUDE.md` (document /metrics endpoint)\n\n## Testing Strategy\n\n**Unit tests:**\n- Verify metric registration\n- Test label cardinality limits\n- Test histogram bucket ranges\n\n**Integration tests:**\n- Full request flow (WS connection + vote processing)\n- Scrape /metrics endpoint\n- Parse Prometheus text format\n- Verify expected metrics present with correct values\n\n**Performance tests:**\n- Measure overhead of metrics collection\n- Target: \u003c2% CPU overhead\n- Target: \u003c50 MB memory overhead for 10K active sessions\n\n**Manual testing:**\n- Deploy to staging\n- Configure Prometheus scraper\n- Verify metrics appear in Prometheus UI\n- Create test Grafana dashboard\n- Trigger alerts manually\n\n## Dependencies\n- **Blocks:** twitch-tow-682 (health checks should be done first)\n- **Required:** prometheus/client_golang v1.20+\n- **Optional:** labstack/echo-contrib/echoprometheus v4+\n\n## Success Metrics\n- 22+ metrics exported successfully\n- Metrics collection overhead \u003c2% CPU\n- Prometheus successfully scrapes /metrics every 15s\n- Can create meaningful Grafana dashboard from metrics\n- Alert rules detect real issues (test by killing Redis)\n\n## Effort Estimate\n**10 developer-days** (2 weeks)\n\nBreakdown:\n- Metrics package + definitions: 1 day\n- Redis instrumentation: 1 day  \n- Broadcaster instrumentation: 1 day\n- WebSocket instrumentation: 1 day\n- Vote processing instrumentation: 1 day\n- Database instrumentation: 1 day\n- HTTP middleware: 0.5 day\n- Tests: 2 days\n- Documentation + alert rules: 1.5 days\n\n## Risk Mitigation\n- **Risk:** Metrics collection adds too much overhead\n  - **Mitigation:** Use sampling for high-frequency operations (every 10th tick)\n  - **Mitigation:** Benchmark before/after, reject if \u003e2% overhead\n- **Risk:** Label cardinality explosion (session_uuid in labels)\n  - **Mitigation:** Never use session_uuid as label, only in logs\n  - **Mitigation:** Limit labels to bounded sets (operation names, status codes)\n- **Risk:** Histogram buckets don't match actual latencies\n  - **Mitigation:** Test with production-like load, adjust buckets\n  - **Mitigation:** Start with wide range, refine after data collected","status":"in_progress","priority":1,"issue_type":"epic","assignee":"dev-infrastructure","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:25:57.03362+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:07:04.009979+01:00"}
{"id":"twitch-tow-kn7","title":"Positive: Clean separation of concerns across layers","description":"The codebase demonstrates excellent layered architecture with clear separation of concerns:\n\nLayer 1 - Domain (no internal dependencies):\n- Pure interfaces and types\n- No coupling to infrastructure\n- Enforces dependency inversion principle\n\nLayer 2 - Infrastructure:\n- database/ implements domain repositories with PostgreSQL\n- redis/ implements domain repositories with Redis\n- crypto/ provides encryption abstraction\n- config/ handles environment loading\n- logging/ wraps slog\n\nLayer 3 - Business logic:\n- sentiment/ implements Engine interface (orchestrates SessionRepo + SentimentStore + Debouncer)\n- app/ implements AppService (orchestrates all repositories and Engine)\n\nLayer 4 - Transport:\n- server/ handles HTTP/WebSocket\n- broadcast/ handles WebSocket fanout\n- twitch/ handles EventSub webhooks\n\nDependency flow is strictly upward - no circular dependencies detected.\n\nKey strengths:\n1. Domain package is truly agnostic - could swap Postgres for MySQL, Redis for Memcached without changing domain layer\n2. Each package has single responsibility - broadcast doesnt know about HTTP, server doesnt know about Redis internals\n3. Interfaces are consumer-side - defined where they are used not where implemented\n4. Testing is easy - can test sentiment.Engine with mocks without spinning up Redis\n\nMinor observation:\n- app/ package is slightly fat (orchestrates everything) but this is appropriate for application layer\n- main.go composition root is explicit and readable\n\nVerdict: This is textbook Clean Architecture. Well done.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:09:13.957194+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:02.747481+01:00","closed_at":"2026-02-12T17:57:02.747481+01:00","close_reason":"Positive architectural notes - no action required"}
{"id":"twitch-tow-krx","title":"EPIC: Broadcaster Actor Resilience - Timeout Protection and Deadlock Prevention","description":"## User Story\nAs a developer maintaining the Broadcaster actor, I want comprehensive timeout protection and deadlock prevention so the system remains stable even when Redis is slow or commands back up.\n\n## Problem Statement\n\nThe Broadcaster actor has some timeout protection but lacks comprehensive safeguards against deadlocks and resource leaks.\n\n**Current protections:**\n- ✅ Command channel buffered (cap 256)\n- ✅ Register/GetClientCount have 5s timeout\n- ✅ Stop() has 10s timeout\n- ✅ Tick loop uses 2s Redis timeout per session\n\n**Missing protections:**\n- ❌ No panic recovery in actor goroutine\n- ❌ No command channel depth metrics\n- ❌ Stop() doesn't force-exit (goroutine could leak)\n- ❌ onFirstClient callback blocks Register command\n- ❌ No circuit breaker for consistently slow sessions\n- ❌ No max tick duration budget\n\n## Implementation Tasks\n\n### Task 1: Add panic recovery wrapper\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\nfunc (b *Broadcaster) run() {\n    defer func() {\n        if r := recover(); err != nil {\n            slog.Error(\"Broadcaster panic recovered\",\n                \"panic\", r,\n                \"stack\", string(debug.Stack()),\n            )\n            metrics.BroadcasterPanicsTotal.Inc()\n            \n            // Attempt graceful cleanup\n            b.closeAllClients(\"broadcaster panic\")\n        }\n    }()\n    \n    ticker := b.clock.NewTicker(b.tickInterval)\n    defer ticker.Stop()\n    \n    // ... existing run loop\n}\n```\n\n**Acceptance criteria:**\n- Panic in tick loop doesn't crash process\n- Panic details logged with stack trace\n- Metric incremented on panic\n\n### Task 2: Add command channel depth metric\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\nfunc (b *Broadcaster) run() {\n    ticker := b.clock.NewTicker(b.tickInterval)\n    defer ticker.Stop()\n    \n    // Track command channel depth every tick\n    depthTicker := b.clock.NewTicker(1 * time.Second)\n    defer depthTicker.Stop()\n    \n    for {\n        select {\n        case \u003c-depthTicker.C:\n            depth := len(b.commands)\n            metrics.BroadcasterCommandChannelDepth.Set(float64(depth))\n            \n            if depth \u003e 200 {  // 80% of 256\n                slog.Warn(\"Command channel near capacity\",\n                    \"depth\", depth,\n                    \"capacity\", cap(b.commands),\n                )\n            }\n            \n        case \u003c-ticker.C:\n            b.broadcastTick()\n            \n        // ... rest of select\n        }\n    }\n}\n```\n\n**Acceptance criteria:**\n- Metric updated every second\n- Warning logged at 80% capacity\n- Can alert in Prometheus on high depth\n\n### Task 3: Force-exit Stop() after timeout\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\nfunc (b *Broadcaster) Stop() {\n    // ... existing stop logic\n    \n    select {\n    case \u003c-b.done:\n        slog.Info(\"Broadcaster stopped gracefully\")\n    case \u003c-b.clock.After(b.stopTimeout):\n        slog.Warn(\"Broadcaster stop timeout exceeded, forcing exit\",\n            \"timeout\", b.stopTimeout,\n        )\n        metrics.BroadcasterStopTimeouts.Inc()\n        \n        // Force goroutine exit\n        close(b.done)\n        \n        // Log goroutine leak for debugging\n        slog.Error(\"Broadcaster goroutine may have leaked\",\n            \"active_sessions\", len(b.activeClients),\n        )\n    }\n}\n```\n\n**Acceptance criteria:**\n- Stop() always returns within timeout\n- Goroutine leak logged with context\n- Metric tracks forced exits\n\n### Task 4: Make onFirstClient callback async\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\nfunc (b *Broadcaster) handleRegister(cmd *registerCmd) {\n    // ... existing validation\n    \n    isFirstClient := len(clients) == 0\n    \n    if isFirstClient \u0026\u0026 b.onFirstClient != nil {\n        // Run callback asynchronously to avoid blocking Register\n        go func() {\n            if err := b.onFirstClient(cmd.sessionUUID); err != nil {\n                slog.Error(\"onFirstClient callback failed\",\n                    \"session_uuid\", cmd.sessionUUID,\n                    \"error\", err,\n                )\n                metrics.BroadcasterCallbackErrors.Inc()\n            }\n        }()\n    }\n    \n    // Register immediately, don't wait for callback\n    clients[cmd.client] = struct{}{}\n    b.activeClients[cmd.sessionUUID] = clients\n    \n    cmd.reply \u003c- registerReply{success: true}\n}\n```\n\n**Trade-off:** IncrRefCount happens async, but this is safe because:\n- Session already exists in Redis (from EnsureSessionActive)\n- Ref count protects against premature cleanup\n- If IncrRefCount fails, worst case is session cleaned up too early (client reconnects)\n\n**Acceptance criteria:**\n- Register returns immediately\n- Callback errors logged and metriced\n- Ref count still incremented (eventually)\n\n### Task 5: Add per-session circuit breaker\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\ntype sessionHealth struct {\n    consecutiveFailures int\n    circuitOpen         bool\n    openUntil           time.Time\n}\n\nfunc (b *Broadcaster) broadcastTick() {\n    for sessionUUID, clients := range b.activeClients {\n        health := b.sessionHealth[sessionUUID]\n        \n        // Skip if circuit open\n        if health.circuitOpen \u0026\u0026 b.clock.Now().Before(health.openUntil) {\n            continue\n        }\n        \n        update, err := b.engine.GetCurrentValue(ctx, sessionUUID)\n        if err != nil {\n            health.consecutiveFailures++\n            \n            if health.consecutiveFailures \u003e= 5 {\n                // Open circuit for 30 seconds\n                health.circuitOpen = true\n                health.openUntil = b.clock.Now().Add(30 * time.Second)\n                \n                slog.Warn(\"Session circuit opened due to failures\",\n                    \"session_uuid\", sessionUUID,\n                    \"failures\", health.consecutiveFailures,\n                )\n                metrics.BroadcasterSessionCircuitOpens.Inc()\n            }\n            continue\n        }\n        \n        // Success - reset circuit\n        health.consecutiveFailures = 0\n        health.circuitOpen = false\n        b.sessionHealth[sessionUUID] = health\n        \n        // Broadcast update\n        b.broadcastToSession(sessionUUID, clients, update)\n    }\n}\n```\n\n**Acceptance criteria:**\n- Circuit opens after 5 consecutive failures\n- Circuit stays open for 30 seconds\n- One slow session doesn't block others\n\n### Task 6: Add tick duration budget\n**File:** `internal/broadcast/broadcaster.go`\n\n```go\nconst maxTickDuration = 40 * time.Millisecond  // Leave 10ms margin from 50ms tick\n\nfunc (b *Broadcaster) broadcastTick() {\n    start := b.clock.Now()\n    defer func() {\n        duration := b.clock.Since(start)\n        metrics.BroadcasterTickDuration.Observe(duration.Seconds())\n        \n        if duration \u003e maxTickDuration {\n            slog.Warn(\"Tick duration exceeded budget\",\n                \"duration\", duration,\n                \"budget\", maxTickDuration,\n                \"sessions_processed\", len(b.activeClients),\n            )\n            metrics.BroadcasterSlowTicks.Inc()\n        }\n    }()\n    \n    // Process sessions\n    for sessionUUID, clients := range b.activeClients {\n        // Early abort if approaching budget\n        if b.clock.Since(start) \u003e maxTickDuration {\n            slog.Debug(\"Aborting tick early due to time budget\",\n                \"elapsed\", b.clock.Since(start),\n            )\n            metrics.BroadcasterAbortedTicks.Inc()\n            break\n        }\n        \n        // ... process session\n    }\n}\n```\n\n**Acceptance criteria:**\n- Tick aborts if exceeding 40ms budget\n- Slow ticks logged with context\n- Metrics track budget violations\n\n### Task 7: Add comprehensive metrics\n**File:** `internal/metrics/metrics.go`\n\n```go\nBroadcasterPanicsTotal = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"broadcaster_panics_total\",\n        Help: \"Total broadcaster panic recoveries\",\n    },\n)\n\nBroadcasterCommandChannelDepth = promauto.NewGauge(\n    prometheus.GaugeOpts{\n        Name: \"broadcaster_command_channel_depth\",\n        Help: \"Current command channel depth\",\n    },\n)\n\nBroadcasterStopTimeouts = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"broadcaster_stop_timeouts_total\",\n        Help: \"Broadcaster stops that exceeded timeout\",\n    },\n)\n\nBroadcasterCallbackErrors = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"broadcaster_callback_errors_total\",\n        Help: \"onFirstClient callback errors\",\n    },\n)\n\nBroadcasterSessionCircuitOpens = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"broadcaster_session_circuit_opens_total\",\n        Help: \"Sessions with circuit opened due to failures\",\n    },\n)\n\nBroadcasterSlowTicks = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"broadcaster_slow_ticks_total\",\n        Help: \"Ticks exceeding duration budget\",\n    },\n)\n\nBroadcasterAbortedTicks = promauto.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"broadcaster_aborted_ticks_total\",\n        Help: \"Ticks aborted early due to budget\",\n    },\n)\n```\n\n### Task 8: Unit tests\n**File:** `internal/broadcast/broadcaster_resilience_test.go` (NEW)\n\nTest scenarios:\n1. **Panic recovery:** Inject panic in tick loop, verify recovery\n2. **Command channel saturation:** Send 256 commands, verify metric\n3. **Stop timeout:** Block actor goroutine, verify forced exit\n4. **Async callback:** Verify Register doesn't block on callback\n5. **Circuit breaker:** Simulate 5 failures, verify circuit opens\n6. **Tick budget:** Simulate slow GetCurrentValue, verify early abort\n\n### Task 9: Integration test\n**File:** `internal/broadcast/broadcaster_resilience_integration_test.go` (NEW)\n\nTest with real Redis:\n1. Slow Redis responses (add latency)\n2. Command flood (500 concurrent registers)\n3. Graceful shutdown under load\n\n### Task 10: Documentation\n**File:** `docs/architecture/broadcaster-resilience.md` (NEW)\n\nDocument:\n- Actor timeout protections\n- Circuit breaker behavior\n- Tick duration budget rationale\n- Monitoring and alerting guidelines\n\n## Acceptance Criteria\n\n- ✅ Broadcaster survives panics without process crash\n- ✅ Command channel depth monitored, alerts at 80%\n- ✅ Stop() always completes within timeout\n- ✅ Register doesn't block on callback\n- ✅ Per-session circuit breaker prevents one slow session blocking others\n- ✅ Tick duration budget prevents overrun\n- ✅ Comprehensive metrics for all failure modes\n- ✅ Unit tests achieve 95%+ coverage\n- ✅ Integration tests verify resilience under stress\n\n## Files Created/Modified\n\n**New files:**\n- `internal/broadcast/broadcaster_resilience_test.go` (400 lines)\n- `internal/broadcast/broadcaster_resilience_integration_test.go` (200 lines)\n- `docs/architecture/broadcaster-resilience.md` (300 lines)\n\n**Modified files:**\n- `internal/broadcast/broadcaster.go` (add panic recovery, metrics, circuit breaker, 100 lines)\n- `internal/metrics/metrics.go` (add 7 new metrics, 50 lines)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock clock for timeout control\n- Mock engine for controlled failures\n- Verify panic recovery with defer/recover test\n- Test circuit breaker state machine\n\n**Integration tests:**\n- Real Redis with artificial latency\n- Command flood test (1000 concurrent)\n- Graceful shutdown stress test\n\n**Chaos testing:**\n- Random Redis failures\n- Kill Redis mid-tick\n- Flood commands continuously\n\n## Dependencies\n- Requires metrics framework (twitch-tow-kgj)\n- Complements broadcaster optimization (twitch-tow-k24)\n\n## Success Metrics\n- Zero broadcaster panics crash process\n- Command channel never exceeds 90% capacity\n- Stop() timeout rate \u003c0.1%\n- Circuit breaker prevents cascade failures\n\n## Effort Estimate\n**6 developer-days**\n\nBreakdown:\n- Panic recovery + metrics: 1 day\n- Circuit breaker: 2 days\n- Timeout improvements: 1 day\n- Tests: 1.5 days\n- Documentation: 0.5 day\n\n## Risks \u0026 Mitigation\n- **Risk:** Async callback causes ref count issues\n  - **Mitigation:** Session already exists, worst case is early cleanup\n- **Risk:** Circuit breaker too aggressive\n  - **Mitigation:** 5 failures threshold, 30s recovery window, tunable\n- **Risk:** Tick budget abort leaves sessions un-broadcast\n  - **Mitigation:** Next tick (50ms later) catches up, acceptable latency","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:46:07.127779+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:43.433067+01:00"}
{"id":"twitch-tow-li3","title":"EPIC: Secret Rotation with Zero-Downtime Key Versioning","description":"**User Story:** As a security operator, I need to rotate cryptographic secrets (encryption keys, session secrets, webhook secrets) without service downtime so that I can respond to key compromise and meet compliance requirements.\n\n**Problem Context:** Current secrets are loaded once at startup with no rotation support:\n- Token encryption key: Cannot decrypt existing tokens after rotation (all users must re-auth)\n- Session secret: Rotation invalidates all sessions (all users logged out)\n- Webhook secret: Must update Twitch conduit (requires coordination)\n- No key versioning, no rotation window, no gradual migration\n\n**Risks:**\n- Key compromise requires full redeployment + all users re-auth\n- Zero-downtime rotation impossible\n- Compliance requirements (90-day rotation) unmet\n\n**Solution Overview:** Add key versioning with rotation window support, background re-encryption job, graceful webhook secret rotation, and emergency rotation procedures.\n\n## Task Breakdown\n\n### 1. Add Encryption Key Versioning\n\n**File:** `internal/crypto/crypto.go`\n\n**Support multiple keys with version prefix:**\n```go\ntype VersionedCryptoService struct {\n    currentKey    cipher.AEAD  // For encryption\n    currentVersion string       // \"v2\"\n    legacyKeys    map[string]cipher.AEAD  // For decryption {\"v1\": key1}\n}\n\nfunc NewVersionedCryptoService(keys map[string]string, currentVersion string) (*VersionedCryptoService, error) {\n    svc := \u0026VersionedCryptoService{\n        currentVersion: currentVersion,\n        legacyKeys:     make(map[string]cipher.AEAD),\n    }\n    \n    for version, keyHex := range keys {\n        key, err := hex.DecodeString(keyHex)\n        if err != nil {\n            return nil, fmt.Errorf(\"invalid key for version %s: %w\", version, err)\n        }\n        \n        if len(key) != 32 {\n            return nil, fmt.Errorf(\"key for version %s must be 32 bytes\", version)\n        }\n        \n        block, err := aes.NewCipher(key)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to create cipher for version %s: %w\", version, err)\n        }\n        \n        gcm, err := cipher.NewGCM(block)\n        if err != nil {\n            return nil, fmt.Errorf(\"failed to create GCM for version %s: %w\", version, err)\n        }\n        \n        if version == currentVersion {\n            svc.currentKey = gcm\n        } else {\n            svc.legacyKeys[version] = gcm\n        }\n    }\n    \n    if svc.currentKey == nil {\n        return nil, fmt.Errorf(\"current version %s not found in keys\", currentVersion)\n    }\n    \n    return svc, nil\n}\n\nfunc (s *VersionedCryptoService) Encrypt(plaintext string) (string, error) {\n    nonce := make([]byte, s.currentKey.NonceSize())\n    if _, err := rand.Read(nonce); err != nil {\n        return \"\", err\n    }\n    \n    ciphertext := s.currentKey.Seal(nonce, nonce, []byte(plaintext), nil)\n    \n    // Prepend version: \"v2:hexciphertext\"\n    versioned := fmt.Sprintf(\"%s:%s\", s.currentVersion, hex.EncodeToString(ciphertext))\n    return versioned, nil\n}\n\nfunc (s *VersionedCryptoService) Decrypt(versioned string) (string, error) {\n    // Parse version prefix\n    parts := strings.SplitN(versioned, \":\", 2)\n    if len(parts) != 2 {\n        // Legacy format (no version), try current key\n        return s.decryptWithKey(versioned, s.currentKey)\n    }\n    \n    version := parts[0]\n    ciphertextHex := parts[1]\n    \n    // Select key based on version\n    var key cipher.AEAD\n    if version == s.currentVersion {\n        key = s.currentKey\n    } else if legacyKey, ok := s.legacyKeys[version]; ok {\n        key = legacyKey\n        keyRotationLegacyDecryptsTotal.WithLabelValues(version).Inc()\n    } else {\n        return \"\", fmt.Errorf(\"unknown key version: %s\", version)\n    }\n    \n    return s.decryptWithKey(ciphertextHex, key)\n}\n\nfunc (s *VersionedCryptoService) decryptWithKey(ciphertextHex string, key cipher.AEAD) (string, error) {\n    ciphertext, err := hex.DecodeString(ciphertextHex)\n    if err != nil {\n        return \"\", err\n    }\n    \n    nonceSize := key.NonceSize()\n    if len(ciphertext) \u003c nonceSize {\n        return \"\", fmt.Errorf(\"ciphertext too short\")\n    }\n    \n    nonce, ciphertext := ciphertext[:nonceSize], ciphertext[nonceSize:]\n    plaintext, err := key.Open(nil, nonce, ciphertext, nil)\n    if err != nil {\n        return \"\", err\n    }\n    \n    return string(plaintext), nil\n}\n```\n\n### 2. Update Config to Support Multiple Keys\n\n**File:** `internal/config/config.go`\n\n**Support key rotation window:**\n```go\ntype Config struct {\n    // ... existing fields\n    \n    // Token encryption keys (current + legacy for rotation)\n    TokenEncryptionKeys map[string]string `env:\"TOKEN_ENCRYPTION_KEYS\"`  // JSON: {\"v1\":\"key1\",\"v2\":\"key2\"}\n    CurrentKeyVersion   string            `env:\"CURRENT_KEY_VERSION\" envDefault:\"v1\"`\n}\n\nfunc Load() (*Config, error) {\n    cfg := \u0026Config{}\n    if err := env.Load(cfg); err != nil {\n        return nil, err\n    }\n    \n    // Parse token encryption keys from JSON\n    if keysJSON := os.Getenv(\"TOKEN_ENCRYPTION_KEYS\"); keysJSON != \"\" {\n        if err := json.Unmarshal([]byte(keysJSON), \u0026cfg.TokenEncryptionKeys); err != nil {\n            return nil, fmt.Errorf(\"failed to parse TOKEN_ENCRYPTION_KEYS: %w\", err)\n        }\n    }\n    \n    return cfg, nil\n}\n```\n\n**File:** `.env.example`\n\n```\n# Token encryption keys (JSON map, current + legacy during rotation)\n# Generate key: openssl rand -hex 32\nTOKEN_ENCRYPTION_KEYS='{\"v1\":\"oldkey64chars\",\"v2\":\"newkey64chars\"}'\nCURRENT_KEY_VERSION=v2\n\n# During rotation:\n# 1. Add v2 key to map\n# 2. Keep v1 in map (for decrypting existing tokens)\n# 3. Set CURRENT_KEY_VERSION=v2\n# 4. New tokens encrypted with v2, old tokens still readable\n# 5. After re-encryption complete, remove v1\n```\n\n### 3. Add Background Re-Encryption Job\n\n**File:** `internal/app/service.go`\n\n**Gradually re-encrypt tokens with new key:**\n```go\nfunc (s *Service) StartReEncryptionJob() {\n    go func() {\n        ticker := time.NewTicker(1 * time.Hour)\n        defer ticker.Stop()\n        \n        for range ticker.C {\n            ctx, cancel := context.WithTimeout(context.Background(), 10*time.Minute)\n            s.reEncryptTokens(ctx)\n            cancel()\n        }\n    }()\n}\n\nfunc (s *Service) reEncryptTokens(ctx context.Context) {\n    // Find users with tokens encrypted with old key version\n    users, err := s.userRepo.ListUsersWithLegacyTokens(ctx, s.currentKeyVersion)\n    if err != nil {\n        s.logger.Error(\"failed to list users for re-encryption\", \"error\", err)\n        return\n    }\n    \n    s.logger.Info(\"re-encrypting tokens\", \"count\", len(users))\n    \n    for _, user := range users {\n        // Re-save user (triggers re-encryption with current key)\n        err := s.userRepo.UpdateTokens(ctx, user.ID, user.AccessToken, user.RefreshToken, user.TokenExpiry)\n        if err != nil {\n            s.logger.Error(\"failed to re-encrypt user tokens\",\n                \"user_id\", user.ID,\n                \"error\", err)\n            keyRotationReEncryptionFailuresTotal.Inc()\n            continue\n        }\n        \n        keyRotationReEncryptionsTotal.Inc()\n    }\n    \n    s.logger.Info(\"re-encryption batch complete\", \"count\", len(users))\n}\n```\n\n**File:** `internal/database/user_repository.go`\n\n**Add query for legacy token detection:**\n```go\nfunc (r *UserRepo) ListUsersWithLegacyTokens(ctx context.Context, currentVersion string) ([]*domain.User, error) {\n    // Find users whose tokens don't start with currentVersion prefix\n    query := `\n        SELECT id, twitch_user_id, username, access_token, refresh_token, token_expiry, overlay_uuid\n        FROM users\n        WHERE access_token NOT LIKE $1\n        LIMIT 100\n    `\n    \n    versionPrefix := currentVersion + \":%\"\n    rows, err := r.pool.Query(ctx, query, versionPrefix)\n    if err != nil {\n        return nil, err\n    }\n    defer rows.Close()\n    \n    var users []*domain.User\n    for rows.Next() {\n        user, err := r.scanUser(rows)\n        if err != nil {\n            return nil, err\n        }\n        users = append(users, user)\n    }\n    \n    return users, nil\n}\n```\n\n### 4. Add Webhook Secret Rotation Endpoint\n\n**File:** `internal/server/handlers_admin.go` (NEW)\n\n**Admin endpoint for webhook secret rotation:**\n```go\nfunc (s *Server) handleRotateWebhookSecret(c echo.Context) error {\n    var req struct {\n        NewSecret string `json:\"new_secret\"`\n    }\n    \n    if err := c.Bind(\u0026req); err != nil {\n        return c.JSON(400, map[string]string{\"error\": \"invalid request\"})\n    }\n    \n    if len(req.NewSecret) \u003c 10 || len(req.NewSecret) \u003e 100 {\n        return c.JSON(400, map[string]string{\"error\": \"secret must be 10-100 characters\"})\n    }\n    \n    ctx := c.Request().Context()\n    \n    // Update Twitch conduit shard with new secret\n    err := s.twitchSvc.UpdateWebhookSecret(ctx, req.NewSecret)\n    if err != nil {\n        return c.JSON(500, map[string]string{\"error\": fmt.Sprintf(\"failed to update Twitch: %s\", err)})\n    }\n    \n    // Update environment variable (requires app restart to persist)\n    os.Setenv(\"WEBHOOK_SECRET\", req.NewSecret)\n    \n    s.logger.Info(\"webhook secret rotated\", \"admin\", c.Get(\"user_id\"))\n    \n    return c.JSON(200, map[string]string{\n        \"status\": \"webhook secret rotated\",\n        \"note\": \"restart required to persist new secret\",\n    })\n}\n\n// Register in routes\nfunc (s *Server) registerRoutes() {\n    admin := s.e.Group(\"/admin\")\n    admin.Use(s.requireAuth)  // Add admin role check\n    admin.POST(\"/rotate-webhook-secret\", s.handleRotateWebhookSecret)\n}\n```\n\n### 5. Add Key Rotation Metrics\n\n**File:** `internal/crypto/crypto.go`\n\n```go\nvar (\n    keyRotationLegacyDecryptsTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"key_rotation_legacy_decrypts_total\",\n            Help: \"Total number of decryptions using legacy keys\",\n        },\n        []string{\"version\"},\n    )\n    \n    keyRotationReEncryptionsTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"key_rotation_re_encryptions_total\",\n            Help: \"Total number of tokens re-encrypted with current key\",\n        })\n    \n    keyRotationReEncryptionFailuresTotal = prometheus.NewCounter(\n        prometheus.CounterOpts{\n            Name: \"key_rotation_re_encryption_failures_total\",\n            Help: \"Total number of failed re-encryption attempts\",\n        })\n)\n```\n\n### 6. Document Rotation Procedures\n\n**File:** `docs/SECRET_ROTATION.md` (NEW)\n\n```markdown\n# Secret Rotation Procedures\n\n## Token Encryption Key Rotation\n\n**Frequency:** Every 90 days or on compromise\n\n**Procedure:**\n\n1. **Generate new key:**\n   ```bash\n   openssl rand -hex 32\n   ```\n\n2. **Add to config (keep old key):**\n   ```bash\n   TOKEN_ENCRYPTION_KEYS='{\"v1\":\"old64chars\",\"v2\":\"new64chars\"}'\n   CURRENT_KEY_VERSION=v2\n   ```\n\n3. **Deploy with rolling restart:**\n   - New encryptions use v2\n   - Old tokens still readable with v1\n\n4. **Monitor re-encryption progress:**\n   - Check `key_rotation_legacy_decrypts_total{version=\"v1\"}`\n   - Background job re-encrypts 100 users/hour\n\n5. **Remove old key after grace period (7 days):**\n   ```bash\n   TOKEN_ENCRYPTION_KEYS='{\"v2\":\"new64chars\"}'\n   ```\n\n**Emergency rotation (key compromise):**\n- Force all users to re-authenticate (revoke all tokens)\n- Follow procedure above with immediate old key removal\n\n## Session Secret Rotation\n\n**Impact:** All users logged out\n\n**Procedure:**\n1. Generate new secret: `openssl rand -base64 32`\n2. Update `SESSION_SECRET` env var\n3. Rolling restart\n4. All users must re-login\n\n## Webhook Secret Rotation\n\n**Procedure:**\n1. Call admin endpoint: `POST /admin/rotate-webhook-secret`\n2. Restart app to persist new secret\n3. Twitch conduit updated automatically\n```\n\n### 7. Testing\n\n**File:** `internal/crypto/crypto_rotation_test.go` (NEW)\n\n```go\n// TestVersionedCrypto_EncryptDecrypt verifies versioning works\nfunc TestVersionedCrypto_EncryptDecrypt(t *testing.T) {\n    // Setup: v1 and v2 keys, v2 current\n    // Act: Encrypt with v2, decrypt with v2\n    // Assert: Ciphertext has \"v2:\" prefix, decrypts correctly\n}\n\n// TestVersionedCrypto_LegacyDecrypt verifies old keys work\nfunc TestVersionedCrypto_LegacyDecrypt(t *testing.T) {\n    // Setup: v1 ciphertext (from old system)\n    // Act: Decrypt with v1 in legacy keys\n    // Assert: Decrypts correctly, metric incremented\n}\n\n// TestReEncryption verifies migration\nfunc TestReEncryption(t *testing.T) {\n    // Setup: 10 users with v1 tokens\n    // Act: Run re-encryption job\n    // Assert: All tokens now have v2 prefix\n}\n```\n\n## Acceptance Criteria\n\n✅ Encryption keys support versioning (v1, v2 prefixes)\n✅ Multiple keys active during rotation window (current + legacy)\n✅ Background job re-encrypts tokens with new key (100/hour)\n✅ Metrics track legacy decrypts and re-encryption progress\n✅ Webhook secret rotation endpoint updates Twitch conduit\n✅ docs/SECRET_ROTATION.md documents all rotation procedures\n✅ Tests verify versioning, legacy decryption, re-encryption\n\n## Dependencies\n\n- None (self-contained security improvement)\n\n## Files Modified\n\n**Modified:**\n- internal/crypto/crypto.go (add versioning)\n- internal/config/config.go (support multiple keys)\n- internal/app/service.go (re-encryption job)\n- internal/database/user_repository.go (query legacy tokens)\n- .env.example (document key rotation)\n\n**New:**\n- internal/server/handlers_admin.go (webhook rotation endpoint)\n- docs/SECRET_ROTATION.md (rotation procedures)\n- internal/crypto/crypto_rotation_test.go (rotation tests)\n\n## Estimated Effort\n\n**Implementation:** 3 developer-days\n- Key versioning: 1 day\n- Re-encryption job: 4 hours\n- Webhook rotation: 2 hours\n- Metrics: 2 hours\n- Testing: 1 day\n- Documentation: 4 hours\n\n**Total:** 3 developer-days\n\n## Rollout Strategy\n\n1. Deploy versioned crypto with v1 as current (no change, backward compatible)\n2. Monitor key_rotation_legacy_decrypts_total (should be 0)\n3. Generate v2 key, add to config with v1 still present\n4. Set CURRENT_KEY_VERSION=v2, rolling restart\n5. Monitor re-encryption progress (100 users/hour)\n6. After 7 days (or when legacy_decrypts reaches 0), remove v1 key\n7. Document emergency rotation procedure for security team","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:46:44.191824+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:43.069886+01:00"}
{"id":"twitch-tow-mjm","title":"CONSOLIDATED: Resource Limits and Rate Limiting - Multi-Layer Defense","description":"# CONSOLIDATED SOLUTION: Comprehensive Resource Limits and Rate Limiting\n\nThis consolidates two nearly-identical proposals:\n- **twitch-tow-tt0** (Scalability - 4-layer defense with detailed implementation)\n- **twitch-tow-mzk** (Maintainability - 4-layer defense, 99% aligned)\n\n**Consensus:** Both architects voted +1 for each other's proposals. This merged solution adopts tt0 as the base (more implementation detail) with alignment confirmation from mzk.\n\n---\n\n## Problem Summary\n\n**Current vulnerabilities:**\n- No global WebSocket connection limit (DoS risk)\n- No per-broadcaster rate limiting on votes (bot attack vector)\n- No per-IP connection limit (single source can exhaust resources)\n- Per-user debounce (1s) insufficient for coordinated attacks\n\n**Attack scenarios:**\n1. Connection exhaustion: 10K connections from botnet\n2. Vote flooding: 1000 unique bots spam trigger words\n3. Session enumeration: Attacker tries random UUIDs\n\n---\n\n## Multi-Layer Defense Strategy\n\n### Layer 1: Global Connection Limits (Week 1)\n**Priority: HIGH** - prevents resource exhaustion\n\n**Goal:** Cap total concurrent WebSocket connections per instance\n\n**Implementation:**\n```go\ntype GlobalConnectionLimiter struct {\n    current atomic.Int64\n    max     int64\n}\n\nfunc (l *GlobalConnectionLimiter) Acquire() bool {\n    for {\n        current := l.current.Load()\n        if current \u003e= l.max {\n            return false\n        }\n        if l.current.CompareAndSwap(current, current+1) {\n            return true\n        }\n    }\n}\n\nfunc (l *GlobalConnectionLimiter) Release() {\n    l.current.Add(-1)\n}\n```\n\n**In WebSocket handler:**\n```go\nfunc (s *Server) handleWebSocket(c echo.Context) error {\n    if !s.globalConnLimiter.Acquire() {\n        return c.String(503, \"Server at capacity, please retry\")\n    }\n    defer s.globalConnLimiter.Release()\n    \n    // ... existing WebSocket upgrade logic\n}\n```\n\n**Limits:**\n- Default: 10,000 connections per instance\n- Configurable via: `MAX_WEBSOCKET_CONNECTIONS`\n- Log warning at 80% capacity\n- Metric: `websocket_connections_current`\n\n**File descriptor check on startup:**\n```go\nfunc checkUlimit() {\n    var rlimit syscall.Rlimit\n    syscall.Getrlimit(syscall.RLIMIT_NOFILE, \u0026rlimit)\n    \n    if rlimit.Cur \u003c 20000 {\n        slog.Warn(\"ulimit may be too low\", \n            \"current\", rlimit.Cur, \n            \"recommended\", 20000)\n    }\n}\n```\n\n---\n\n### Layer 2: Per-IP Connection Limits (Week 1)\n**Priority: HIGH** - prevents single-source attacks\n\n**Goal:** Limit concurrent connections from single IP address\n\n**Implementation:**\n```go\ntype IPConnectionLimiter struct {\n    mu     sync.RWMutex\n    ips    map[string]int\n    maxPer int\n}\n\nfunc (l *IPConnectionLimiter) Acquire(ip string) bool {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    if l.ips[ip] \u003e= l.maxPer {\n        return false\n    }\n    l.ips[ip]++\n    return true\n}\n\nfunc (l *IPConnectionLimiter) Release(ip string) {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    if count := l.ips[ip]; count \u003e 0 {\n        l.ips[ip] = count - 1\n        if l.ips[ip] == 0 {\n            delete(l.ips, ip)\n        }\n    }\n}\n```\n\n**In WebSocket handler:**\n```go\nip := c.RealIP()\nif !s.ipLimiter.Acquire(ip) {\n    return c.String(429, \"Too many connections from your IP\")\n}\ndefer s.ipLimiter.Release(ip)\n```\n\n**Limits:**\n- Default: 100 connections per IP\n- Configurable: `MAX_CONNECTIONS_PER_IP`\n- Accounts for NAT/proxy scenarios (higher limit acceptable)\n\n---\n\n### Layer 3: Token Bucket Rate Limiter for Votes (Week 2)\n**Priority: MEDIUM** - prevents bot vote spam\n\n**Goal:** Limit vote rate per broadcaster session\n\n**Implementation using Redis:**\n```lua\n-- Token bucket: 100 tokens, refills at 100/minute\nlocal key = KEYS[1]\nlocal now = tonumber(ARGV[1])\nlocal capacity = tonumber(ARGV[2])\nlocal rate = tonumber(ARGV[3])\n\nlocal tokens = redis.call('HGET', key, 'tokens')\nlocal last_update = redis.call('HGET', key, 'last_update')\n\nif not tokens then\n    tokens = capacity\n    last_update = now\nelse\n    tokens = tonumber(tokens)\n    last_update = tonumber(last_update)\nend\n\nlocal elapsed = math.max(0, now - last_update)\ntokens = math.min(capacity, tokens + elapsed * rate / 60000.0)\n\nif tokens \u003c 1 then\n    return 0\nend\n\ntokens = tokens - 1\nredis.call('HSET', key, 'tokens', tokens, 'last_update', now)\nredis.call('EXPIRE', key, 300)\nreturn 1\n```\n\n**In ProcessVote pipeline:**\n```go\n// After debounce check, before ApplyVote\nallowed, err := e.checkVoteRateLimit(ctx, sessionUUID)\nif !allowed {\n    slog.Warn(\"Vote rate limit exceeded\", \"session_uuid\", sessionUUID)\n    return 0, false\n}\n```\n\n**Parameters:**\n- Capacity: 100 tokens (burst allowance)\n- Refill rate: 100 tokens/minute (sustained rate)\n- Allows: 100 votes immediately, then 100/min sustained\n\n**Why token bucket:**\n- Allows natural bursts (chat excitement)\n- Prevents sustained bot spam\n- Fair: refills gradually\n\n---\n\n### Layer 4: Connection Rate Limiting (Week 2)\n**Priority: LOW** - defense in depth\n\n**Goal:** Limit new connection rate per IP\n\n**Implementation:**\n```go\nimport \"golang.org/x/time/rate\"\n\ntype ConnectionRateLimiter struct {\n    mu       sync.Mutex\n    limiters map[string]*rate.Limiter\n    rate     rate.Limit\n    burst    int\n}\n\nfunc (l *ConnectionRateLimiter) Allow(ip string) bool {\n    l.mu.Lock()\n    limiter, exists := l.limiters[ip]\n    if !exists {\n        limiter = rate.NewLimiter(l.rate, l.burst)\n        l.limiters[ip] = limiter\n    }\n    l.mu.Unlock()\n    \n    return limiter.Allow()\n}\n```\n\n**In middleware:**\n```go\nif !s.connRateLimiter.Allow(c.RealIP()) {\n    return c.String(429, \"Connection rate limit exceeded\")\n}\n```\n\n**Configuration:** 10 connections per second per IP\n\n---\n\n## Configuration Summary\n\n**Environment variables:**\n```bash\nMAX_WEBSOCKET_CONNECTIONS=10000    # Global limit per instance\nMAX_CONNECTIONS_PER_IP=100         # Per-IP concurrent limit\nVOTE_RATE_LIMIT_CAPACITY=100       # Token bucket capacity\nVOTE_RATE_LIMIT_RATE=100           # Tokens per minute\nCONNECTION_RATE_PER_IP=10          # New connections per second per IP\n```\n\n**Metrics to add:**\n```\nwebsocket_connections_rejected_total{reason}  // global_limit, ip_limit, rate_limit\nvote_rate_limit_exceeded_total{session_uuid}\nconnection_limiter_capacity_pct\n```\n\n**Alerts:**\n```yaml\n- alert: WebSocketCapacityHigh\n  expr: websocket_connection_capacity_pct \u003e 80\n  for: 5m\n  annotations:\n    summary: \"WebSocket connections at 80% capacity\"\n```\n\n---\n\n## Implementation Timeline\n\n**Week 1:**\n- Global connection limit (atomic counter)\n- Per-IP connection limit (map + mutex)\n- File descriptor check on startup\n- Metrics for connection tracking\n\n**Week 2:**\n- Token bucket rate limiter (Redis Lua script)\n- Connection rate limiting (golang.org/x/time/rate)\n- Integration tests for limits\n- Documentation for tuning parameters\n\n**Total:** 2 weeks (10 developer-days)\n\n---\n\n## Success Criteria\n\n**Global Limits:**\n- ✅ Instance cannot exceed MAX_WEBSOCKET_CONNECTIONS\n- ✅ Graceful rejection with 503 response\n- ✅ Metrics show current connection count\n\n**Per-IP Limits:**\n- ✅ Single IP cannot exceed MAX_CONNECTIONS_PER_IP\n- ✅ NAT scenarios tested (higher limit acceptable)\n- ✅ 429 response with clear error message\n\n**Vote Rate Limiting:**\n- ✅ Bot spam scenarios blocked (1000 bots → rate limited)\n- ✅ Legitimate high-engagement allowed (burst to 100)\n- ✅ Sustained rate limited to configured value\n\n**Connection Rate Limiting:**\n- ✅ Rapid connection spam prevented\n- ✅ Per-IP tracking with periodic cleanup\n\n---\n\n## Testing Strategy\n\n**Load testing scenarios:**\n1. 10K concurrent connections (should succeed)\n2. 11K concurrent connections (1K rejected)\n3. 200 connections from single IP (100 succeed, 100 rejected)\n4. 1000 votes in 10 seconds (100 applied, 900 rate limited)\n\n**Tools:**\n- k6 or Apache Bench for load testing\n- WebSocket load testing: ws-harness or custom script\n\n**Integration tests:**\n- Test with real WebSocket connections\n- Verify 503/429 responses\n- Test metrics are updated\n- Test all four layers independently\n\n---\n\n## Implementation Files\n\n**New files:**\n- `internal/server/connection_limiter.go` (250 lines)\n- `internal/server/connection_limiter_test.go` (400 lines)\n- `internal/server/connection_limits_integration_test.go` (200 lines)\n- `internal/redis/vote_rate_limit.lua` (60 lines)\n- `internal/redis/vote_rate_limit_test.go` (300 lines)\n- `scripts/load-test-connections.sh` (50 lines)\n- `scripts/load-test-vote-rate-limit.go` (80 lines)\n- `docs/operations/connection-limits.md` (400 lines)\n- `docs/operations/vote-rate-limiting.md` (400 lines)\n\n**Modified files:**\n- `internal/server/handlers_overlay.go` (add limiter checks, 20 lines)\n- `cmd/server/main.go` (ulimit check, limiter init, 30 lines)\n- `internal/config/config.go` (add 5 config fields)\n- `.env.example` (document limit variables)\n- `internal/metrics/metrics.go` (add rejection + capacity metrics)\n- `internal/sentiment/engine.go` (add vote rate limit check, 30 lines)\n\n---\n\n## Trade-offs Analysis\n\n**Pros:**\n+ Multi-layer defense (defense in depth)\n+ Configurable limits per deployment\n+ Low overhead (atomic ops, efficient algorithms)\n+ Prevents common DoS scenarios\n+ Token bucket allows legitimate bursts\n\n**Cons:**\n- May reject legitimate traffic under extreme load\n- Shared IPs (NAT) require higher per-IP limits\n- Token bucket adds Redis calls (mitigated: only for votes that match triggers)\n\n**Verdict:** Essential for production. Benefits far outweigh costs.\n\n---\n\n## Dependencies\n**Adds:**\n- `golang.org/x/time/rate` (stdlib extension)\n\n**Requires:**\n- Redis Functions for vote rate limiting (Redis 7.0+)\n\n---\n\n## Supersedes\nThis consolidated solution replaces:\n- twitch-tow-tt0 (scalability - adopted as base)\n- twitch-tow-mzk (maintainability - 99% aligned, voted +1 for tt0)\n\nBoth architects agreed these proposals are essentially identical. Using tt0 as base due to more implementation detail (code samples, testing scenarios).\n\n---\n\n## Notes from Team Alignment\n\n**Architect-Resilience on mzk vs tt0:**\n\u003e \"Solid 4-layer approach, nearly identical to twitch-tow-tt0. Agreement on all 4 layers, limits, and libraries. Why I slightly prefer twitch-tow-tt0: More implementation detail, token bucket in Redis (better for multi-instance), includes testing scenarios. Either proposal works - they're 95% identical.\"\n\n**Verdict:** Unanimous consensus. tt0 selected as consolidated solution with full alignment from maintainability architect (mzk author).","status":"open","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:34:34.621161+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:34:34.621161+01:00","labels":["phase-3-consolidated"]}
{"id":"twitch-tow-mxs","title":"Discussion: Broadcaster actor deadlock and timeout handling","description":"## Issue\nThe Broadcaster actor has timeout protection for commands but no deadlock prevention for channel blocking. The tick loop could block indefinitely if Redis hangs longer than timeout.\n\n## Current State\n- Command channel is buffered (cap 256) but could still block if flooded\n- Register/GetClientCount have 5s timeout but return error, not panic (good)\n- Stop() has 10s timeout but just logs warning if exceeded\n- Tick loop uses 2s Redis timeout but continues processing other sessions\n- No circuit breaker if Redis consistently slow (keeps retrying every 50ms)\n\n## Failure Modes\n1. **Command channel exhaustion**: 256 register commands could accumulate faster than actor processes\n2. **Slow client detection**: Non-blocking send identifies slow clients, but detection happens per-tick (50ms granularity)\n3. **Graceful shutdown timeout**: Stop() timeout doesn't force exit (goroutine could leak)\n4. **Redis slowdown amplification**: 2s timeout per session * N sessions could exceed 50ms tick budget\n5. **onFirstClient callback blocking**: If IncrRefCount blocks, it delays Register reply (line 177-180)\n\n## Risks\n- **Resource exhaustion**: Leaked broadcaster goroutine continues consuming resources\n- **Cascading slowdown**: One slow session's Redis timeout delays broadcast to all other sessions\n- **Head-of-line blocking**: Slow Register commands delay subsequent commands\n- **Unobservable deadlocks**: Timeout warnings logged but no metrics/alerts\n\n## Suggestions\n1. Add panic recovery in actor goroutine (defer + recover)\n2. Track command channel depth as metric (alert if \u003e 80% full)\n3. Make Stop() force-exit after timeout (close done channel + log goroutine leak)\n4. Run onFirstClient callback asynchronously (non-blocking Register)\n5. Skip sessions with consecutive Redis failures (circuit breaker per session)\n6. Add max tick duration budget (e.g., 40ms) - abort early if exceeded\n7. Instrument tick duration, Redis latency, slow client evictions\n\n## Files\n- internal/broadcast/broadcaster.go:87-100 (Register timeout)\n- internal/broadcast/broadcaster.go:127-142 (Stop timeout)\n- internal/broadcast/broadcaster.go:220-257 (Tick loop)\n- internal/broadcast/broadcaster.go:171-193 (handleRegister with callback)","notes":"CONVERTED TO EPIC: twitch-tow-krx (Epic: Broadcaster Actor Resilience). Implements 6 resilience improvements: (1) Panic recovery, (2) Command channel depth monitoring, (3) Force-exit Stop() timeout, (4) Async onFirstClient callback, (5) Per-session circuit breaker, (6) Tick duration budget. 6 developer-days effort. Includes 7 new metrics and comprehensive testing strategy.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:20.570702+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:46:13.23892+01:00","closed_at":"2026-02-12T17:46:13.238923+01:00"}
{"id":"twitch-tow-mzk","title":"Solution: Resource limits and rate limiting","description":"SOLUTION PROPOSAL for resource limit concerns (twitch-tow-08s, twitch-tow-aa4, twitch-tow-4v4)\n\nProblem: No global WebSocket connection limits, no rate limiting on webhooks, no abuse prevention.\n\nProposed Solution: Multi-layer rate limiting\n\nLayer 1 - Global Resource Limits:\nConfigure limits in Config struct with env vars.\nMAX_WEBSOCKET_CONNECTIONS_GLOBAL = 10000 (across all instances).\nMAX_WEBSOCKET_CONNECTIONS_PER_SESSION = 50 (already enforced).\nMAX_SESSIONS_PER_USER = 1 (enforce overlay_uuid uniqueness).\n\nImplementation: Add Redis counter for global WebSocket count. Increment on Register, decrement on Unregister. Reject new connections when limit reached.\n\nLayer 2 - Per-IP Rate Limiting:\nUse golang.org/x/time/rate for token bucket rate limiter.\nOverlay endpoint: 10 req/min per IP.\nWebhook endpoint: 100 req/min per IP (Twitch sends bursts).\nDashboard: 60 req/min per IP.\n\nImplementation: Add rate limiting middleware to Echo using sync.Map to store per-IP limiters.\n\nLayer 3 - Per-User Rate Limiting:\nVote debounce: Already implemented (1 vote per user per second).\nConfig updates: 10 updates per hour per user.\nUUID rotation: 5 rotations per day per user.\n\nImplementation: Extend existing debouncer pattern with different TTLs.\n\nLayer 4 - Webhook Signature Validation:\nAlready implemented (Kappopher HMAC verification).\nPrevents unauthorized webhook POSTs.\n\nTrade-offs:\nPros: Prevents abuse and resource exhaustion. Protects against DDoS. Reasonable limits for legitimate users.\nCons: May need tuning based on actual usage. Per-IP limiting can be bypassed with distributed attacks. Adds Redis operations (but lightweight).\n\nEstimated effort: 12 hours (global limits 4h, per-IP middleware 6h, per-user limits 2h).\n\nVote: +1 from Maintainability architect. Rate limiting is essential for production resilience and security.","notes":"Vote: +1 from Architect-Resilience (but prefer twitch-tow-tt0)\n\nSolid 4-layer approach, nearly identical to twitch-tow-tt0.\n\n**Agreement on:**\n- All 4 layers (global, per-IP, per-user, webhook validation)\n- Limits (10K global, 100 per IP, 50 per session)\n- golang.org/x/time/rate for rate limiting\n\n**Why I slightly prefer twitch-tow-tt0:**\n- More implementation detail\n- Token bucket in Redis (better for multi-instance)\n- Includes testing scenarios\n\n**Either proposal works** - they're 95% identical. Team should choose based on implementation detail preference.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:12:46.06296+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.591589+01:00","closed_at":"2026-02-12T17:56:57.591589+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-nrf","title":"Add comprehensive broadcaster shutdown tests","description":"**Medium Priority (Testing)**\n\nLocation: internal/broadcast/broadcaster_test.go\n\nMissing coverage: shutdown race conditions, goroutine cleanup, Stop() timeout, concurrent commands during shutdown, cleanup of active clients during stop.\n\nImpact: No verification that broadcaster shuts down cleanly.\n\nFix:\n- Add TestBroadcasterStopCleansUpGoroutines\n- Add TestBroadcasterStopTimeout\n- Add TestBroadcasterStopWithActiveClients\n- Use runtime.NumGoroutine() to verify cleanup","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:13.509925+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:38:41.191568+01:00","closed_at":"2026-02-12T16:38:41.191568+01:00","close_reason":"Added comprehensive broadcaster shutdown tests\n\nNew tests added to internal/broadcast/broadcaster_test.go:\n\n1. TestBroadcasterStopCleansUpGoroutines\n   - Verifies goroutine cleanup using runtime.NumGoroutine()\n   - Creates multiple sessions with multiple clients (5 total clients)\n   - Tracks baseline, after-create, after-register, and final goroutine counts\n   - Documents known issue (twitch-tow-563) with goroutine synchronization\n   - Uses GC and generous sleep times to allow cleanup\n   - Asserts final goroutine count is reasonable (\u003c10 leak tolerance)\n\n2. TestBroadcasterStopWithActiveClients\n   - Tests Stop() with active clients across multiple sessions\n   - Verifies onSessionEmpty callback fires for all sessions\n   - Ensures cleanup happens even with active connections\n\n3. TestBroadcasterStopIdempotent\n   - Verifies calling Stop() multiple times doesn't panic\n   - Tests defensive programming and thread safety\n\n4. TestBroadcasterStopBlocksCommandProcessing\n   - Verifies commands sent after Stop() are handled gracefully\n   - Tests that actor loop exits properly\n\nAll tests pass (121s total runtime for broadcast package).\nTests use runtime.NumGoroutine() as requested to track goroutine cleanup.\nTests document interaction with twitch-tow-563 (Stop() synchronization issue).\n\nFiles modified:\n- /Users/xogl/Projects/private/twitch-tow/internal/broadcast/broadcaster_test.go"}
{"id":"twitch-tow-nvq","title":"Discussion: Interface granularity - too many small interfaces","description":"The domain package has been split into very granular interfaces (SessionRepository 11 methods, SentimentStore 3 methods, Debouncer 1 method). While this follows ISP (Interface Segregation Principle), it may be over-engineered:\n\n1. **Single implementation per interface**: Each interface has exactly ONE implementation (SessionRepo, SentimentStore, Debouncer) - all in the same redis package\n2. **Tight coupling via composition**: The Engine depends on all three interfaces, but they're always used together\n3. **No true separation of concerns**: All three interfaces operate on the same Redis keys and data model\n4. **Testing friction**: Tests need to mock 3 separate interfaces even though they represent a cohesive Redis data store\n\n**Current**: SessionRepository (11) + SentimentStore (3) + Debouncer (1) = 3 interfaces, 3 types\n**Alternative**: Single SessionStore interface with 15 methods, single implementation\n\n**Trade-off**: The split does make the Engine's dependencies explicit (queries vs mutations vs rate limiting), but this may be premature optimization given there's only one storage backend.","notes":"RESOLVED: Current interface granularity is intentional and appropriate. The 3-interface split (SessionRepository, SentimentStore, Debouncer) follows Interface Segregation Principle and makes Engine dependencies explicit. Single implementation per interface is fine - interfaces are on consumer side (domain/), implementation in producer (redis/). The split enables clear testing (mock only what you use) and prevents bloated god-object interfaces. No consolidation needed - pattern aligns with Go best practices.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:03:48.719931+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:50:57.049155+01:00","closed_at":"2026-02-12T17:50:57.049157+01:00"}
{"id":"twitch-tow-ojd","title":"SOLUTION: Redis resilience via Sentinel + replicas + circuit breaker","description":"SOLUTION: Redis Resilience via Sentinel + Read Replicas + Circuit Breaker\n\nThis solution addresses consensus beads:\n- twitch-tow-usj (P1 - Redis SPOF, Scalability)\n- twitch-tow-fgm (P2 - Circuit breaker, Resilience)\n- twitch-tow-fc8 (P2 - Polling overhead, Scalability)\n- twitch-tow-f6g (P2 - Config re-fetch, Scalability)\n\n## Problem Summary\n\n**Current state:**\n- Single Redis instance = SPOF\n- No failover, no replication\n- Throughput limit: ~200K ops/sec\n- Broadcaster polls 20 times/sec per session\n- Config fetched every 50ms (wasteful)\n\n**At scale:**\n- 10 instances × 1K sessions = 200K ops/sec (at limit)\n- Redis crash = total outage\n- No read scaling\n\n---\n\n## Multi-Phase Solution\n\n### Phase 1: Circuit Breaker Pattern (Week 1)\nPriority: HIGH - prevents Redis failures from cascading\n\n**Goal:** Graceful degradation when Redis is slow/unavailable\n\n**Implementation:**\n```go\nimport \"github.com/sony/gobreaker\"\n\ntype CircuitBreakerRedis struct {\n    client *redis.Client\n    cb     *gobreaker.CircuitBreaker\n}\n\nfunc (r *CircuitBreakerRedis) Get(ctx, key) (string, error) {\n    val, err := r.cb.Execute(func() (interface{}, error) {\n        return r.client.Get(ctx, key).Result()\n    })\n    if err == gobreaker.ErrOpenState {\n        // Circuit open, return cached value or error\n        return \"\", ErrRedisUnavailable\n    }\n    return val.(string), err\n}\n```\n\n**Circuit breaker settings:**\n- Max failures: 5 consecutive errors\n- Timeout: 10 seconds open → half-open\n- Success threshold: 3 successful requests → close\n\n**Behavior when circuit open:**\n1. Broadcaster: Return last known value (stale but better than crash)\n2. Vote processing: Log error, continue (votes are not critical)\n3. Session activation: Return error to client (retry logic)\n\n**Trade-offs:**\n+ Prevents Redis failures from taking down app\n+ Configurable thresholds\n- Adds complexity\n- Stale data during outages\n\n**Verdict:** Essential pattern for production resilience\n\n---\n\n### Phase 2: Local Config Cache (Week 2)\nPriority: MEDIUM - reduces Redis load by 95%\n\n**Goal:** Cache config in memory, reduce fetches from 20/sec to 0.1/sec per session\n\n**Implementation:**\n```go\ntype ConfigCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]*cacheEntry\n    ttl     time.Duration\n}\n\ntype cacheEntry struct {\n    config    domain.ConfigSnapshot\n    expiresAt time.Time\n}\n\nfunc (c *ConfigCache) Get(sessionUUID uuid.UUID) (*ConfigSnapshot, bool) {\n    c.mu.RLock()\n    defer c.mu.RUnlock()\n    \n    entry, ok := c.entries[sessionUUID]\n    if !ok || time.Now().After(entry.expiresAt) {\n        return nil, false\n    }\n    return \u0026entry.config, true\n}\n\nfunc (c *ConfigCache) Set(sessionUUID uuid.UUID, config ConfigSnapshot) {\n    c.mu.Lock()\n    defer c.mu.Unlock()\n    \n    c.entries[sessionUUID] = \u0026cacheEntry{\n        config:    config,\n        expiresAt: time.Now().Add(c.ttl),\n    }\n}\n```\n\n**TTL:** 10 seconds (acceptable staleness)\n\n**Cache invalidation:**\n- Option A: TTL-based (simple, 10s delay for updates)\n- Option B: Pub/sub invalidation (complex, real-time)\n\n**Recommendation:** Start with TTL, add pub/sub if 10s is too slow\n\n**Impact:**\n- Before: 1K sessions × 20 calls/sec = 20K config reads/sec\n- After: 1K sessions × 0.1 calls/sec = 100 config reads/sec\n- **Reduction: 99.5% fewer Redis calls**\n\n**Memory overhead:**\n- ~200 bytes per cached config\n- 1K sessions = 200 KB (negligible)\n\n---\n\n### Phase 3: Redis Sentinel for HA (Week 3-4)\nPriority: HIGH - eliminates SPOF\n\n**Architecture:**\n```\n[App Instance 1] ──┐\n[App Instance 2] ──┼─→ [Sentinel 1] ─┐\n[App Instance N] ──┘   [Sentinel 2] ─┼─→ [Redis Master]\n                       [Sentinel 3] ─┘   [Redis Replica 1]\n                                          [Redis Replica 2]\n```\n\n**Setup:**\n1. Deploy 3 Redis Sentinel instances (HA quorum)\n2. Configure 1 master + 2 replicas\n3. Sentinels monitor master health\n4. Auto-failover on master failure (30s typical)\n\n**Application changes:**\n```go\n// Instead of:\nopts, _ := redis.ParseURL(redisURL)\nclient := redis.NewClient(opts)\n\n// Use:\nclient := redis.NewFailoverClient(\u0026redis.FailoverOptions{\n    MasterName:    \"chatpulse-master\",\n    SentinelAddrs: []string{\n        \"sentinel1:26379\",\n        \"sentinel2:26379\",\n        \"sentinel3:26379\",\n    },\n    Password: cfg.RedisPassword,\n})\n```\n\n**Failover behavior:**\n1. Master crashes\n2. Sentinels detect failure (10s)\n3. Quorum votes on new master\n4. Replica promoted to master (20s)\n5. Clients auto-reconnect to new master\n6. **Total downtime: 30-60 seconds**\n\n**Trade-offs:**\n+ Automatic failover (no human intervention)\n+ Near-zero data loss (replication lag 50-200ms)\n+ Handles master failures gracefully\n- Operational complexity (3 more processes)\n- Replication lag (50-200ms, acceptable for sentiment)\n- Split-brain scenarios (mitigated by quorum)\n\n---\n\n### Phase 4: Read Replicas for Scale (Week 5-6)\nPriority: MEDIUM - scale reads beyond 200K ops/sec\n\n**Goal:** Route read-only operations to replicas\n\n**Read operations (95% of load):**\n- Broadcaster ticks: GetSentiment (Lua FCALL_RO)\n- Config reads (if not cached)\n\n**Write operations (5% of load):**\n- Vote processing: ApplyVote (Lua FCALL)\n- Session lifecycle: ActivateSession, MarkDisconnected\n- Ref counting: INCR/DECR\n\n**Implementation:**\n```go\ntype SplitRedisClient struct {\n    master   *redis.Client  // writes + consistent reads\n    replicas []*redis.Client // read-only operations\n}\n\nfunc (s *SplitRedisClient) GetSentiment(ctx, sessionUUID) (float64, error) {\n    // Round-robin across replicas\n    replica := s.replicas[atomic.AddUint64(\u0026s.counter, 1) % len(s.replicas)]\n    return replica.FCallRO(ctx, \"get_decayed_value\", ...)\n}\n\nfunc (s *SplitRedisClient) ApplyVote(ctx, sessionUUID, delta) (float64, error) {\n    // Always use master for writes\n    return s.master.FCall(ctx, \"apply_vote\", ...)\n}\n```\n\n**Scaling capacity:**\n- 1 master: 100K write ops/sec\n- 2 replicas: 200K read ops/sec each = 400K total read capacity\n- **Combined: 500K ops/sec (5x improvement)**\n\n**Replication lag:**\n- Typical: 50-200ms\n- Max acceptable: 1 second\n\n**Impact on overlay:**\n- Votes applied to master\n- Broadcaster reads from replica (50-200ms stale)\n- Overlay shows value with small delay\n- **Acceptable:** Sentiment is not real-time critical\n\n**Trade-offs:**\n+ Massive read scaling (10x+ capacity)\n+ Reduces master load (master handles only writes)\n- Replication lag (50-200ms staleness)\n- More complex client routing logic\n- More infrastructure to manage\n\n---\n\n## Implementation Timeline\n\n**Week 1:** Circuit breaker pattern\n- Wrap Redis client\n- Add fallback behavior\n- Test with Redis failures\n\n**Week 2:** Config caching\n- Implement cache with TTL\n- Integrate with Engine.GetCurrentValue\n- Measure Redis call reduction\n\n**Week 3-4:** Redis Sentinel deployment\n- Deploy 3 Sentinels + 1 master + 2 replicas\n- Update application to use FailoverClient\n- Test failover scenarios\n- Document runbook\n\n**Week 5-6:** Read replica routing\n- Implement split client (master/replica)\n- Route reads to replicas\n- Measure throughput improvement\n- Monitor replication lag\n\n**Total:** 6 weeks, can be parallelized\n\n---\n\n## Success Criteria\n\n**Circuit Breaker:**\n- ✅ Redis failure doesn't crash application\n- ✅ Graceful degradation (stale data served)\n- ✅ Automatic recovery when Redis returns\n\n**Config Caching:**\n- ✅ Redis config reads reduced by 99%+\n- ✅ 10-second staleness acceptable\n- ✅ Memory overhead under 500 KB\n\n**Sentinel HA:**\n- ✅ Master failure auto-recovers in under 60s\n- ✅ Zero manual intervention required\n- ✅ No data loss (replication)\n\n**Read Replicas:**\n- ✅ Read capacity scaled 5x+ (500K ops/sec)\n- ✅ Replication lag under 200ms p99\n- ✅ Overlay staleness acceptable (under 1s)\n\n---\n\n## Cost Analysis\n\n**Infrastructure costs (AWS example):**\n- Current: 1 Redis (r6g.large) = $150/month\n- With Sentinel: 1 master + 2 replicas + 3 sentinels (smaller) = $450/month\n- **Increase: $300/month**\n\n**Engineering costs:**\n- Circuit breaker: 1 week\n- Config cache: 1 week\n- Sentinel setup: 2 weeks\n- Read replicas: 2 weeks\n- **Total: 6 developer-weeks**\n\n**ROI:**\n- Eliminates SPOF (prevents outages)\n- Enables horizontal scaling (10x capacity)\n- Reduces incident response time\n\n**Verdict:** Essential investment for production scale\n\n---\n\n## Risks \u0026 Mitigations\n\n**Risk 1: Replication lag causes confusion**\n- Mitigation: Monitor lag, alert if over 500ms\n- Fallback: Route reads to master if lag too high\n\n**Risk 2: Sentinel misconfiguration causes split-brain**\n- Mitigation: Use 3 sentinels (quorum), test failover thoroughly\n- Documentation: Clear runbook for split-brain recovery\n\n**Risk 3: Circuit breaker false positives**\n- Mitigation: Tune thresholds based on production data\n- Monitoring: Track circuit state changes\n\n---\n\n## Open Questions for Team\n\n1. Can we tolerate 50-200ms staleness in overlays?\n2. Should circuit breaker return stale data or error?\n3. Do we need pub/sub for config invalidation or is TTL sufficient?\n4. What Redis Sentinel deployment model? (K8s StatefulSet vs VMs)\n\nVote: Use bd update to add your +1 or concerns\n","notes":"Vote: +1 from architect-resilience (Resilience Expert).\n\n**STRONG APPROVAL with suggestions for circuit breaker tuning**\n\n## Answers to Your Questions\n\n### 1. Circuit Breaker Settings - NEEDS ADJUSTMENT\n\n**Your proposed settings:**\n- 5 consecutive failures → open\n- 10 second timeout\n- 3 successes → close\n\n**My recommendation (from twitch-tow-sb3):**\n- **60% failure rate** over sliding 10-second window (min 5 requests)\n- **30 second** timeout (not 10s)\n- 3 successes → close ✓ (this is good)\n\n**Why percentage-based is better:**\n\n❌ **Consecutive failures approach problems:**\n- 5 failures in a row is TOO AGGRESSIVE for transient issues\n- A single success resets the counter → circuit never opens during intermittent issues\n- Example: Pattern FFFFSFFFFFSFFFFFF (many failures, but never 5 consecutive) → circuit stays closed\n\n✅ **Failure ratio approach benefits:**\n- Catches intermittent issues (60% failure over 10s window)\n- More resilient to occasional successes during degradation\n- Industry standard (used by Hystrix, resilience4j, etc.)\n\n**Recommended gobreaker config:**\n```go\nsettings := gobreaker.Settings{\n    Name:        \"redis\",\n    MaxRequests: 3,  // Half-open: 3 requests before closing ✓\n    Interval:    10 * time.Second,  // Reset counter every 10s\n    Timeout:     30 * time.Second,  // Open for 30s (not 10s)\n    ReadyToTrip: func(counts gobreaker.Counts) bool {\n        failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)\n        return counts.Requests \u003e= 5 \u0026\u0026 failureRatio \u003e= 0.6\n    },\n}\n```\n\n**Why 30s timeout (not 10s)?**\n- Redis issues often take 20-30s to resolve (restart, failover, GC pause)\n- 10s is too short → circuit flaps (open → half-open → open repeatedly)\n- 30s gives Redis time to recover before we retry\n\n### 2. Fallback Strategy - OPTION A (stale data) ✓\n\n**My vote: Option A (your proposal) - Return stale data from memory cache**\n\n**Rationale:**\n- Sentiment overlay is NOT mission-critical (fail-open philosophy)\n- Stale sentiment better than blank overlay\n- Users expect real-time updates, but 30s staleness acceptable during outage\n- Aligns with graceful degradation principles\n\n**Reject Option B (fail fast):**\n- Would cause blank overlays for all streamers during Redis outage\n- Cascading failure (every WebSocket broadcast fails)\n\n**Reject Option C (queue requests):**\n- Adds complexity (queue management, replay logic, TTL)\n- Votes can be dropped (not critical data)\n- Overkill for sentiment overlay\n\n**Implementation suggestion (from my twitch-tow-sb3):**\n```go\n// Cache last known value in memory (sync.Map, 60s TTL)\ntype SentimentCache struct {\n    mu      sync.RWMutex\n    entries map[uuid.UUID]cachedSentiment\n}\n\ntype cachedSentiment struct {\n    value     float64\n    timestamp time.Time\n}\n\nfunc (s *SentimentStore) GetSentiment(ctx, sessionUUID) (float64, error) {\n    result, err := s.client.FCallRO(ctx, \"get_decayed_value\", ...)\n    \n    if err != nil {\n        // Circuit breaker open or Redis error\n        if cached, ok := s.cache.Get(sessionUUID); ok {\n            if time.Since(cached.timestamp) \u003c 60*time.Second {\n                // Return cached value with degraded status\n                return cached.value, ErrRedisDegraded  // Special error\n            }\n        }\n        return 0, err  // Cache miss or too old\n    }\n    \n    // Success: update cache\n    s.cache.Set(sessionUUID, result, time.Now())\n    return result, nil\n}\n```\n\n**Broadcaster must handle ErrRedisDegraded:**\n```go\nvalue, err := engine.GetCurrentValue(ctx, sessionUUID)\nstatus := \"active\"\nif errors.Is(err, ErrRedisDegraded) {\n    status = \"degraded\"  // Show yellow indicator in overlay\n}\nbroadcast(SessionUpdate{Value: value, Status: status})\n```\n\n### 3. Replication Lag Tolerance - YES ✓\n\n**My vote: 50-200ms staleness is ACCEPTABLE**\n\n**Rationale:**\n- Sentiment is not financial data (no money at risk)\n- Current broadcaster tick is 50ms → already 50ms delayed\n- 50-200ms additional lag = 100-250ms total → imperceptible to viewers\n- Real-time chat is already 1-2s delayed (Twitch RTMP → HLS → viewer)\n\n**When replication lag is NOT acceptable:**\n- Financial transactions (stock prices, payments)\n- Strong consistency requirements (leader election, distributed locks)\n- User authentication state (session tokens)\n\n**For sentiment overlay:** Acceptable. 5x capacity gain is worth it.\n\n### 4. Multiple Failure Modes - Mostly Covered ✓\n\n**Your solution addresses:**\n✅ Redis crash → Sentinel failover\n✅ Redis slowness → Circuit breaker\n✅ Redis overload → Read replicas\n❌ Network partition → NOT ADDRESSED\n❌ Split-brain → Mitigated by Sentinel quorum (good enough)\n\n**Should you add retry logic?**\n- **NO for writes** (ApplyVote) → Idempotency not guaranteed, risk duplicate votes\n- **YES for reads** (GetSentiment) → Safe to retry, improves availability\n- **Use built-in go-redis retry** (3 retries with exponential backoff, already enabled by default)\n\n**Should you add bulkhead pattern?**\n- **NO** - Overkill for single Redis instance\n- Useful if you have multiple Redis clusters (e.g., sessions vs cache vs queues)\n- Not applicable here\n\n**Should you fallback to PostgreSQL for session activation?**\n- **NO** - PostgreSQL doesn't have session state\n- Session activation requires Redis (session hash, broadcaster mapping, ref count)\n- If Redis unavailable, session activation SHOULD fail (return 503)\n\n**Network partition handling:**\n- Already handled by circuit breaker (timeouts → failures → circuit opens)\n- go-redis has built-in connection pool + health checks\n- Sentinel handles master unreachability (failover)\n- **No additional work needed**\n\n### 5. Observability for Resilience - Excellent + Additions\n\n**Your proposed metrics:**\n✅ Circuit breaker state changes\n✅ Replication lag\n✅ Failover events\n\n**Additional resilience metrics I recommend:**\n\n**Circuit breaker metrics (from my twitch-tow-sb3):**\n```go\nredis_circuit_state{operation} // 0=closed, 1=half-open, 2=open (gauge)\nredis_circuit_trips_total // counter (how many times circuit opened)\nredis_circuit_fallback_hits_total // counter (cache hits during outage)\n```\n\n**Redis operation metrics:**\n```go\nredis_operation_duration_seconds{operation, result} // histogram\nredis_operation_errors_total{operation, error_type} // counter\nredis_connection_pool_active // gauge (detect pool exhaustion)\nredis_connection_pool_idle // gauge\n```\n\n**Sentinel metrics:**\n```go\nredis_failover_total // counter (how many failovers)\nredis_failover_duration_seconds // histogram (recovery time)\nredis_master_last_failover_timestamp // gauge (when was last failover)\n```\n\n**Alerting rules (from my earlier input):**\n- redis_circuit_state == 2 → **ALERT** \"Redis circuit open\" (incident in progress)\n- redis_operation_duration_seconds{quantile=\"0.99\"} \u003e 0.1 → **WARN** \"Redis slow\"\n- redis_connection_errors_total rate \u003e 5/min → **ALERT** \"Redis connection issues\"\n- redis_replication_lag_seconds \u003e 1.0 → **WARN** \"Replica lag high\"\n\n---\n\n## Alignment with My Resilience Principles\n\n**Your solution STRONGLY ALIGNS with my beads:**\n\n✅ **twitch-tow-fgm (Circuit breaker)** - Directly implemented in Phase 1\n✅ **twitch-tow-bqx (Error handling)** - Structured errors (ErrRedisDegraded, ErrRedisUnavailable)\n✅ **twitch-tow-c8q (Observability)** - Excellent metrics coverage\n✅ **Fail-open philosophy** - Stale data fallback, continue on errors\n✅ **Graceful degradation** - Overlay degrades (yellow status) but doesn't crash\n\n**Synergy with my epic beads:**\n- **twitch-tow-sb3 (Circuit breaker epic)** - Your Phase 1 + my epic = comprehensive implementation\n- **twitch-tow-bqx (Error handling epic)** - Error classification (ErrRedisDegraded vs ErrRedisUnavailable)\n\n---\n\n## Final Recommendations\n\n1. ✅ **Adopt percentage-based circuit breaker** (60% failure rate over 10s window)\n2. ✅ **Use 30s timeout** (not 10s) for circuit breaker open duration\n3. ✅ **Return stale data** from cache when circuit open (Option A)\n4. ✅ **Accept 50-200ms replication lag** (acceptable for sentiment)\n5. ✅ **Use go-redis built-in retry** for reads (already enabled by default)\n6. ✅ **No bulkhead or Postgres fallback** (not needed)\n7. ✅ **Add circuit breaker metrics** (state, trips, fallback hits)\n\n---\n\n## Gaps/Concerns\n\n**Minor concern: Config cache invalidation**\n- You propose TTL-based (10s staleness)\n- Alternative: Pub/sub for real-time invalidation\n\n**My recommendation:** Start with TTL (simpler), add pub/sub only if users complain about 10s delay.\n\n**Rationale:** Config changes are rare (streamer adjusts triggers once per week). 10s delay acceptable.\n\n---\n\n## Consolidation with My Epics\n\n**Merge strategy:**\n- **Phase 1 (Circuit breaker):** Merge your ojd with my twitch-tow-sb3 epic\n  - Use my circuit breaker settings (60% failure ratio, 30s timeout)\n  - Use my fallback implementation (memory cache with TTL)\n  - Use my metrics (circuit state, trips, fallback hits)\n- **Phase 2 (Config cache):** Keep as-is from ojd (excellent design)\n- **Phase 3 (Sentinel):** Keep as-is from ojd (comprehensive deployment guide)\n- **Phase 4 (Read replicas):** Keep as-is from ojd (solid scaling plan)\n\n**Result:** 4-phase plan with enhanced Phase 1 circuit breaker implementation.\n\n---\n\n## Vote Summary\n\n**+1 STRONG APPROVAL**\n\nReady to implement with circuit breaker tuning suggestions. Excellent comprehensive solution that addresses Redis SPOF, scaling bottleneck, and graceful degradation.\n\n**Estimated effort:** 6 weeks (accurate)\n**Priority:** HIGH (Phase 1-2), MEDIUM (Phase 3-4)\n**Cost:** /month infrastructure (justified by availability gains)\nVote: +1 with recommendations from Maintainability Architect\n\nRATIONALE: 4-phase incremental rollout. Circuit breaker + caching solve immediate issues. Sentinel + replicas solve long-term scaling. BUT operational complexity increases.\n\nINTEGRATION: \n- ADR-001 update required (HA strategy, Sentinel vs Cluster, replication lag, failover)\n- ERROR_HANDLING.md update (circuit breaker states)\n- Operational runbooks needed (troubleshooting, manual failover, tuning)\n\nANSWERS:\n- Q1 (50-200ms staleness): YES - Already 50ms tick staleness. Document trade-off in ADR-001.\n- Q2 (circuit breaker behavior): Reads = stale data (lastKnownValue cache), Writes = error (fail-fast)\n- Q3 (pub/sub for config): NO - 10s TTL sufficient. KISS principle. Defer unless complaints.\n\nRECOMMENDATIONS:\n- Log circuit breaker state changes with structured context\n- Document cache invalidation (10s TTL + manual API)\n- Document Sentinel failure scenarios (quorum lost, master failover, split-brain)\n- Create runbooks: CB troubleshooting, Sentinel failover, threshold tuning\n\nEFFORT: 6 dev-weeks + 1 week docs","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:13:18.61938+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:56.90845+01:00","closed_at":"2026-02-12T17:56:56.90845+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics","labels":["consensus","phase-2-solution","redis","resilience"]}
{"id":"twitch-tow-op7","title":"Discussion: Test file organization - mixed patterns","description":"Test files show inconsistent organization patterns:\n\n**Pattern 1: Integration test files separate**\n- redis/: client_integration_test.go, session_repository_integration_test.go, sentiment_store_integration_test.go, debouncer_integration_test.go\n- database/: postgres_test.go (integration), user_repository_test.go (integration)\n- Suffix '_integration_test' clearly marks integration tests\n\n**Pattern 2: Unit + integration mixed**\n- app/service_test.go - contains both unit tests (with mocks) and integration tests (marked with t.Skip)\n- sentiment/engine_test.go - all unit tests with mocks\n\n**Pattern 3: Server tests by feature**\n- handlers_test.go (shared infrastructure)\n- handlers_unit_test.go (input validation)\n- handlers_auth_test.go, handlers_dashboard_test.go, handlers_api_test.go, handlers_overlay_test.go, handlers_csrf_test.go\n- Split by HTTP route grouping, not by test type\n\n**Concerns:**\n1. Hard to run ONLY integration tests (need to grep for _integration_test.go OR check for testing.Short())\n2. No consistent naming for test helpers (setupTestDB, setupTestClient, newTestServer)\n3. Some packages have integration_test.go (redis/), others embed in main test file (database/)\n\n**Current workaround**: Use -short flag to skip integration tests\n\n**Options:**\nA. Consolidate: Always use _integration_test.go suffix\nB. Split further: _unit_test.go + _integration_test.go everywhere\nC. Keep mixed: Argue that -short flag is sufficient\nD. Build tags: Use //go:build integration tag\n\n**Recommendation**: Standardize on _integration_test.go suffix OR document -short as the official convention.","notes":"RESOLVED: Converted to Epic 11 (twitch-tow-6zq) - Standardize Test File Organization. Implements: _integration_test.go suffix convention, make test-integration target, documentation in CLAUDE.md + README.md. 1.5 hours effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:58.950827+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:47:30.143755+01:00","closed_at":"2026-02-12T17:47:30.143761+01:00"}
{"id":"twitch-tow-oy1","title":"Solution: Standardized error handling patterns","description":"SOLUTION PROPOSAL for error handling concerns (twitch-tow-5sz, twitch-tow-uf7, twitch-tow-4vo, twitch-tow-heu)\n\nProblem: Two architects identified inconsistent error handling - mixed logging locations, wrapped vs sentinel errors, inconsistent structured logging, no error budgets.\n\nProposed Solution: Three-tier error classification\n\nTier A - Domain Errors (sentinel errors): ErrUserNotFound, ErrConfigNotFound, ErrSubscriptionNotFound. Expected business errors. Should NOT be logged (caller decides). Used for control flow.\n\nTier B - Infrastructure Errors (wrap with context): Database connection failures, Redis timeouts, Network errors, Twitch API failures. SHOULD be logged at origin with context. Wrap with fmt.Errorf.\n\nTier C - Programming Errors: Nil pointer dereferences, index out of bounds. Use panic, caught by middleware.\n\nLogging Rule: Log at decision boundary not at origin. Handlers log infrastructure errors, not repositories. Prevents double-logging.\n\nError Wrapping Standards: Infrastructure errors wrapped with context. Domain errors returned unwrapped to preserve sentinel semantics.\n\nStructured Logging Standards: Always include relevant context. Use consistent field names (user_id, session_uuid, broadcaster_user_id, error).\n\nImplementation: Week 1 - Add ERROR_HANDLING.md guide. Week 2 - Fix double-logging in app/service.go and broadcast/broadcaster.go. Week 3 - Add golangci-lint rules (errname, errorlint, wrapcheck).\n\nEstimated effort: 3 days total (4h docs, 16h refactoring, 2h linting).\n\nVote: +1 from Maintainability architect. Error handling consistency is crucial for debugging production issues.","notes":"Vote: +1 from architect-scalability\n\nRATIONALE: 3-tier classification (Transient/Permanent/User-facing) is clean taxonomy. ERROR_HANDLING.md separate doc is good for maintainability.\n\nPROPOSAL: Adopt as primary, merge twitch-tow-ay7 patterns. Combine classification with 6 patterns.\n\nSCALABILITY PERSPECTIVE: Transient error retries will reduce perceived failures. Classification helps with alerting thresholds.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:12:29.4051+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:57.664305+01:00","closed_at":"2026-02-12T17:56:57.664305+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics"}
{"id":"twitch-tow-pec","title":"Discussion: Deployment and rollback safety","description":"## Issue\nNo blue/green deployment support, no canary testing, and schema migrations run on every instance startup (potential race conditions).\n\n## Current State\n- Migrations run unconditionally at startup (main.go:110-113, postgres.go:36-69)\n- Tern uses public.schema_version table for coordination (good)\n- No health check endpoint for load balancer (cannot mark instance ready after migration)\n- Lua function REPLACE affects all instances immediately (no gradual rollout)\n- No version endpoint to verify running code version\n- Graceful shutdown implemented but no pre-stop hook for load balancer drain\n\n## Deployment Risks\n1. **Migration race**: Multiple instances starting simultaneously could conflict on migrations\n2. **Breaking migrations**: Schema change incompatible with old code → rolling deploy breaks\n3. **Lua function changes**: REPLACE overwrites for all instances (no canary)\n4. **No rollback plan**: Failed migration requires manual intervention\n5. **Health check gap**: Load balancer might route traffic before migrations complete\n\n## Rollback Challenges\n- Down migrations not automated (Tern supports them, but not invoked)\n- Encryption key changes cannot be rolled back (old tokens unreadable)\n- Lua function changes cannot be rolled back easily (no version history)\n- EventSub subscriptions persist (rollback doesn't clean up Twitch side)\n\n## Suggestions\n1. Add /ready endpoint that returns 200 only after migrations + Lua load complete\n2. Use advisory locks for migration coordination (pgx advisory lock API)\n3. Implement expand/contract migration pattern (backward-compatible schema changes)\n4. Add version endpoint (Git commit SHA, build timestamp)\n5. Document rollback procedure for each component (schema, Lua, secrets)\n6. Add migration smoke tests (run against test DB in CI)\n7. Consider separate migration job (runs once before deploy, not per-instance)\n8. Add pre-stop hook: mark instance unhealthy, drain WebSocket connections (30s), then exit\n9. Version Lua functions (apply_vote_v1 → apply_vote_v2) for gradual rollout\n10. Add deployment checklist to CLAUDE.md\n\n## Files\n- internal/database/postgres.go:36-69 (migrations run per-instance)\n- cmd/server/main.go:110-113 (migration failure is fatal)\n- internal/redis/client.go:23 (Lua function REPLACE)\n- No /ready endpoint in internal/server/","notes":"Comment from Maintainability: Critical analysis of deployment safety. Your suggestions 1-3 are essential: /ready endpoint (already in my observability proposal twitch-tow-1n1), advisory locks for migrations, and expand/contract pattern. Suggestions 7 and 8 are highest impact: separate migration job eliminates per-instance races, pre-stop hook enables zero-downtime deploys. Lua function versioning (suggestion 9) is clever but adds complexity. Current REPLACE is acceptable if we have rollback plan. Add to my documentation proposal (twitch-tow-e4l): deployment checklist in ADR-009. Estimated effort: 16 hours (ready endpoint 2h, advisory locks 4h, pre-stop hook 6h, deployment docs 4h). Priority: Should be P1 for production - deployment safety is critical. Vote: +1 for suggestions 1,2,3,7,8.\nRESOLVED: Already addressed by Epic 5 (twitch-tow-dmu) Task 5 - ADR-015: Deployment strategy and zero-downtime deploys. Covers: /ready endpoint, expand/contract migrations, Lua function versioning, graceful shutdown, pre-stop hook.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:54.044927+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:38:13.355726+01:00","closed_at":"2026-02-12T17:38:13.35573+01:00"}
{"id":"twitch-tow-php","title":"CONSOLIDATED: Comprehensive Observability - Health Checks + Metrics + Version Endpoint","description":"# CONSOLIDATED SOLUTION: Comprehensive Observability\n\nThis consolidates three aligned proposals:\n- **twitch-tow-eyl** (Scalability - 4-phase comprehensive plan)\n- **twitch-tow-3bx** (Maintainability - phased with /version endpoint)\n- **twitch-tow-1n1** (Maintainability - structured logging aligned)\n\n**Consensus:** All three architects voted +1. This merged solution adopts the best elements from each.\n\n---\n\n## Phase 1: Health Checks + Version Endpoint (Week 1)\n**Priority: CRITICAL** - blocks production deployment\n\n### New Endpoints\n\n**GET /health/live** - Liveness probe\n```go\nfunc (s *Server) handleLiveness(c echo.Context) error {\n    return c.JSON(200, map[string]string{\"status\": \"ok\"})\n}\n```\nAlways returns 200 OK. Used by load balancer to detect if process is alive.\n\n**GET /health/ready** - Readiness probe\n```go\nfunc (s *Server) handleReadiness(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 5*time.Second)\n    defer cancel()\n    \n    checks := []struct{\n        name string\n        fn func(context.Context) error\n    }{\n        {\"redis\", s.checkRedis},\n        {\"postgres\", s.checkPostgres},\n        {\"redis_functions\", s.checkRedisFunc},\n    }\n    \n    for _, check := range checks {\n        if err := check.fn(ctx); err != nil {\n            return c.JSON(503, map[string]any{\n                \"status\": \"unhealthy\",\n                \"failed_check\": check.name,\n                \"error\": err.Error(),\n            })\n        }\n    }\n    \n    return c.JSON(200, map[string]string{\"status\": \"ready\"})\n}\n```\n\nReadiness checks:\n- **Redis:** PING command (1s timeout)\n- **PostgreSQL:** SELECT 1 (1s timeout)\n- **Redis Functions:** Verify chatpulse library loaded\n- **Twitch EventSub:** Check conduit exists (if webhooks configured)\n\n**GET /version** - Build information *(added from twitch-tow-3bx)*\n```go\nvar (\n    // Set at build time via -ldflags\n    GitCommit  string\n    BuildTime  string\n    GoVersion  string = runtime.Version()\n)\n\nfunc (s *Server) handleVersion(c echo.Context) error {\n    return c.JSON(200, map[string]string{\n        \"git_commit\":  GitCommit,\n        \"build_time\":  BuildTime,\n        \"go_version\":  GoVersion,\n    })\n}\n```\n\nBuild command:\n```bash\ngo build -ldflags \"-X main.GitCommit=$(git rev-parse --short HEAD) -X main.BuildTime=$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n```\n\n### Implementation Files\n- `internal/server/handlers_health.go` (NEW - 150 lines)\n- `internal/server/routes.go` (add 3 routes)\n- `cmd/server/main.go` (add version variables)\n- `Makefile` (update build command with ldflags)\n\n### Testing\n- Unit tests with mocked dependencies (200 lines)\n- Integration test simulating Redis failure (100 lines)\n- Load balancer config examples (AWS ALB, Nginx, K8s, HAProxy)\n\n### Effort: 5 developer-days\n\n---\n\n## Phase 2: Prometheus Metrics (Week 2-3)\n**Priority: HIGH** - needed for production monitoring\n\n### Metrics Categories (22 total metrics)\n\n**1. Redis Operations**\n```go\nredis_operations_total{operation, status} // counter\nredis_operation_duration_seconds{operation} // histogram\nredis_connection_errors_total // counter\n```\n\n**2. Broadcaster**\n```go\nbroadcaster_active_sessions // gauge\nbroadcaster_connected_clients_total // gauge\nbroadcaster_tick_duration_seconds // histogram\nbroadcaster_slow_clients_evicted_total // counter\n```\n\n**3. WebSocket**\n```go\nwebsocket_connections_current // gauge\nwebsocket_connections_total{result} // counter\nwebsocket_message_send_duration_seconds // histogram\nwebsocket_connection_duration_seconds // histogram\n```\n\n**4. Vote Processing**\n```go\nvote_processing_total{result} // counter\nvote_processing_duration_seconds // histogram\nvote_trigger_matches_total{trigger_type} // counter\n```\n\n**5. Database**\n```go\ndb_query_duration_seconds{query} // histogram\ndb_connections_current{state} // gauge\ndb_errors_total{query} // counter\n```\n\n**6. HTTP (via Echo middleware)**\n```go\nhttp_requests_total{method, path, status} // counter\nhttp_request_duration_seconds{method, path} // histogram\n```\n\n### Implementation Approach\n\n**Use Redis hooks for automatic instrumentation:**\n```go\nclient.AddHook(\u0026MetricsHook{})\n\ntype MetricsHook struct{}\n\nfunc (h *MetricsHook) ProcessHook(next redis.ProcessHook) redis.ProcessHook {\n    return func(ctx context.Context, cmd redis.Cmder) error {\n        start := time.Now()\n        err := next(ctx, cmd)\n        \n        status := \"success\"\n        if err != nil {\n            status = \"error\"\n        }\n        \n        metrics.RedisOpsTotal.WithLabelValues(cmd.Name(), status).Inc()\n        metrics.RedisOpDuration.WithLabelValues(cmd.Name()).Observe(time.Since(start).Seconds())\n        \n        return err\n    }\n}\n```\n\n**Endpoint:**\n```go\nGET /metrics → Prometheus text format\ne.GET(\"/metrics\", echo.WrapHandler(promhttp.Handler()))\n```\n\n### Implementation Files\n- `internal/metrics/metrics.go` (NEW - 300 lines, all metric definitions)\n- `internal/redis/instrumented_client.go` (NEW - Redis hooks, 150 lines)\n- `internal/broadcast/broadcaster.go` (add metrics, 10 lines)\n- `internal/server/handlers_overlay.go` (add WS metrics, 15 lines)\n- `internal/sentiment/engine.go` (add vote metrics, 20 lines)\n- `internal/server/server.go` (add Echo Prometheus middleware)\n- `docs/deployment/prometheus-metrics.md` (NEW - comprehensive docs with alert examples)\n\n### Testing\n- Unit tests for metric registration (100 lines)\n- Integration test for metrics export (150 lines)\n- Performance test (overhead \u003c2% CPU)\n\n### Effort: 10 developer-days\n\n---\n\n## Phase 3: Structured Logging Improvements (Week 4)\n**Priority: MEDIUM** - current logging works but could be better\n\n*(Aligned with twitch-tow-1n1 maintainability proposal)*\n\n### Enhancements\n\n**Request ID propagation:**\n```go\nfunc requestIDMiddleware(next echo.HandlerFunc) echo.HandlerFunc {\n    return func(c echo.Context) error {\n        reqID := uuid.New().String()\n        c.Set(\"request_id\", reqID)\n        c.Response().Header().Set(\"X-Request-ID\", reqID)\n        \n        logger := slog.With(\"request_id\", reqID)\n        c.Set(\"logger\", logger)\n        \n        return next(c)\n    }\n}\n```\n\n**Consistent field names:**\n```\nsession_uuid (not sessionUUID or session_id)\nuser_id (not userID or user)\nerror (always lowercase)\nduration_ms (not latency or elapsed)\n```\n\n### Implementation Files\n- `internal/server/middleware.go` (NEW - request ID middleware)\n- Update all handlers to use consistent field names (15 files)\n\n### Effort: 5 developer-days\n\n---\n\n## Phase 4: Distributed Tracing (Future - DEFERRED)\n**Priority: LOW** - defer until multi-service architecture\n\n**When to implement:**\n- Multiple services communicating\n- Need to trace requests across services\n- Performance debugging requires detailed traces\n\n**Approach:** OpenTelemetry with Jaeger/Tempo backend\n\n**Defer because:**\n- Single monolith doesn't need distributed tracing\n- Prometheus metrics + logs sufficient for now\n- Added complexity not justified yet\n\n---\n\n## Success Criteria\n\n**Phase 1:**\n- ✅ Load balancer can route traffic to healthy instances\n- ✅ /version shows git commit + build timestamp\n- ✅ Health checks complete in \u003c1 second\n\n**Phase 2:**\n- ✅ Prometheus can scrape /metrics endpoint\n- ✅ 22+ metrics exported across all layers\n- ✅ Metrics used in Grafana dashboard\n- ✅ Alert rules configured for critical metrics\n- ✅ Metrics collection overhead \u003c2% CPU\n\n**Phase 3:**\n- ✅ Request IDs in all logs\n- ✅ Can trace request flow through components\n- ✅ Consistent field names across codebase\n\n---\n\n## Trade-offs\n\n**Pros:**\n+ Production-ready monitoring\n+ Can detect issues before users report them\n+ Enables capacity planning\n+ Required for SLO tracking\n+ /version endpoint aids debugging deployments\n\n**Cons:**\n- Adds dependencies (prometheus/client_golang)\n- Metrics collection has CPU/memory cost (~1-2% overhead)\n- More code to maintain\n\n**Verdict:** Essential for production. Small overhead is acceptable.\n\n---\n\n## Total Implementation Effort\n- **Phase 1:** 5 developer-days (health + version)\n- **Phase 2:** 10 developer-days (metrics)\n- **Phase 3:** 5 developer-days (logging)\n- **Total:** 20 developer-days (4 weeks)\n\n---\n\n## Dependencies\n**Adds:**\n- `github.com/prometheus/client_golang` v1.20+\n- `github.com/labstack/echo-contrib/echoprometheus` v4+\n\n---\n\n## Supersedes\nThis consolidated solution replaces:\n- twitch-tow-eyl (scalability)\n- twitch-tow-3bx (maintainability)\n- twitch-tow-1n1 (maintainability - logging aligned)\n\nAll three architects voted +1. Merging into single authoritative plan.","status":"open","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:32:27.312134+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:32:27.312134+01:00","labels":["phase-3-consolidated"]}
{"id":"twitch-tow-pnf","title":"Discussion: Context propagation and cancellation handling","description":"## Issue\nContext cancellation is not consistently checked in long-running operations. Some operations use background context instead of request context, losing cancellation signals.\n\n## Current State\n- Broadcaster tick uses context.Background() (broadcaster.go:223)\n- Orphan cleanup uses context.Background() for Twitch calls (app/service.go:200)\n- EnsureSessionActive uses request context correctly (good)\n- WebSocket handler doesn't check context in read pump loop (handlers_overlay.go:116-120)\n- Redis SCAN checks context between batches (good - session_repository.go:185-189)\n\n## Missing Cancellation Checks\n1. **Broadcaster tick loop**: Uses Background context, not cancellable (line 223)\n2. **Orphan cleanup Twitch calls**: Uses Background, ignores service stop signal (line 200)\n3. **WebSocket read pump**: Blocks on ReadMessage forever, ignores context (line 117)\n4. **ClientWriter run loop**: No context parameter, can't propagate shutdown (writer.go:40)\n\n## Risks\n- **Graceful shutdown delays**: Operations don't respect shutdown signal\n- **Resource cleanup**: Goroutines may not exit promptly on context cancel\n- **User experience**: Request timeouts don't cancel backend operations\n- **Testing**: Cannot simulate context cancellation in tests (timeout-based only)\n\n## Suggestions\n1. Add context parameter to Broadcaster tick loop (accept via command?)\n2. Add context parameter to clientWriter.run() (propagate from handler)\n3. Check context.Done() in WebSocket read pump (exit loop on cancel)\n4. Use service's cleanup context for Twitch calls (with timeout, not Background)\n5. Add context cancellation tests for all long-running operations\n6. Document context propagation patterns in CLAUDE.md\n7. Consider using errgroup for concurrent operations with shared context\n\n## Files\n- internal/broadcast/broadcaster.go:220-257 (tick uses Background)\n- internal/app/service.go:198-214 (cleanup uses Background)\n- internal/server/handlers_overlay.go:115-124 (read pump ignores context)\n- internal/broadcast/writer.go:40-64 (no context parameter)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:18.806982+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:45:30.538014+01:00","closed_at":"2026-02-12T17:45:30.538014+01:00","close_reason":"Duplicate of twitch-tow-euk, superseded by implementation epic twitch-tow-9c6 (Context Propagation and Configurable Timeouts). Epic addresses all context cancellation concerns: shutdown context, request context propagation, configurable timeouts."}
{"id":"twitch-tow-poa","title":"Add timeout to Redis scan operations","description":"**Low Priority (Performance)**\n\nLocation: internal/redis/session_repository.go lines 178-202\n\nIssue: Cleanup scan has no timeout. With millions of sessions, scan can take minutes.\n\nImpact: Architectural vulnerability for growth.\n\nFix:\n- Add timeout to scan context (e.g., 30 seconds)\n- Consider paginating cleanup\n- Add monitoring/metrics for scan duration\n- Consider alternative data structures","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:31.10049+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:45:00.393955+01:00","closed_at":"2026-02-12T16:45:00.393955+01:00","close_reason":"Closed"}
{"id":"twitch-tow-pye","title":"EPIC: Defensive Ref Counting with Anomaly Detection and Self-Healing","description":"**User Story:** As an operator, I need defensive ref counting with anomaly detection so that race conditions self-heal and ref count leaks are observable.\n\n**Problem Context:** Current ref counting has race conditions:\n- Connect/disconnect races across instances\n- Ref count leaks (instance crashes after INCR)\n- Ref count underflow (more DECR than INCR)\n- Concurrent activation + cleanup\n\nThese are partially mitigated by 30s grace period but can still cause:\n- Memory leaks (sessions never cleaned up)\n- Premature cleanup (active sessions deleted)\n\n**Solution Overview:** Add defensive checks with self-healing, metrics for anomaly detection, and ref count validation in critical paths. Accept eventual consistency as design philosophy.\n\n## Task Breakdown\n\n### 1. Add Defensive Ref Count Checks\n\n**File:** `internal/redis/session_repository.go`\n\n**Add validation to DecrRefCount:**\n```go\nfunc (r *SessionRepo) DecrRefCount(ctx context.Context, sessionUUID uuid.UUID) (int, error) {\n    key := fmt.Sprintf(\"ref_count:%s\", sessionUUID)\n    count, err := r.client.Decr(ctx, key).Result()\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to decrement ref count: %w\", err)\n    }\n    \n    // Defensive check: ref count went negative (anomaly)\n    if count \u003c 0 {\n        refCountAnomaliesTotal.WithLabelValues(\"negative\").Inc()\n        r.logger.Warn(\"ref count went negative, resetting to 0\",\n            \"session_uuid\", sessionUUID,\n            \"count\", count)\n        \n        // Self-heal: reset to 0\n        err = r.client.Set(ctx, key, 0, 0).Err()\n        if err != nil {\n            r.logger.Error(\"failed to reset negative ref count\", \"error\", err)\n        }\n        return 0, nil\n    }\n    \n    // Defensive check: suspiciously high ref count (likely leak)\n    if count \u003e 100 {\n        refCountAnomaliesTotal.WithLabelValues(\"high\").Inc()\n        r.logger.Warn(\"ref count suspiciously high\",\n            \"session_uuid\", sessionUUID,\n            \"count\", count)\n    }\n    \n    return int(count), nil\n}\n```\n\n**Add validation to IncrRefCount:**\n```go\nfunc (r *SessionRepo) IncrRefCount(ctx context.Context, sessionUUID uuid.UUID) (int, error) {\n    key := fmt.Sprintf(\"ref_count:%s\", sessionUUID)\n    count, err := r.client.Incr(ctx, key).Result()\n    if err != nil {\n        return 0, fmt.Errorf(\"failed to increment ref count: %w\", err)\n    }\n    \n    // Defensive check: first increment resulted in count \u003e 1 (missed decrement from previous lifecycle)\n    if count \u003e 1 \u0026\u0026 count \u003c 5 {\n        // Likely legitimate (multiple instances connecting)\n        // Just track it\n    } else if count \u003e= 5 {\n        refCountAnomaliesTotal.WithLabelValues(\"high_incr\").Inc()\n        r.logger.Warn(\"ref count already high on increment\",\n            \"session_uuid\", sessionUUID,\n            \"count\", count)\n    }\n    \n    return int(count), nil\n}\n```\n\n### 2. Add Ref Count Anomaly Metrics\n\n**File:** `internal/redis/session_repository.go`\n\n```go\nvar (\n    refCountAnomaliesTotal = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"ref_count_anomalies_total\",\n            Help: \"Total number of ref count anomalies detected\",\n        },\n        []string{\"type\"},  // \"negative\", \"high\", \"high_incr\"\n    )\n    \n    refCountDistribution = prometheus.NewHistogramVec(\n        prometheus.HistogramOpts{\n            Name: \"ref_count_distribution\",\n            Help: \"Distribution of ref count values\",\n            Buckets: []float64{0, 1, 2, 5, 10, 20, 50, 100},\n        },\n        []string{\"operation\"},  // \"incr\", \"decr\"\n    )\n)\n\nfunc init() {\n    prometheus.MustRegister(\n        refCountAnomaliesTotal,\n        refCountDistribution,\n    )\n}\n\n// Track distribution on every operation\nfunc (r *SessionRepo) IncrRefCount(...) (int, error) {\n    // ... existing logic\n    refCountDistribution.WithLabelValues(\"incr\").Observe(float64(count))\n    return int(count), nil\n}\n\nfunc (r *SessionRepo) DecrRefCount(...) (int, error) {\n    // ... existing logic\n    refCountDistribution.WithLabelValues(\"decr\").Observe(float64(count))\n    return int(count), nil\n}\n```\n\n### 3. Add Ref Count Audit Endpoint\n\n**File:** `internal/server/handlers_debug.go` (NEW)\n\n```go\nfunc (s *Server) handleRefCountAudit(c echo.Context) error {\n    ctx, cancel := context.WithTimeout(c.Request().Context(), 10*time.Second)\n    defer cancel()\n    \n    // List all ref counts (for debugging)\n    sessions, err := s.sessions.ListAllSessions(ctx)\n    if err != nil {\n        return c.JSON(500, map[string]string{\"error\": err.Error()})\n    }\n    \n    type refCountInfo struct {\n        SessionUUID string `json:\"session_uuid\"`\n        RefCount    int    `json:\"ref_count\"`\n        Anomaly     string `json:\"anomaly,omitempty\"`\n    }\n    \n    var results []refCountInfo\n    for _, sessionUUID := range sessions {\n        count, err := s.sessions.GetRefCount(ctx, sessionUUID)\n        if err != nil {\n            continue\n        }\n        \n        info := refCountInfo{\n            SessionUUID: sessionUUID.String(),\n            RefCount:    count,\n        }\n        \n        // Flag anomalies\n        if count \u003c 0 {\n            info.Anomaly = \"negative\"\n        } else if count \u003e 50 {\n            info.Anomaly = \"suspiciously_high\"\n        }\n        \n        results = append(results, info)\n    }\n    \n    return c.JSON(200, map[string]interface{}{\n        \"total_sessions\": len(results),\n        \"ref_counts\": results,\n    })\n}\n\n// Register in routes.go (debug endpoint, requires auth)\nfunc (s *Server) registerRoutes() {\n    // ... existing routes\n    \n    // Debug endpoints (auth required)\n    debug := s.e.Group(\"/debug\")\n    debug.Use(s.requireAuth)\n    debug.GET(\"/ref-counts\", s.handleRefCountAudit)\n}\n```\n\n### 4. Add Ref Count Reset Command\n\n**File:** `internal/redis/session_repository.go`\n\n```go\n// ResetRefCount resets ref count to correct value based on active sessions\nfunc (r *SessionRepo) ResetRefCount(ctx context.Context, sessionUUID uuid.UUID, correctCount int) error {\n    key := fmt.Sprintf(\"ref_count:%s\", sessionUUID)\n    \n    r.logger.Info(\"resetting ref count\",\n        \"session_uuid\", sessionUUID,\n        \"new_count\", correctCount)\n    \n    err := r.client.Set(ctx, key, correctCount, 0).Err()\n    if err != nil {\n        return fmt.Errorf(\"failed to reset ref count: %w\", err)\n    }\n    \n    refCountResetsTotal.Inc()\n    return nil\n}\n\nvar refCountResetsTotal = prometheus.NewCounter(\n    prometheus.CounterOpts{\n        Name: \"ref_count_resets_total\",\n        Help: \"Total number of manual ref count resets\",\n    })\n```\n\n### 5. Document Ref Counting Philosophy\n\n**File:** `CLAUDE.md`\n\nAdd section under \"Application Layer\":\n```markdown\n### Ref Counting Philosophy\n\nRef counting tracks how many instances are serving each session. It's **best-effort** by design, not strongly consistent.\n\n**Design principles:**\n- Eventual consistency acceptable (30s grace period absorbs most races)\n- Self-healing: anomalies logged + reset automatically\n- Defensive: negative counts reset to 0, high counts flagged\n\n**Known race conditions (accepted):**\n1. **Connect/disconnect race:** Session marked disconnected while new client connects\n   - Mitigation: 30s grace period, rarely overlaps\n   - Impact: Session might be deleted during reconnect (client sees \"reconnecting\" briefly)\n\n2. **Ref count leak:** Instance crashes after IncrRefCount\n   - Mitigation: High count alerts (\u003e50), manual reset endpoint\n   - Impact: Session never cleaned up (bounded by Redis restart)\n\n3. **Ref count underflow:** More decrements than increments\n   - Mitigation: Automatic reset to 0 when negative detected\n   - Impact: Session cleaned up prematurely\n\n4. **Concurrent activation + cleanup:** No distributed locks\n   - Mitigation: 30s grace period, idempotent EventSub subscribe\n   - Impact: Rare duplicate subscriptions (Twitch handles gracefully)\n\n**Metrics for observability:**\n- ref_count_anomalies_total{type=\"negative|high|high_incr\"}\n- ref_count_distribution{operation=\"incr|decr\"} (histogram)\n- ref_count_resets_total\n\n**Debug endpoint:** `GET /debug/ref-counts` (auth required) lists all ref counts, flags anomalies.\n\n**Trade-off:** Distributed locks would prevent races but add latency (Redis SETNX) and complexity. Current design favors availability over strict consistency.\n```\n\n### 6. Add Ref Count Background Health Check\n\n**File:** `internal/app/service.go`\n\n```go\n// StartRefCountHealthCheck periodically scans for anomalies\nfunc (s *Service) StartRefCountHealthCheck() {\n    go func() {\n        ticker := time.NewTicker(5 * time.Minute)\n        defer ticker.Stop()\n        \n        for range ticker.C {\n            ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)\n            s.scanRefCountAnomalies(ctx)\n            cancel()\n        }\n    }()\n}\n\nfunc (s *Service) scanRefCountAnomalies(ctx context.Context) {\n    sessions, err := s.sessions.ListAllSessions(ctx)\n    if err != nil {\n        s.logger.Error(\"failed to list sessions for ref count health check\", \"error\", err)\n        return\n    }\n    \n    var negativeCount, highCount int\n    for _, sessionUUID := range sessions {\n        count, err := s.sessions.GetRefCount(ctx, sessionUUID)\n        if err != nil {\n            continue\n        }\n        \n        if count \u003c 0 {\n            negativeCount++\n            // Auto-heal\n            s.sessions.ResetRefCount(ctx, sessionUUID, 0)\n        } else if count \u003e 100 {\n            highCount++\n            s.logger.Warn(\"detected high ref count\",\n                \"session_uuid\", sessionUUID,\n                \"count\", count)\n        }\n    }\n    \n    if negativeCount \u003e 0 || highCount \u003e 0 {\n        s.logger.Info(\"ref count health check completed\",\n            \"negative_counts\", negativeCount,\n            \"high_counts\", highCount,\n            \"total_sessions\", len(sessions))\n    }\n}\n```\n\n### 7. Testing\n\n**File:** `internal/redis/session_repository_refcount_test.go` (NEW)\n\n```go\n// TestRefCount_NegativeAutoReset verifies negative count self-heals\nfunc TestRefCount_NegativeAutoReset(t *testing.T) {\n    // Setup: Manually set ref count to -5\n    // Act: DecrRefCount\n    // Assert: Returns 0, ref count reset to 0, metric incremented\n}\n\n// TestRefCount_HighCountDetection verifies anomaly detection\nfunc TestRefCount_HighCountDetection(t *testing.T) {\n    // Setup: Session with ref count 150\n    // Act: IncrRefCount\n    // Assert: Anomaly metric incremented, warning logged\n}\n\n// TestRefCount_DistributionTracking verifies histogram\nfunc TestRefCount_DistributionTracking(t *testing.T) {\n    // Setup: Multiple incr/decr operations\n    // Assert: Histogram buckets contain expected counts\n}\n\n// TestRefCountHealthCheck verifies background scan\nfunc TestRefCountHealthCheck(t *testing.T) {\n    // Setup: 10 sessions (2 negative, 1 high, 7 normal)\n    // Act: scanRefCountAnomalies\n    // Assert: 2 resets, 1 warning log, metrics updated\n}\n```\n\n## Acceptance Criteria\n\n✅ Negative ref counts automatically reset to 0 (self-healing)\n✅ High ref counts (\u003e100) flagged with warning logs\n✅ Metrics track anomalies (negative, high, high_incr)\n✅ Ref count distribution histogram shows actual usage patterns\n✅ Debug endpoint lists all ref counts with anomaly flags\n✅ Manual reset endpoint available for operator intervention\n✅ Background health check scans every 5 minutes, auto-heals negative counts\n✅ CLAUDE.md documents ref counting philosophy and accepted trade-offs\n✅ Tests verify self-healing behavior and anomaly detection\n\n## Dependencies\n\n- Synergy with: Orphan cleanup robustness epic (twitch-tow-bo6)\n- Synergy with: Observability epic (metrics)\n\n## Files Modified\n\n**Modified:**\n- internal/redis/session_repository.go (defensive checks, metrics)\n- internal/app/service.go (background health check)\n- CLAUDE.md (document philosophy)\n\n**New:**\n- internal/server/handlers_debug.go (ref count audit endpoint)\n- internal/redis/session_repository_refcount_test.go (ref count tests)\n\n## Estimated Effort\n\n**Implementation:** 2 developer-days\n- Defensive checks: 3 hours\n- Metrics: 2 hours\n- Debug endpoint: 2 hours\n- Background health check: 2 hours\n- Testing: 1 day\n- Documentation: 2 hours\n\n**Total:** 2 developer-days\n\n## Rollout Strategy\n\n1. Deploy with metrics first (observe baseline anomaly rate)\n2. Monitor ref_count_anomalies_total (should be \u003c1/hour in healthy system)\n3. Enable background health check (5min interval)\n4. Alert if anomaly rate \u003e10/hour (investigate root cause)\n5. Use debug endpoint to audit ref counts manually if alerts fire","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:41:55.771801+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:44.868472+01:00"}
{"id":"twitch-tow-q21","title":"EPIC: Write Tier 3 Security/Resilience ADRs (3 ADRs)","description":"Write 3 ADRs covering security and resilience: UUID-based overlay access control, token encryption at rest, and graceful degradation philosophy. These explain production-readiness decisions.\n\n## User Story\nAs a security auditor or SRE, I need to understand the security model and failure handling philosophy so I can assess risk and configure monitoring correctly.\n\n## Value Proposition\n- Documents security boundaries and threat model\n- Explains encryption strategy for sensitive data\n- Clarifies eventual consistency philosophy\n- Enables informed production deployment\n\n## Tasks\n\n### 1. Write ADR-008: UUID-based overlay access control\n\n**Context:**\n- Overlay URLs embedded in OBS (public streaming software)\n- Viewers watch stream (see overlay), but shouldn't control sentiment\n- Need access control without adding complexity to OBS setup\n- CORS must allow all origins (OBS local file:// or streaming services)\n\n**Decision:**\n- Use separate **`overlay_uuid`** column (not user's internal `id`)\n- Overlay UUID acts as **bearer token** in URL: `/overlay/{uuid}`\n- No authentication required (public access if you know UUID)\n- UUID is **rotatable** via `POST /api/rotate-overlay-uuid` (invalidates old URLs)\n- CORS policy: accept all origins (required for OBS browser sources)\n\n**Threat model:**\n- ❌ **NOT protected against:** URL leakage (shared stream, network sniffing, OBS screenshots)\n- ✅ **Protected against:** Casual guessing (128-bit UUID = 2^128 space)\n- ✅ **Protected against:** Old URLs after rotation (lookup returns 404)\n\n**Alternatives considered:**\n1. **Streamer passwords** - Require password in query string\n   - Rejected: More complex OBS setup (need to configure password)\n   - Rejected: Same leakage risk as UUID (password in URL)\n   - Rejected: Harder to rotate (need to update OBS)\n\n2. **OAuth for viewers** - Viewers log in with Twitch to see overlay\n   - Rejected: Overlay is public (embedded in stream), viewers don't interact\n   - Rejected: Adds complexity for zero security gain (stream is public anyway)\n\n3. **Signed URLs with expiry** - JWT token in query string, expires after 24h\n   - Rejected: More complex (need signing key, expiry logic)\n   - Rejected: Rotation already solves expiry use case\n   - Rejected: OBS URLs shouldn't expire (annoying to update)\n\n**Consequences:**\n\n✅ **Positive:**\n- Zero-config OBS setup (just paste URL)\n- Simple rotation (one API call invalidates old URLs)\n- No CORS issues (all origins allowed)\n- Works with local files and streaming services\n\n❌ **Negative:**\n- URL leakage = public access (until rotated)\n- No fine-grained permissions (can't revoke specific viewers)\n- Screenshot of OBS config exposes overlay URL\n\n🔄 **Trade-offs:**\n- Chose simplicity over fine-grained control\n- Accept URL leakage risk (stream is public anyway)\n- Prioritize OBS ease-of-use over perfect security\n\n**Security posture:**\n- Overlay is **read-only** (viewers can't change sentiment)\n- Sentiment manipulation requires Twitch chat access (separate control)\n- Rotation mitigates leaked URLs (new UUID, old one 404s)\n\n**Related decisions:**\n- ADR-009: Token encryption (streamer credentials protected, overlay UUID is intentionally public)\n\n**Files to create:**\n- `docs/adr/008-uuid-overlay-access-control.md`\n\n**Time estimate:** 60 minutes\n\n### 2. Write ADR-009: AES-256-GCM token encryption at rest\n\n**Context:**\n- Store Twitch OAuth tokens in PostgreSQL (access + refresh tokens)\n- Database dumps may leak to untrusted parties (backups, logs, monitoring)\n- Need defense-in-depth for token security\n\n**Decision:**\n- Encrypt tokens with **AES-256-GCM** before storing in PostgreSQL\n- Encryption key from `TOKEN_ENCRYPTION_KEY` env var (64 hex chars = 32 bytes)\n- **Nonce prepended to ciphertext** (12-byte random nonce per encryption)\n- Hex-encoded for TEXT column storage (`24 nonce hex + 2N ciphertext hex`)\n- **`crypto.Service` interface** with two implementations:\n  - **`AesGcmCryptoService`** - Production (validates key length, encrypts/decrypts)\n  - **`NoopService`** - Dev/test (plaintext passthrough if key not set)\n\n**Implementation:**\n- `UserRepo` takes `crypto.Service` via constructor (injected from main.go)\n- Encrypt on write (`Upsert`, `UpdateTokens`)\n- Decrypt on read (`GetByID`, `GetByOverlayUUID`)\n- Key validation at startup (64 hex chars or empty)\n\n**Alternatives considered:**\n1. **Database-level encryption** - PostgreSQL transparent data encryption (TDE)\n   - Rejected: Requires PostgreSQL Enterprise or pgcrypto extension\n   - Rejected: Encrypts entire database (overkill, key management same)\n   - Rejected: Less portable (extension dependency)\n\n2. **No encryption** - Store tokens in plaintext\n   - Rejected: Fails security audit (tokens are sensitive credentials)\n   - Rejected: Database dumps expose all tokens\n   - Accepted only for dev/test (NoopService when key not set)\n\n3. **External KMS (AWS KMS, HashiCorp Vault)** - Fetch key from external service\n   - Rejected: Adds external dependency and network latency\n   - Rejected: Overkill for current scale (single encryption key)\n   - Future consideration: Can swap `crypto.Service` implementation later\n\n**Consequences:**\n\n✅ **Positive:**\n- Defense-in-depth (database dump doesn't leak tokens)\n- Application-level encryption (no PostgreSQL extensions)\n- Pluggable via interface (can swap to KMS later)\n- NoopService for dev (no key management burden)\n\n❌ **Negative:**\n- Key rotation is complex (need to re-encrypt all tokens)\n- Key must be securely stored (env var or secrets manager)\n- Encryption/decryption overhead (negligible \u003c1ms per token)\n\n🔄 **Trade-offs:**\n- Chose defense-in-depth over simplicity\n- Accept key rotation complexity for security\n- Prioritize token protection over zero-config dev setup (NoopService fallback)\n\n**Key management:**\n- Production: 32-byte random key, hex-encoded (`openssl rand -hex 32`)\n- Stored in `TOKEN_ENCRYPTION_KEY` env var (load from secrets manager)\n- Dev/test: Omit key → NoopService (plaintext)\n\n**Related decisions:**\n- ADR-007: Database vs Redis separation (tokens in PostgreSQL, not Redis)\n\n**Files to create:**\n- `docs/adr/009-token-encryption-at-rest.md`\n\n**Time estimate:** 60 minutes\n\n### 3. Write ADR-010: Graceful degradation - eventual consistency over strong consistency\n\n**Context:**\n- Distributed system across multiple instances + Redis + PostgreSQL\n- Network partitions, failures, race conditions inevitable\n- Need to choose consistency vs availability trade-offs (CAP theorem)\n\n**Decision:**\n- **Accept eventual consistency** throughout the system\n- **Prioritize availability** over strong consistency\n- **Design for self-healing** (30s cleanup timers, automatic recovery)\n- **No distributed locks** (Redlock, etcd) - use best-effort coordination\n\n**Examples of eventual consistency:**\n\n1. **Ref counting race conditions** (ADR-006)\n   - Two instances decrement simultaneously → orphaned key\n   - Self-healing: 30s timer deletes orphaned keys\n   - Trade-off: 30s window where session lingers in Redis\n\n2. **Config updates** (ADR-007)\n   - Streamer saves config → PostgreSQL updated\n   - Active sessions use stale config (snapshot in Redis)\n   - Self-healing: Next activation fetches fresh config\n   - Trade-off: Config changes not live (need reconnect)\n\n3. **Cleanup races** (ADR-006)\n   - Timer scans for orphans, instance activates session simultaneously\n   - Result: Session deleted, immediately recreated\n   - Self-healing: Correct end state (session active)\n   - Trade-off: Wasted work (delete + recreate)\n\n**Alternatives considered:**\n1. **Strong consistency with distributed locks** (Redlock)\n   - Rejected: Complex protocol (5 Redis nodes, clock skew)\n   - Rejected: Performance overhead (lock acquisition latency)\n   - Rejected: Availability risk (lock unavailable = operations blocked)\n\n2. **Linearizable consistency** (etcd, Raft)\n   - Rejected: Adds external dependency (etcd cluster)\n   - Rejected: Single leader becomes bottleneck\n   - Rejected: Over-engineering for sentiment bar (not financial transactions)\n\n3. **Sacrifice availability** - Block on failures, strong consistency\n   - Rejected: Sentiment bar should degrade gracefully (not fail hard)\n   - Rejected: Poor user experience (overlay goes dark on transient errors)\n\n**Consequences:**\n\n✅ **Positive:**\n- Simpler code (no lock management)\n- Faster operations (no lock acquisition)\n- Better availability (no blocking on failures)\n- Self-healing system (timers clean up inconsistencies)\n\n❌ **Negative:**\n- Race conditions possible (ref count, cleanup)\n- Eventual consistency (30s cleanup window)\n- Debugging harder (no single source of truth at any instant)\n\n🔄 **Trade-offs:**\n- Chose availability + simplicity over perfect consistency\n- Accept 30s eventual consistency window\n- Prioritize user experience (degrade gracefully) over strict correctness\n\n**Philosophy:**\n- **Design for failure** (assume race conditions will happen)\n- **Self-healing over prevention** (timers clean up, don't prevent races)\n- **Availability over consistency** (show stale data vs no data)\n\n**Related decisions:**\n- ADR-006: Ref counting (consequence: accept race conditions)\n- ADR-007: Config separation (consequence: stale config snapshots)\n\n**Files to create:**\n- `docs/adr/010-eventual-consistency-philosophy.md`\n\n**Time estimate:** 60 minutes\n\n### 4. Update ADR index page\n\nAdd Tier 3 ADRs to `docs/adr/README.md` table.\n\n**Time estimate:** 10 minutes\n\n### 5. Review and polish\n\n- Read all 3 ADRs for consistency\n- Cross-reference related decisions\n- Verify security posture explained clearly\n\n**Time estimate:** 20 minutes\n\n## Acceptance Criteria\n\n- ✅ All 3 ADRs written\n- ✅ Security threat model documented (ADR-008)\n- ✅ Encryption key management explained (ADR-009)\n- ✅ Consistency philosophy clarified (ADR-010)\n- ✅ Examples show real race conditions\n- ✅ Index page updated\n\n## Files Changed\n\n**Created:**\n- `docs/adr/008-uuid-overlay-access-control.md`\n- `docs/adr/009-token-encryption-at-rest.md`\n- `docs/adr/010-eventual-consistency-philosophy.md`\n\n**Modified:**\n- `docs/adr/README.md` (add Tier 3 links)\n\n## Dependencies\n- Epic 1 complete (Tier 1 foundation)\n- Epic 2 complete (ref counting context)\n\n## Effort Estimate\n**Total: 3 hours** (180 minutes)\n- ADR-008 (60 min): UUID access control\n- ADR-009 (60 min): Token encryption\n- ADR-010 (60 min): Eventual consistency\n- Polish (30 min): Review + index update","status":"closed","priority":2,"issue_type":"epic","assignee":"dev-quality","owner":"patrick.scheid@deepl.com","estimated_minutes":180,"created_at":"2026-02-12T17:27:45.771245+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:16:23.1015+01:00","closed_at":"2026-02-12T18:16:23.1015+01:00","close_reason":"Implemented all Tier 3 Security/Resilience ADRs. Created ADR-008 (UUID-based overlay access control), ADR-009 (Token encryption at rest), and ADR-010 (Eventual consistency philosophy). All ADRs follow consistent template with Context/Decision/Alternatives/Consequences sections. Updated README.md index with Tier 3 table. All acceptance criteria met. Committed in 790a138. Complete ADR suite: 10 ADRs total."}
{"id":"twitch-tow-qcb","title":"Fix Broadcaster command channel blocking potential","description":"**Medium Priority (Concurrency)**\n\nLocation: internal/broadcast/broadcaster.go lines 84-86, 96-98\n\nIssue: If broadcaster's run() goroutine is stuck, the reply channel will never be read. The sender blocks forever.\n\nImpact: Can cause handler goroutines to hang indefinitely during high load.\n\nFix:\n- Add timeout to channel receives using select with context\n- Example: select { case err := \u003c-errCh: return err; case \u003c-time.After(5*time.Second): return timeout error }","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:10.567397+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:46:13.538125+01:00","closed_at":"2026-02-12T16:46:13.538125+01:00","close_reason":"FIXED: Added 5-second timeout to Register() and GetClientCount() commands to prevent indefinite blocking when broadcaster's run loop is stuck. Register returns timeout error, GetClientCount returns -1 on timeout with WARNING log. Uses clock.NewTimer for testability. Added comprehensive test TestBroadcaster_CommandTimeoutHandling that verifies both commands timeout correctly when broadcaster is blocked. All 13 tests pass."}
{"id":"twitch-tow-qvk","title":"Discussion: Redis Lua function failure modes and atomicity guarantees","description":"## Issue\nRedis Functions (apply_vote, get_decayed_value) assume Redis is always available. No handling for function execution errors, invalid input, or concurrent modification scenarios.\n\n## Current State\n- Lua functions loaded once at startup (client.go:23)\n- No retry if FunctionLoadReplace fails (client closes, error returned)\n- No version tracking or migration path for Lua function changes\n- No input validation in Lua (assumes well-formed float args)\n- math.exp could overflow/underflow on extreme decay_rate or dt values\n- No handling for corrupted hash fields (non-numeric values)\n\n## Failure Modes\n1. **Function load failure**: Syntax error in Lua aborts startup (no fallback)\n2. **Concurrent modifications**: Multiple instances could race on hash updates\n3. **Numeric overflow**: Extreme decay values (large dt * decay_rate) could produce NaN/inf\n4. **Invalid state**: If 'value' or 'last_update' corrupted, tonumber() returns nil\n5. **Function version mismatch**: Rolling deploy with Lua changes could have mixed versions\n6. **No idempotency**: apply_vote called twice (retry) applies delta twice\n\n## Risks\n- **Data corruption**: Invalid hash values cause Lua function failures (nil propagation)\n- **Vote double-counting**: Retries without idempotency key apply vote multiple times\n- **Silent failures**: Lua errors return empty string, parsed as 0 in Go (line 39, 57)\n- **Deployment risk**: Lua changes require coordinated rollout (REPLACE affects all instances)\n\n## Suggestions\n1. Add input validation in Lua (bounds check on decay_rate, dt, delta)\n2. Handle nil values gracefully (default to 0 instead of error)\n3. Add idempotency: apply_vote checks debounce key in Lua (single atomic operation)\n4. Version Lua functions (e.g., apply_vote_v1) for rolling deploys\n5. Add integration tests for Lua edge cases (overflow, nil fields, concurrent calls)\n6. Log Lua function errors with full context (keys, args)\n7. Add fallback Go implementation if Lua unavailable (graceful degradation)\n8. Document Lua function deployment process (version migration steps)\n\n## Files\n- internal/redis/chatpulse.lua:1-30 (function implementations)\n- internal/redis/client.go:23-26 (function loading)\n- internal/redis/sentiment_store.go:26-43 (ApplyVote error handling)\n- internal/redis/sentiment_store.go:46-63 (GetSentiment error handling)","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:52.927764+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:48:03.249213+01:00","closed_at":"2026-02-12T17:48:03.249213+01:00","close_reason":"Superseded by implementation epic twitch-tow-bhh (Lua Function Robustness). Epic provides input validation with bounds checking, nil handling, overflow protection, error status codes, comprehensive edge case tests."}
{"id":"twitch-tow-rl8","title":"Discussion: Missing database indexes on high-traffic columns","description":"The PostgreSQL schema has PRIMARY KEY and UNIQUE constraints but may be missing indexes on columns used in hot query paths.\n\n**Current schema analysis:**\n\n**users table:**\n- id (PK) → indexed ✓\n- overlay_uuid (UNIQUE) → indexed ✓  \n- twitch_user_id (UNIQUE) → indexed ✓\n- All lookups use indexed columns ✓\n\n**configs table:**\n- user_id (PK, FK) → indexed ✓\n- Lookup by user_id only → optimal ✓\n\n**eventsub_subscriptions table:**\n- user_id (PK, FK) → indexed ✓\n- No queries by broadcaster_user_id or subscription_id\n- ListEventSubSubscriptions does full table scan (used rarely, small table)\n\n**Index analysis:**\n✓ All hot path queries use primary/unique indexes\n✓ No N+1 query patterns detected\n✓ JOIN operations minimal (handled by sqlc)\n\n**Query patterns:**\n1.  - uses overlay_uuid UNIQUE index ✓\n2.  - uses user_id PK ✓  \n3.  - uses id PK ✓\n4.  - uses twitch_user_id UNIQUE index for conflict detection ✓\n\n**Potential issues:**\n1.  has no WHERE clause (full scan) but:\n   - Only called on startup to restore subscriptions\n   - Table size = active streamer count (small)\n   - Not a performance concern\n   \n2. No composite indexes but no queries need them\n\n**Recommendation:**\n✓ **Current indexing is optimal** - no changes needed\n- All user-facing queries hit indexed columns\n- EventSub full scan is acceptable (startup only, small table)\n- No missing indexes detected\n\n**Future considerations:**\n- If EventSub queries by subscription_id are added, index it\n- If filtering configs by trigger values is needed, add GIN/trigram indexes\n- Monitor query performance with pg_stat_statements\n\nPriority: P3 (low) - no action needed, indexing is appropriate","notes":"ANALYZED: Current database indexing is already optimal. All hot-path queries use primary key or unique indexes. EventSub ListSubscriptions does full table scan but this is acceptable (startup only, small table). No missing indexes detected. No action needed. Future: If EventSub queries by subscription_id are added, index that column. For now, schema is production-ready.","status":"closed","priority":3,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:19.379947+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:49:17.622257+01:00","closed_at":"2026-02-12T17:49:17.62226+01:00"}
{"id":"twitch-tow-rn7","title":"EPIC: Instance Coordination - Registry + Pub/Sub + Leader Election","description":"## Epic Overview\nImplement lightweight instance coordination mechanisms using Redis to enable fleet visibility, real-time config updates, and leader election for singleton tasks.\n\n## Parent Discussion\ntwitch-tow-wfl (No instance coordination beyond shared Redis state)\n\n## User Story\nAs an operator managing multiple ChatPulse instances, I need visibility into the active fleet and coordination for singleton tasks so I can monitor fleet health and eliminate duplicate work like orphan cleanup.\n\n## Problem Analysis\n\n**Current state:**\n- ✅ Session state shared via Redis\n- ✅ Ref counting uses atomic Redis operations\n- ❌ No instance discovery (can't enumerate fleet)\n- ❌ No leader election (orphan cleanup duplicated)\n- ❌ No pub/sub (config updates require 10s polling)\n\n**Impact:**\n- Orphan cleanup runs on every instance (wasteful Twitch API calls)\n- Config changes take 10s to propagate (TTL-based cache)\n- No fleet-wide health visibility\n- Can't distribute work intelligently\n\n## Solution: Three-Phase Coordination\n\n### Phase 1: Instance Registry (Week 1) ✅\n\nTrack active instances in Redis:\n\n```go\ntype InstanceRegistry struct {\n    redis      *redis.Client\n    instanceID string\n    heartbeat  time.Duration\n}\n\nfunc NewInstanceRegistry(redis, instanceID, heartbeat) *InstanceRegistry {\n    return \u0026InstanceRegistry{\n        redis:      redis,\n        instanceID: instanceID,\n        heartbeat:  heartbeat,\n    }\n}\n\nfunc (r *InstanceRegistry) Start(ctx context.Context) {\n    ticker := time.NewTicker(r.heartbeat)\n    defer ticker.Stop()\n    \n    for {\n        select {\n        case \u003c-ticker.C:\n            r.register()\n        case \u003c-ctx.Done():\n            r.unregister()\n            return\n        }\n    }\n}\n\nfunc (r *InstanceRegistry) register() {\n    key := \"instances\"\n    value := map[string]interface{}{\n        \"timestamp\": time.Now().Unix(),\n        \"version\":   GitCommit,\n    }\n    \n    data, _ := json.Marshal(value)\n    r.redis.HSet(context.Background(), key, r.instanceID, data)\n}\n\nfunc (r *InstanceRegistry) GetActiveInstances() ([]string, error) {\n    instances, err := r.redis.HGetAll(context.Background(), \"instances\").Result()\n    if err != nil {\n        return nil, err\n    }\n    \n    active := []string{}\n    now := time.Now().Unix()\n    \n    for instanceID, data := range instances {\n        var value map[string]interface{}\n        json.Unmarshal([]byte(data), \u0026value)\n        \n        timestamp := int64(value[\"timestamp\"].(float64))\n        if now-timestamp \u003c 60 {  // Active if heartbeat within 60s\n            active = append(active, instanceID)\n        }\n    }\n    \n    return active, nil\n}\n```\n\n**Benefits:**\n- Know how many instances are running\n- Detect instance failures (no heartbeat for 60s)\n- Foundation for leader election\n- Enable fleet-wide metrics aggregation\n\n### Phase 2: Pub/Sub for Config Updates (Week 2) ✅\n\nBroadcast config changes to all instances:\n\n```go\ntype ConfigInvalidator struct {\n    redis  *redis.Client\n    engine *sentiment.Engine\n}\n\n// Publisher (in app.Service.SaveConfig)\nfunc (s *Service) SaveConfig(ctx, userID, updates) error {\n    if err := s.configs.Update(ctx, userID, updates); err != nil {\n        return err\n    }\n    \n    // Invalidate local cache\n    user, _ := s.users.GetByID(ctx, userID)\n    s.engine.InvalidateConfigCache(user.OverlayUUID)\n    \n    // Broadcast to other instances\n    s.redis.Publish(ctx, \"config:invalidate\", user.OverlayUUID.String())\n    \n    return nil\n}\n\n// Subscriber (in cmd/server/main.go)\nfunc (c *ConfigInvalidator) Start(ctx context.Context) {\n    pubsub := c.redis.Subscribe(ctx, \"config:invalidate\")\n    defer pubsub.Close()\n    \n    ch := pubsub.Channel()\n    for msg := range ch {\n        overlayUUID, err := uuid.Parse(msg.Payload)\n        if err != nil {\n            continue\n        }\n        \n        c.engine.InvalidateConfigCache(overlayUUID)\n        slog.Debug(\"Config cache invalidated via pub/sub\", \"overlay_uuid\", overlayUUID)\n    }\n}\n```\n\n**Benefits:**\n- Config changes propagate in \u003c1 second (down from 10s)\n- Reduced Redis polling load\n- Better user experience (immediate updates)\n\n### Phase 3: Leader Election for Cleanup (Week 3) ✅\n\nSingle leader runs orphan cleanup:\n\n```go\ntype LeaderElection struct {\n    redis      *redis.Client\n    instanceID string\n    ttl        time.Duration\n}\n\nfunc (l *LeaderElection) TryBecomeLeader(ctx context.Context) (bool, error) {\n    key := \"leader:orphan_cleanup\"\n    success, err := l.redis.SetNX(ctx, key, l.instanceID, l.ttl).Result()\n    return success, err\n}\n\nfunc (l *LeaderElection) RenewLease(ctx context.Context) error {\n    key := \"leader:orphan_cleanup\"\n    \n    // Only renew if we're still the leader\n    script := `\n    if redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n        return redis.call(\"EXPIRE\", KEYS[1], ARGV[2])\n    else\n        return 0\n    end\n    `\n    \n    return l.redis.Eval(ctx, script, []string{key}, l.instanceID, int(l.ttl.Seconds())).Err()\n}\n\n// In app.Service.CleanupOrphans\nfunc (s *Service) CleanupOrphans(ctx context.Context) error {\n    // Try to become leader\n    isLeader, err := s.leaderElection.TryBecomeLeader(ctx)\n    if err != nil || !isLeader {\n        slog.Debug(\"Not leader for orphan cleanup\", \"instance_id\", s.instanceID)\n        return nil\n    }\n    \n    slog.Info(\"Running orphan cleanup as leader\", \"instance_id\", s.instanceID)\n    \n    // Run cleanup (existing logic)\n    orphans, err := s.sessions.ListOrphans(ctx, 30*time.Second)\n    // ...\n    \n    return nil\n}\n```\n\n**Benefits:**\n- Single instance runs cleanup (no duplicate Twitch API calls)\n- Automatic failover if leader crashes (TTL expires)\n- Reduced Twitch API usage\n\n## Implementation Tasks\n\n### Task 1: Implement instance registry\n**Files:** `internal/coordination/registry.go` (NEW, 150 lines)\n\nCreate InstanceRegistry with heartbeat mechanism.\n\n### Task 2: Add registry to main.go\n**File:** `cmd/server/main.go`\n\nStart registry goroutine:\n```go\nregistry := coordination.NewInstanceRegistry(redisClient, instanceID, 30*time.Second)\ngo registry.Start(ctx)\n```\n\n### Task 3: Implement config pub/sub\n**Files:** `internal/coordination/pubsub.go` (NEW, 100 lines)\n\nCreate ConfigInvalidator subscriber.\n\n### Task 4: Add pub/sub publisher\n**File:** `internal/app/service.go`\n\nPublish config invalidation in SaveConfig.\n\n### Task 5: Implement leader election\n**Files:** `internal/coordination/leader.go` (NEW, 120 lines)\n\nCreate LeaderElection with SETNX + Lua renewal.\n\n### Task 6: Use leader election in cleanup\n**File:** `internal/app/service.go`\n\nWrap CleanupOrphans with leader check.\n\n### Task 7: Add coordination metrics\n**File:** `internal/metrics/metrics.go`\n\n```go\nInstanceRegistrySize = promauto.NewGauge(...)\nPubSubMessagesReceived = promauto.NewCounter(...)\nLeaderElections = promauto.NewCounter(...)\n```\n\n### Task 8: Unit tests\n**File:** `internal/coordination/coordination_test.go` (NEW, 300 lines)\n\nTest registry, pub/sub, leader election with mock Redis.\n\n### Task 9: Integration test\n**File:** `internal/coordination/coordination_integration_test.go` (NEW, 200 lines)\n\nTest with 3 real instances, verify leader election and failover.\n\n### Task 10: Documentation\n**File:** `docs/architecture/instance-coordination.md` (NEW, 250 lines)\n\nDocument coordination mechanisms and operational implications.\n\n## Acceptance Criteria\n\n- ✅ Instance registry tracks active instances with 30s heartbeat\n- ✅ Pub/sub propagates config changes in \u003c1 second\n- ✅ Leader election ensures single instance runs cleanup\n- ✅ Leader failover works (new leader elected within 60s)\n- ✅ Metrics track registry size and pub/sub messages\n- ✅ Integration test with 3 instances passes\n\n## Files Created/Modified\n\n**New files:**\n- `internal/coordination/registry.go` (150 lines)\n- `internal/coordination/pubsub.go` (100 lines)\n- `internal/coordination/leader.go` (120 lines)\n- `internal/coordination/coordination_test.go` (300 lines)\n- `internal/coordination/coordination_integration_test.go` (200 lines)\n- `docs/architecture/instance-coordination.md` (250 lines)\n\n**Modified files:**\n- `cmd/server/main.go` (start registry + pubsub, 20 lines)\n- `internal/app/service.go` (publish config invalidation, leader election, 30 lines)\n- `internal/metrics/metrics.go` (coordination metrics, 20 lines)\n\n## Dependencies\n- Complements config cache (twitch-tow-4c4)\n- Enables distributed orphan cleanup optimization\n\n## Success Metrics\n- Config updates propagate in \u003c1s (down from 10s)\n- Orphan cleanup runs on 1 instance only (down from N)\n- Instance registry size matches deployed instance count\n- Leader election completes in \u003c5s\n\n## Effort Estimate\n**5 developer-days** (1 week)\n\nBreakdown:\n- Registry: 1 day\n- Pub/sub: 1 day\n- Leader election: 1.5 days\n- Tests: 1 day\n- Documentation: 0.5 day\n\n## Trade-offs\n\n**Pros:**\n+ Fleet visibility\n+ Real-time config updates\n+ Eliminates duplicate cleanup work\n+ Simple Redis-based solution\n\n**Cons:**\n- More moving parts (registry, pub/sub, leader election)\n- Redis pub/sub not guaranteed delivery\n- Leader election requires careful TTL tuning\n\n## Not Implemented (Deferred)\n\n- **Service mesh:** Overkill for current scale\n- **Distributed tracing:** Covered in observability Phase 4\n- **Config broadcast for non-config changes:** Pub/sub only for config invalidation\n\n## Risk Mitigation\n\n- **Risk:** Pub/sub message loss (Redis restart)\n  - **Mitigation:** TTL-based cache as fallback (10s max staleness)\n- **Risk:** Split brain (multiple leaders)\n  - **Mitigation:** Lua script ensures atomic leader renewal\n- **Risk:** Registry heartbeat stops (network partition)\n  - **Mitigation:** 60s timeout removes stale instances","status":"open","priority":3,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:43:43.657336+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:51.039596+01:00"}
{"id":"twitch-tow-s7p","title":"Idea: Consolidate domain package files","description":"The domain package is split into 9 concept-oriented files (errors.go, user.go, config.go, session.go, sentiment.go, debounce.go, engine.go, twitch.go, app.go). Each file is very small:\n\n- errors.go: 10 lines (3 sentinel errors)\n- engine.go: 14 lines (1 interface, 3 methods)\n- debounce.go: ~10 lines (1 interface, 1 method)\n- sentiment.go: ~14 lines (1 interface, 3 methods)\n\n**Current structure** prioritizes semantic grouping over discoverability.\n\n**Concerns**:\n1. High cognitive load - need to open 9 files to understand domain contracts\n2. IDE/editor overhead - more files to search through\n3. Git noise - simple interface changes touch multiple files\n4. Not following Go conventions - most packages consolidate related types\n\n**Options**:\nA. **Consolidate by layer**: domain.go (all types), repositories.go (all repos), services.go (all services)\nB. **Consolidate by aggregate**: user.go (User + UserRepository), session.go (all session-related), twitch.go (all Twitch-related)\nC. **Keep current** - argue that semantic separation is worth the navigation cost\n\n**Benchmark**: pgx/v5 (similar complexity) uses ~4-5 files. This codebase has 9 for a much simpler domain.","notes":"RESOLVED: Current structure (9 concept-oriented files) is intentional and appropriate. Benefits: clear semantic grouping, prevents circular imports, easy to find specific contracts. The cognitive load concern is addressed by package docs (Epic 6 adds doc.go). Small files (10-14 lines) are fine in Go. No consolidation needed - pattern is working well for this domain complexity.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:04:05.465461+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:50:42.231895+01:00","closed_at":"2026-02-12T17:50:42.231899+01:00"}
{"id":"twitch-tow-sb3","title":"EPIC: Redis Circuit Breaker for Graceful Degradation","description":"Epic: Redis Circuit Breaker for Graceful Degradation\n\n**User Story:** As a system, I need to gracefully degrade when Redis is unavailable rather than cascading failures across all WebSocket connections.\n\n**Problem Context:** Currently, Redis failures (network issues, instance restarts, capacity limits) cause immediate failures across all operations. This leads to cascading failures where every WebSocket broadcast attempt fails, impacting all streamers simultaneously.\n\n**Solution Overview:** Implement circuit breaker pattern around Redis operations using sony/gobreaker. Circuit opens after sustained failures, fails fast during outages, and automatically recovers when Redis becomes healthy.\n\n## Task Breakdown\n\n### 1. Add Circuit Breaker Dependency\n- Add `github.com/sony/gobreaker v1.0.0` to go.mod\n- Run `make deps` to download\n- **Files:** go.mod, go.sum\n\n### 2. Create Circuit Breaker Wrapper (internal/redis/circuit_breaker.go)\n```go\npackage redis\n\nimport (\n    \"context\"\n    \"time\"\n    \"github.com/redis/go-redis/v9\"\n    \"github.com/sony/gobreaker\"\n)\n\ntype CircuitBreakerRedis struct {\n    client *redis.Client\n    cb     *gobreaker.CircuitBreaker\n}\n\nfunc NewCircuitBreakerRedis(client *redis.Client) *CircuitBreakerRedis {\n    settings := gobreaker.Settings{\n        Name:        \"redis\",\n        MaxRequests: 3,  // Half-open: 3 requests before closing\n        Interval:    10 * time.Second,  // Reset failure count every 10s\n        Timeout:     30 * time.Second,  // Stay open for 30s before half-open\n        ReadyToTrip: func(counts gobreaker.Counts) bool {\n            // Open after 60% failure rate with min 5 requests\n            failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)\n            return counts.Requests \u003e= 5 \u0026\u0026 failureRatio \u003e= 0.6\n        },\n    }\n    \n    return \u0026CircuitBreakerRedis{\n        client: client,\n        cb:     gobreaker.NewCircuitBreaker(settings),\n    }\n}\n\n// Wrap critical operations\nfunc (cbr *CircuitBreakerRedis) HGet(ctx context.Context, key, field string) *redis.StringCmd {\n    cmd := redis.NewStringCmd(ctx)\n    _, err := cbr.cb.Execute(func() (interface{}, error) {\n        result := cbr.client.HGet(ctx, key, field)\n        return nil, result.Err()\n    })\n    if err != nil {\n        cmd.SetErr(err)\n    } else {\n        cmd = cbr.client.HGet(ctx, key, field)\n    }\n    return cmd\n}\n\nfunc (cbr *CircuitBreakerRedis) State() gobreaker.State {\n    return cbr.cb.State()\n}\n```\n\n### 3. Integrate Circuit Breaker into SessionRepo/SentimentStore\n- Modify `redis.NewClient()` to optionally wrap client with circuit breaker\n- Add `ENABLE_REDIS_CIRCUIT_BREAKER` env var (default: true in production)\n- Pass wrapped client to SessionRepo/SentimentStore constructors\n- **Files:** internal/redis/client.go, internal/redis/session_repository.go\n\n### 4. Implement Fallback Behavior\n**GetSentiment (read-only):**\n- Circuit open → return cached value with `status: \"degraded\"`\n- Cache last known value in memory (sync.Map, TTL 60s)\n\n**ApplyVote (write):**\n- Circuit open → drop vote silently (better than failing broadcast)\n- Log dropped votes for monitoring\n\n**GetSessionConfig (critical):**\n- Circuit open → return error (prevents session activation with stale config)\n\n**IncrRefCount/DecrRefCount:**\n- Circuit open → log warning, continue (ref count inconsistency is recoverable)\n\n### 5. Add Metrics (Prometheus)\n```go\nvar (\n    redisCircuitStateGauge = prometheus.NewGaugeVec(\n        prometheus.GaugeOpts{\n            Name: \"redis_circuit_state\",\n            Help: \"Current state of Redis circuit breaker (0=closed, 1=half-open, 2=open)\",\n        },\n        []string{\"operation\"},\n    )\n)\n\n// Export state as metric (0=closed, 1=half-open, 2=open)\nfunc (cbr *CircuitBreakerRedis) updateMetrics() {\n    state := cbr.cb.State()\n    var stateValue float64\n    switch state {\n    case gobreaker.StateClosed: stateValue = 0\n    case gobreaker.StateHalfOpen: stateValue = 1\n    case gobreaker.StateOpen: stateValue = 2\n    }\n    redisCircuitStateGauge.WithLabelValues(\"redis\").Set(stateValue)\n}\n```\n**Files:** internal/redis/circuit_breaker.go (add metrics)\n\n### 6. Update Config Loading\nAdd to `.env.example`:\n```\n# Circuit Breaker (optional, default: true)\nENABLE_REDIS_CIRCUIT_BREAKER=true\n```\nAdd to `internal/config/config.go`:\n```go\ntype Config struct {\n    // ... existing fields\n    EnableRedisCircuitBreaker bool `env:\"ENABLE_REDIS_CIRCUIT_BREAKER\" envDefault:\"true\"`\n}\n```\n\n### 7. Testing\n**Unit tests (internal/redis/circuit_breaker_test.go):**\n- Circuit opens after 60% failure rate\n- Circuit stays closed with \u003c60% failures\n- Half-open transitions after timeout\n- Half-open closes after 3 successes\n- Operations fail fast when circuit open\n\n**Integration tests:**\n- Use testcontainers to pause/unpause Redis container\n- Verify fallback behavior during outage\n- Verify recovery after Redis returns\n- Measure fail-fast latency (should be \u003c1ms vs 5s timeout)\n\n**Manual test:**\n```bash\n# Stop Redis mid-session\ndocker compose stop redis\n# Observe circuit opens within 10s\n# Observe WebSocket still broadcasts (with degraded status)\ndocker compose start redis\n# Observe circuit closes after 3 successful ops\n```\n\n### 8. Documentation Updates\n**CLAUDE.md additions:**\n- Circuit Breaker section under \"Redis Architecture\"\n- Document thresholds (60% failure, 10s window, 30s open duration)\n- Document fallback behavior per operation\n- Document metrics\n- Document configuration\n\n## Acceptance Criteria\n\n✅ Circuit opens after 60% failures in 10s window (min 5 requests)\n✅ Circuit stays open for 30s before attempting recovery\n✅ Circuit requires 3 consecutive successes in half-open to close\n✅ Operations fail fast (\u003c1ms) when circuit open\n✅ GetSentiment returns cached value with `status: \"degraded\"` during outage\n✅ ApplyVote drops votes silently during outage (logged)\n✅ Metrics track circuit state changes\n✅ Tests verify all state transitions\n✅ Integration test simulates Redis outage and recovery\n✅ CLAUDE.md documents behavior and configuration\n\n## Dependencies\n\n- Depends on: None (can be implemented independently)\n- Blocks: Epic: Redis HA Setup (circuit breaker useful during failovers)\n\n## Files Modified/Created\n\n**New files:**\n- internal/redis/circuit_breaker.go (~200 lines)\n- internal/redis/circuit_breaker_test.go (~300 lines)\n\n**Modified files:**\n- go.mod (add gobreaker dependency)\n- internal/config/config.go (add ENABLE_REDIS_CIRCUIT_BREAKER field)\n- internal/redis/client.go (wrap client conditionally)\n- internal/redis/session_repository.go (handle circuit open state)\n- internal/redis/sentiment_store.go (cache last value for fallback)\n- .env.example (document new env var)\n- CLAUDE.md (document circuit breaker architecture)\n\n## Estimated Effort\n\n**Implementation:** 3-4 developer-days\n**Testing:** 2 developer-days\n**Documentation:** 1 developer-day\n**Total:** ~1 developer-week\n\n## Rollout Strategy\n\n1. Deploy with `ENABLE_REDIS_CIRCUIT_BREAKER=false` initially\n2. Monitor Redis error rates in production for 1 week\n3. Enable circuit breaker in staging, simulate Redis failures\n4. Enable in production gradually (10% → 50% → 100% of instances)\n5. Monitor circuit state metrics, adjust thresholds if needed","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:25:15.937054+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:48.82019+01:00"}
{"id":"twitch-tow-tsg","title":"EPIC: Vote Rate Limiting - Token Bucket Rate Limiter to Prevent Bot Spam","description":"## Epic Overview\nImplement token bucket rate limiter in Redis to prevent bot spam attacks on vote processing while allowing legitimate burst activity during high-engagement moments.\n\n## User Story\nAs an operator, I need vote rate limiting per session to prevent coordinated bot attacks from manipulating sentiment while allowing natural chat bursts during exciting moments.\n\n## Parent Solution\ntwitch-tow-tt0 (Multi-layer Resource Limits and Rate Limiting)\n\n## Dependencies\n- twitch-tow-dmg (Connection Limits) - should be complete first\n\n## Problem Analysis\n\n**Current vulnerability:**\n- Per-user debounce (1 second) prevents one user from spam voting\n- But 1,000 unique bots can each vote once = 1,000 votes instantly\n- No per-session rate limiting\n\n**Attack scenario:**\n- Attacker deploys 1,000 Twitch bots\n- Bots join target channel\n- Each bot sends trigger word once\n- Sentiment bar maxes out instantly\n- Repeat attack every second\n\n**Solution:**\n- Token bucket rate limiter per session\n- Allows bursts (100 tokens immediately) for legitimate excitement\n- Limits sustained rate (100 votes/minute) to prevent bot spam\n- Implemented in Redis (multi-instance safe)\n\n## Technical Requirements\n\n### Token Bucket Parameters\n- **Capacity:** 100 tokens (burst allowance)\n- **Refill rate:** 100 tokens per minute (1.67 tokens/second)\n- **Behavior:** Allows 100 votes immediately, then 100/minute sustained\n\n### Why Token Bucket?\n- **Better UX than fixed window:** Allows natural bursts\n- **Fair:** Refills gradually, no boundary abuse\n- **Production-ready:** Used by AWS, GitHub, Cloudflare\n\n### Implementation Location\n- Redis Lua script (atomic, multi-instance safe)\n- Executed in `Engine.ProcessVote` pipeline\n- After debounce check, before `ApplyVote`\n\n## Implementation Tasks\n\n### Task 1: Create token bucket Lua script\n**File:** `internal/redis/vote_rate_limit.lua` (NEW)\n```lua\n#!lua name=vote_rate_limit\n\nlocal function check_rate_limit(keys, args)\n    local key = keys[1]\n    local now = tonumber(args[1])\n    local capacity = tonumber(args[2])\n    local rate_per_minute = tonumber(args[3])\n    \n    -- Get current state\n    local tokens = redis.call('HGET', key, 'tokens')\n    local last_update = redis.call('HGET', key, 'last_update')\n    \n    -- Initialize if first call\n    if not tokens then\n        tokens = capacity\n        last_update = now\n    else\n        tokens = tonumber(tokens)\n        last_update = tonumber(last_update)\n    end\n    \n    -- Calculate new tokens (refill based on time elapsed)\n    local elapsed_ms = math.max(0, now - last_update)\n    local tokens_to_add = (elapsed_ms / 60000.0) * rate_per_minute\n    tokens = math.min(capacity, tokens + tokens_to_add)\n    \n    -- Check if tokens available\n    if tokens \u003c 1 then\n        -- Rate limit exceeded, update timestamp but don't consume\n        redis.call('HSET', key, 'tokens', tokens, 'last_update', now)\n        redis.call('EXPIRE', key, 300)  -- 5-minute TTL\n        return 0  -- Rejected\n    end\n    \n    -- Consume one token\n    tokens = tokens - 1\n    redis.call('HSET', key, 'tokens', tokens, 'last_update', now)\n    redis.call('EXPIRE', key, 300)  -- Reset TTL\n    \n    return 1  -- Allowed\nend\n\nredis.register_function('check_vote_rate_limit', check_rate_limit)\n```\n\n### Task 2: Load Lua script on startup\n**File:** `internal/redis/client.go`\n```go\n//go:embed vote_rate_limit.lua\nvar voteRateLimitScript string\n\nfunc loadVoteRateLimitFunction(ctx context.Context, client *redis.Client) error {\n    result := client.FunctionLoad(ctx, voteRateLimitScript)\n    if err := result.Err(); err != nil {\n        // Check if already loaded\n        if strings.Contains(err.Error(), \"Library already exists\") {\n            slog.Debug(\"vote_rate_limit library already loaded\")\n            return nil\n        }\n        return fmt.Errorf(\"failed to load vote_rate_limit library: %w\", err)\n    }\n    \n    slog.Info(\"Loaded vote_rate_limit Redis Function\")\n    return nil\n}\n\nfunc NewClient(ctx, redisURL, clock) (*redis.Client, error) {\n    // ... existing setup\n    \n    // Load both libraries\n    if err := loadChatPulseFunctions(ctx, client); err != nil {\n        return nil, err\n    }\n    \n    if err := loadVoteRateLimitFunction(ctx, client); err != nil {\n        return nil, err\n    }\n    \n    return client, nil\n}\n```\n\n### Task 3: Add rate limiter to Engine\n**File:** `internal/sentiment/engine.go`\n```go\nfunc (e *Engine) checkVoteRateLimit(ctx context.Context, sessionUUID uuid.UUID) (bool, error) {\n    key := fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)\n    \n    result := e.redis.FCall(ctx, \"check_vote_rate_limit\",\n        []string{key},\n        e.clock.Now().UnixMilli(),\n        100,  // capacity\n        100,  // rate per minute\n    )\n    \n    if err := result.Err(); err != nil {\n        return false, fmt.Errorf(\"rate limit check failed: %w\", err)\n    }\n    \n    allowed, _ := result.Int()\n    return allowed == 1, nil\n}\n\nfunc (e *Engine) ProcessVote(ctx, broadcasterUserID, chatterUserID, message) (float64, bool) {\n    // ... existing session + config lookup\n    \n    delta := matchTrigger(message, config)\n    if delta == 0 {\n        return 0, false\n    }\n    \n    // Check debounce (per-user)\n    allowed, err := e.debouncer.CheckDebounce(ctx, sessionUUID, chatterUserID)\n    if err != nil {\n        slog.Error(\"Debounce check failed\", \"error\", err)\n        return 0, false\n    }\n    if !allowed {\n        metrics.VoteProcessingTotal.WithLabelValues(\"debounced\").Inc()\n        return 0, false\n    }\n    \n    // NEW: Check rate limit (per-session)\n    allowed, err = e.checkVoteRateLimit(ctx, sessionUUID)\n    if err != nil {\n        slog.Error(\"Rate limit check failed\", \"error\", err)\n        // Fail open: allow vote if check fails (don't block legitimate votes)\n        allowed = true\n    }\n    if !allowed {\n        slog.Debug(\"Vote rate limited\", \"session_uuid\", sessionUUID)\n        metrics.VoteProcessingTotal.WithLabelValues(\"rate_limited\").Inc()\n        return 0, false\n    }\n    \n    // Apply vote\n    newValue, err := e.sentiment.ApplyVote(ctx, sessionUUID, delta, e.clock.Now())\n    if err != nil {\n        slog.Error(\"Failed to apply vote\", \"error\", err)\n        metrics.VoteProcessingTotal.WithLabelValues(\"error\").Inc()\n        return 0, false\n    }\n    \n    metrics.VoteProcessingTotal.WithLabelValues(\"applied\").Inc()\n    return newValue, true\n}\n```\n\n### Task 4: Add configuration\n**File:** `.env.example`\n```bash\n# Vote rate limiting (token bucket)\nVOTE_RATE_LIMIT_CAPACITY=100      # Burst allowance (tokens)\nVOTE_RATE_LIMIT_RATE=100          # Tokens per minute (sustained rate)\n```\n\n**File:** `internal/config/config.go`\n```go\ntype Config struct {\n    // ... existing fields\n    \n    VoteRateLimitCapacity int `env:\"VOTE_RATE_LIMIT_CAPACITY\" default:\"100\"`\n    VoteRateLimitRate     int `env:\"VOTE_RATE_LIMIT_RATE\" default:\"100\"`\n}\n```\n\nThread config through to Engine:\n```go\nfunc NewEngine(sessions, sentiment, debouncer, clock, cache, cfg) *Engine {\n    return \u0026Engine{\n        sessions:            sessions,\n        sentiment:           sentiment,\n        debouncer:           debouncer,\n        clock:               clock,\n        configCache:         cache,\n        rateLimitCapacity:   cfg.VoteRateLimitCapacity,\n        rateLimitRate:       cfg.VoteRateLimitRate,\n    }\n}\n```\n\n### Task 5: Add metrics\n**File:** `internal/metrics/metrics.go`\n```go\nVoteRateLimitChecks = promauto.NewCounterVec(\n    prometheus.CounterOpts{\n        Name: \"vote_rate_limit_checks_total\",\n        Help: \"Vote rate limit checks by result\",\n    },\n    []string{\"result\"},  // allowed, rejected, error\n)\n\nVoteRateLimitTokens = promauto.NewHistogram(\n    prometheus.HistogramOpts{\n        Name: \"vote_rate_limit_tokens\",\n        Help: \"Remaining tokens in vote rate limit bucket\",\n        Buckets: []float64{0, 10, 25, 50, 75, 90, 100},\n    },\n)\n```\n\nUpdate metrics in checkVoteRateLimit:\n```go\nif allowed {\n    metrics.VoteRateLimitChecks.WithLabelValues(\"allowed\").Inc()\n} else {\n    metrics.VoteRateLimitChecks.WithLabelValues(\"rejected\").Inc()\n}\n```\n\n### Task 6: Unit tests for Lua script\n**File:** `internal/redis/vote_rate_limit_test.go` (NEW - integration test)\nTest scenarios with testcontainers Redis:\n1. **Initial burst:** 100 votes succeed immediately\n2. **Burst exhaustion:** 101st vote rejected\n3. **Refill:** Wait 60s, tokens refilled to 100\n4. **Sustained rate:** 100 votes/minute allowed long-term\n5. **Partial refill:** Use 50 tokens, wait 30s, have 100 tokens\n6. **Key expiry:** Verify TTL set to 5 minutes\n\n```go\nfunc TestVoteRateLimit_InitialBurst(t *testing.T) {\n    if testing.Short() {\n        t.Skip(\"integration test\")\n    }\n    \n    client := setupTestClient(t)\n    sessionUUID := uuid.New()\n    \n    // Should allow 100 votes immediately (burst)\n    for i := 0; i \u003c 100; i++ {\n        result := client.FCall(context.Background(), \"check_vote_rate_limit\",\n            []string{fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)},\n            time.Now().UnixMilli(),\n            100,  // capacity\n            100,  // rate per minute\n        )\n        allowed, _ := result.Int()\n        assert.Equal(t, 1, allowed, \"Vote %d should be allowed\", i+1)\n    }\n    \n    // 101st vote should be rejected\n    result := client.FCall(context.Background(), \"check_vote_rate_limit\",\n        []string{fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)},\n        time.Now().UnixMilli(),\n        100, 100,\n    )\n    allowed, _ := result.Int()\n    assert.Equal(t, 0, allowed, \"Vote 101 should be rejected\")\n}\n\nfunc TestVoteRateLimit_Refill(t *testing.T) {\n    if testing.Short() {\n        t.Skip(\"integration test\")\n    }\n    \n    client := setupTestClient(t)\n    sessionUUID := uuid.New()\n    now := time.Now()\n    \n    // Exhaust bucket (100 votes)\n    for i := 0; i \u003c 100; i++ {\n        client.FCall(context.Background(), \"check_vote_rate_limit\",\n            []string{fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)},\n            now.UnixMilli(), 100, 100,\n        )\n    }\n    \n    // Advance time by 60 seconds (should refill 100 tokens)\n    now = now.Add(60 * time.Second)\n    \n    // Should allow 100 votes again\n    result := client.FCall(context.Background(), \"check_vote_rate_limit\",\n        []string{fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)},\n        now.UnixMilli(), 100, 100,\n    )\n    allowed, _ := result.Int()\n    assert.Equal(t, 1, allowed, \"Vote should be allowed after refill\")\n}\n```\n\n### Task 7: Unit tests for Engine integration\n**File:** `internal/sentiment/engine_rate_limit_test.go` (NEW)\nTest ProcessVote with mocked Redis client:\n1. **Below limit:** Votes processed normally\n2. **At limit:** Vote rejected with rate_limited result\n3. **Redis error:** Vote allowed (fail open for availability)\n\n```go\ntype mockRedisClient struct {\n    rateLimitResult int  // 0=reject, 1=allow\n    rateLimitError  error\n}\n\nfunc (m *mockRedisClient) FCall(ctx, fn, keys, args) *redis.Cmd {\n    if fn == \"check_vote_rate_limit\" {\n        if m.rateLimitError != nil {\n            return redis.NewCmdResult(nil, m.rateLimitError)\n        }\n        return redis.NewCmdResult(m.rateLimitResult, nil)\n    }\n    return redis.NewCmdResult(nil, nil)\n}\n\nfunc TestProcessVote_RateLimited(t *testing.T) {\n    redisClient := \u0026mockRedisClient{rateLimitResult: 0}  // Reject\n    engine := NewEngine(mockSessions, mockSentiment, mockDebouncer, clock, cache, redisClient, cfg)\n    \n    value, applied := engine.ProcessVote(ctx, broadcasterID, chatterID, \"trigger word\")\n    \n    assert.False(t, applied)\n    assert.Equal(t, 0.0, value)\n    // Verify metrics incremented\n}\n```\n\n### Task 8: Load testing for rate limiter\n**File:** `scripts/load-test-vote-rate-limit.go` (NEW)\nSimulate bot attack:\n```go\npackage main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"sync\"\n    \"time\"\n    \n    \"github.com/google/uuid\"\n    \"github.com/redis/go-redis/v9\"\n)\n\nfunc main() {\n    client := redis.NewClient(\u0026redis.Options{Addr: \"localhost:6379\"})\n    sessionUUID := uuid.New()\n    \n    ctx := context.Background()\n    successCount := 0\n    rejectedCount := 0\n    var mu sync.Mutex\n    \n    // Simulate 1000 bots voting within 1 second\n    var wg sync.WaitGroup\n    for i := 0; i \u003c 1000; i++ {\n        wg.Add(1)\n        go func(botNum int) {\n            defer wg.Done()\n            \n            result := client.FCall(ctx, \"check_vote_rate_limit\",\n                []string{fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)},\n                time.Now().UnixMilli(), 100, 100,\n            )\n            \n            allowed, _ := result.Int()\n            mu.Lock()\n            if allowed == 1 {\n                successCount++\n            } else {\n                rejectedCount++\n            }\n            mu.Unlock()\n        }(i)\n    }\n    \n    wg.Wait()\n    \n    fmt.Printf(\"Results:\\n\")\n    fmt.Printf(\"  Allowed: %d (expected ~100)\\n\", successCount)\n    fmt.Printf(\"  Rejected: %d (expected ~900)\\n\", rejectedCount)\n}\n```\n\n### Task 9: Documentation\n**File:** `docs/operations/vote-rate-limiting.md` (NEW)\nDocument:\n- Token bucket algorithm explained\n- Why 100 capacity / 100 per minute chosen\n- How to tune parameters for different use cases\n- Monitoring via metrics\n- Testing rate limiter behavior\n- Troubleshooting guide\n\nExample scenarios:\n```\nScenario: Normal chat (50 viewers, 10% engagement)\n- 5 votes/minute average\n- Well under limit, no impact\n\nScenario: Hype moment (1000 viewers, 50% engagement)\n- 500 potential votes in 5 seconds\n- Rate limiter allows 100 immediately (burst)\n- Then 100/minute sustained\n- 400 votes rejected (prevents manipulation)\n\nScenario: Bot attack (1000 bots coordinated)\n- 1000 vote attempts in 1 second\n- Rate limiter allows 100 (burst)\n- 900 rejected\n- Attack mitigated\n```\n\n### Task 10: Update CLAUDE.md\nAdd vote rate limiting to ## Vote Processing Pipeline section\n\n## Acceptance Criteria\n\n- ✅ Token bucket rate limiter implemented in Redis Lua\n- ✅ Allows 100-vote burst, then 100 votes/minute sustained\n- ✅ Bot attack (1000 bots) limited to 100 votes\n- ✅ Legitimate bursts (hype moments) allowed\n- ✅ Rate limiter fails open (allows votes on Redis error)\n- ✅ Metrics track allowed vs rejected votes\n- ✅ Integration tests verify burst + refill behavior\n- ✅ Load test simulates bot attack\n- ✅ Documentation explains tuning\n- ✅ Zero false negatives (legitimate votes never blocked incorrectly)\n\n## Files Created/Modified\n\n**New files:**\n- `internal/redis/vote_rate_limit.lua` (60 lines Lua)\n- `internal/redis/vote_rate_limit_test.go` (300 lines integration tests)\n- `internal/sentiment/engine_rate_limit_test.go` (150 lines unit tests)\n- `scripts/load-test-vote-rate-limit.go` (80 lines load test)\n- `docs/operations/vote-rate-limiting.md` (400 lines)\n\n**Modified files:**\n- `internal/redis/client.go` (load Lua script, 15 lines)\n- `internal/sentiment/engine.go` (add checkVoteRateLimit, 30 lines)\n- `internal/config/config.go` (add 2 config fields)\n- `.env.example` (document rate limit variables)\n- `internal/metrics/metrics.go` (add rate limit metrics)\n- `CLAUDE.md` (document vote rate limiting)\n\n## Testing Strategy\n\n**Unit tests:**\n- Mock Redis client returns rate limit results\n- Test ProcessVote integration\n- Test fail-open behavior (Redis error)\n\n**Integration tests:**\n- Real Redis (testcontainers)\n- Test burst allowance (100 votes immediately)\n- Test sustained rate (100/minute long-term)\n- Test refill behavior (advance time)\n- Test TTL expiry\n\n**Load tests:**\n- Simulate 1000 concurrent bots\n- Verify ~100 allowed, ~900 rejected\n- Measure Redis performance (should handle easily)\n\n**Manual tests:**\n- Deploy to staging\n- Use webhook simulator to send 1000 votes\n- Verify rate limiting via metrics\n- Verify overlay not manipulated\n\n## Dependencies\n- **Blocks:** twitch-tow-dmg (connection limits should be first)\n- **Requires:** Redis Functions support (Redis 7.0+)\n\n## Success Metrics\n- Bot attacks limited to 100 votes/minute\n- Legitimate bursts allowed (99% of real votes pass)\n- Zero false positives (legitimate votes blocked)\n- \u003c1ms overhead per vote check\n\n## Effort Estimate\n**5 developer-days** (1 week)\n\nBreakdown:\n- Lua script: 1 day\n- Engine integration: 1 day\n- Unit tests: 1 day\n- Integration + load tests: 1 day\n- Documentation: 1 day\n\n## Risk Mitigation\n- **Risk:** Rate limiter too strict, blocks legitimate votes\n  - **Mitigation:** 100 burst + 100/min is generous for real usage\n  - **Mitigation:** Monitor rejected vote metrics, adjust if needed\n- **Risk:** Token bucket parameters wrong for different stream sizes\n  - **Mitigation:** Configurable via env vars\n  - **Mitigation:** Document how to calculate appropriate values\n- **Risk:** Redis failures block all votes\n  - **Mitigation:** Fail open (allow votes on error)\n  - **Mitigation:** Circuit breaker handles Redis outages gracefully","status":"open","priority":2,"issue_type":"epic","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:30:42.112703+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:46.997479+01:00"}
{"id":"twitch-tow-tt0","title":"SOLUTION: Multi-layer resource limits and rate limiting","description":"SOLUTION: Comprehensive Resource Limits and Rate Limiting\n\nThis solution addresses consensus beads:\n- twitch-tow-08s (P2 - No WS connection limits, Scalability)\n- twitch-tow-4v4 (P2 - No webhook rate limiting, Scalability)\n- twitch-tow-aa4 (P2 - Rate limiting and abuse prevention, Resilience)\n- twitch-tow-597 (P2 - WS connection lifecycle, Resilience)\n\n## Problem Summary\n\n**Current vulnerabilities:**\n- No global WebSocket connection limit (DoS risk)\n- No per-broadcaster rate limiting on votes (bot attack vector)\n- No per-IP connection limit (single source can exhaust resources)\n- Per-user debounce (1s) insufficient for coordinated attacks\n\n**Attack scenarios:**\n1. Connection exhaustion: 10K connections from botnet\n2. Vote flooding: 1000 unique bots spam trigger words\n3. Session enumeration: Attacker tries random UUIDs\n\n---\n\n## Multi-Layer Defense Strategy\n\n### Layer 1: Global Connection Limits (Week 1)\nPriority: HIGH - prevents resource exhaustion\n\n**Goal:** Cap total concurrent WebSocket connections per instance\n\n**Implementation:**\n```go\ntype ConnectionLimiter struct {\n    current atomic.Int64\n    max     int64\n}\n\nfunc (l *ConnectionLimiter) Acquire() bool {\n    for {\n        current := l.current.Load()\n        if current \u003e= l.max {\n            return false\n        }\n        if l.current.CompareAndSwap(current, current+1) {\n            return true\n        }\n    }\n}\n\nfunc (l *ConnectionLimiter) Release() {\n    l.current.Add(-1)\n}\n```\n\n**In WebSocket handler:**\n```go\nfunc (s *Server) handleWebSocket(c echo.Context) error {\n    if !s.connLimiter.Acquire() {\n        return c.String(503, \"Server at capacity, please retry\")\n    }\n    defer s.connLimiter.Release()\n    \n    // ... existing WebSocket upgrade logic\n}\n```\n\n**Recommended limits:**\n- Default: 10,000 connections per instance\n- Configurable via env: MAX_WEBSOCKET_CONNECTIONS\n- Log warning at 80% capacity\n- Monitor via metric: websocket_connections_current\n\n**File descriptor check on startup:**\n```go\nfunc checkUlimit() {\n    var rlimit syscall.Rlimit\n    syscall.Getrlimit(syscall.RLIMIT_NOFILE, \u0026rlimit)\n    \n    if rlimit.Cur \u003c 20000 {\n        slog.Warn(\"ulimit may be too low\", \n            \"current\", rlimit.Cur, \n            \"recommended\", 20000)\n    }\n}\n```\n\n**Trade-offs:**\n+ Simple, effective DoS protection\n+ Low overhead (atomic counter)\n+ Configurable per deployment\n- Some legitimate users may be rejected under load\n- No prioritization (first-come-first-served)\n\n---\n\n### Layer 2: Per-IP Connection Limits (Week 1)\nPriority: HIGH - prevents single-source attacks\n\n**Goal:** Limit concurrent connections from single IP address\n\n**Implementation:**\n```go\ntype IPLimiter struct {\n    mu      sync.RWMutex\n    ips     map[string]int\n    maxPer  int\n}\n\nfunc (l *IPLimiter) Acquire(ip string) bool {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    if l.ips[ip] \u003e= l.maxPer {\n        return false\n    }\n    l.ips[ip]++\n    return true\n}\n\nfunc (l *IPLimiter) Release(ip string) {\n    l.mu.Lock()\n    defer l.mu.Unlock()\n    \n    if count := l.ips[ip]; count \u003e 0 {\n        l.ips[ip] = count - 1\n        if l.ips[ip] == 0 {\n            delete(l.ips, ip)\n        }\n    }\n}\n```\n\n**In WebSocket handler:**\n```go\nip := c.RealIP()\nif !s.ipLimiter.Acquire(ip) {\n    return c.String(429, \"Too many connections from your IP\")\n}\ndefer s.ipLimiter.Release(ip)\n```\n\n**Recommended limits:**\n- Default: 100 connections per IP\n- Configurable: MAX_CONNECTIONS_PER_IP\n\n**Considerations:**\n- NAT/proxy: Many users behind same IP\n- Adjust limit higher (100-500) for production\n- Monitor per-IP distribution\n\n**Trade-offs:**\n+ Prevents single-source exhaustion\n+ Protects against simple DDoS\n- Shared IPs (NAT) may hit limit legitimately\n- Memory overhead: ~40 bytes per unique IP\n\n---\n\n### Layer 3: Token Bucket Rate Limiter for Votes (Week 2)\nPriority: MEDIUM - prevents bot vote spam\n\n**Goal:** Limit vote rate per broadcaster session\n\n**Implementation using Redis:**\n```go\n// Token bucket: 100 tokens, refills at 100/minute\nfunc (e *Engine) checkVoteRateLimit(ctx, sessionUUID) (bool, error) {\n    key := fmt.Sprintf(\"rate_limit:votes:%s\", sessionUUID)\n    \n    script := `\n    local key = KEYS[1]\n    local now = tonumber(ARGV[1])\n    local capacity = tonumber(ARGV[2])\n    local rate = tonumber(ARGV[3])\n    \n    local tokens = redis.call('HGET', key, 'tokens')\n    local last_update = redis.call('HGET', key, 'last_update')\n    \n    if not tokens then\n        tokens = capacity\n        last_update = now\n    else\n        tokens = tonumber(tokens)\n        last_update = tonumber(last_update)\n    end\n    \n    local elapsed = math.max(0, now - last_update)\n    tokens = math.min(capacity, tokens + elapsed * rate / 60000.0)\n    \n    if tokens \u003c 1 then\n        return 0\n    end\n    \n    tokens = tokens - 1\n    redis.call('HSET', key, 'tokens', tokens, 'last_update', now)\n    redis.call('EXPIRE', key, 300)\n    return 1\n    `\n    \n    result, err := rdb.Eval(ctx, script, []string{key}, \n        time.Now().UnixMilli(), 100, 100).Int()\n    return result == 1, err\n}\n```\n\n**In ProcessVote pipeline:**\n```go\n// After debounce check, before ApplyVote\nallowed, err := e.checkVoteRateLimit(ctx, sessionUUID)\nif !allowed {\n    slog.Warn(\"Vote rate limit exceeded\", \"session_uuid\", sessionUUID)\n    return 0, false\n}\n```\n\n**Parameters:**\n- Capacity: 100 tokens (burst allowance)\n- Refill rate: 100 tokens/minute (sustained rate)\n- Allows: 100 votes immediately, then 100/min sustained\n\n**Why token bucket:**\n- Allows natural bursts (chat excitement)\n- Prevents sustained bot spam\n- Fair: refills gradually\n\n**Alternative: Fixed window**\n- Simpler but allows burst at boundary\n- Token bucket is better for UX\n\n**Trade-offs:**\n+ Protects against bot spam\n+ Allows legitimate bursts\n+ Per-session, doesn't affect other channels\n- Legitimate high-engagement moments might hit limit\n- Adds Redis call to hot path (mitigated: only if vote matches trigger)\n\n---\n\n### Layer 4: Connection Rate Limiting (Week 2)\nPriority: LOW - defense in depth\n\n**Goal:** Limit new connection rate per IP\n\n**Implementation:**\n```go\ntype ConnectionRateLimiter struct {\n    limiter *rate.Limiter\n}\n\nfunc (l *ConnectionRateLimiter) Allow(ip string) bool {\n    // 10 connections per second per IP\n    return l.limiter.Allow()\n}\n```\n\n**Using golang.org/x/time/rate:**\n```go\nlimiter := rate.NewLimiter(10, 20)  // 10/sec, burst 20\n```\n\n**In middleware:**\n```go\nif !s.connRateLimiter.Allow(c.RealIP()) {\n    return c.String(429, \"Connection rate limit exceeded\")\n}\n```\n\n**Trade-offs:**\n+ Prevents rapid connection spam\n+ Built-in Go standard library\n- Shared IPs may hit limit\n- Less critical than other layers\n\n---\n\n## Configuration Summary\n\n**Environment variables:**\n```bash\nMAX_WEBSOCKET_CONNECTIONS=10000    # Global limit per instance\nMAX_CONNECTIONS_PER_IP=100         # Per-IP concurrent limit\nVOTE_RATE_LIMIT_CAPACITY=100       # Token bucket capacity\nVOTE_RATE_LIMIT_RATE=100           # Tokens per minute\nCONNECTION_RATE_PER_IP=10          # New connections per second per IP\n```\n\n**Metrics to add:**\n```\nwebsocket_connections_rejected_total{reason}  // global_limit, ip_limit, rate_limit\nvote_rate_limit_exceeded_total{session_uuid}\nconnection_limiter_capacity_pct\n```\n\n**Alerts:**\n```\n- Alert: WebSocket capacity over 80%\n- Alert: Vote rate limit exceeded (sustained)\n- Alert: High connection rejection rate\n```\n\n---\n\n## Implementation Timeline\n\n**Week 1:**\n- Global connection limit (atomic counter)\n- Per-IP connection limit (map + mutex)\n- File descriptor check on startup\n- Metrics for connection tracking\n\n**Week 2:**\n- Token bucket rate limiter (Redis Lua script)\n- Connection rate limiting (golang.org/x/time/rate)\n- Integration tests for limits\n- Documentation for tuning parameters\n\n**Total:** 2 weeks\n\n---\n\n## Success Criteria\n\n**Global Limits:**\n- ✅ Instance cannot exceed MAX_WEBSOCKET_CONNECTIONS\n- ✅ Graceful rejection with 503 response\n- ✅ Metrics show current connection count\n\n**Per-IP Limits:**\n- ✅ Single IP cannot exceed MAX_CONNECTIONS_PER_IP\n- ✅ NAT scenarios tested (higher limit acceptable)\n- ✅ 429 response with clear error message\n\n**Vote Rate Limiting:**\n- ✅ Bot spam scenarios blocked (1000 bots → rate limited)\n- ✅ Legitimate high-engagement allowed (burst to 100)\n- ✅ Sustained rate limited to configured value\n\n---\n\n## Testing Strategy\n\n**Load testing scenarios:**\n1. 10K concurrent connections (should succeed)\n2. 11K concurrent connections (1K rejected)\n3. 200 connections from single IP (100 succeed, 100 rejected)\n4. 1000 votes in 10 seconds (100 applied, 900 rate limited)\n\n**Tools:**\n- k6 or Apache Bench for load testing\n- WebSocket load testing: ws-harness or custom script\n\n---\n\n## Trade-offs Analysis\n\n**Pros:**\n+ Multi-layer defense (defense in depth)\n+ Configurable limits per deployment\n+ Low overhead (atomic ops, efficient algorithms)\n+ Prevents common DoS scenarios\n\n**Cons:**\n- May reject legitimate traffic under extreme load\n- Shared IPs (NAT) require higher per-IP limits\n- Token bucket adds Redis calls (mitigated: only for votes)\n\n**Verdict:** Essential for production. Benefits far outweigh costs.\n\n---\n\n## Open Questions for Team\n\n1. What should MAX_WEBSOCKET_CONNECTIONS be? (10K default reasonable?)\n2. Token bucket parameters: 100 capacity / 100 per minute acceptable?\n3. Should we implement all 4 layers or just global + IP limits first?\n4. Do we need admin API to adjust limits at runtime?\n\nVote: Use bd update to add your +1 or concerns\n","notes":"Vote: +1 from Maintainability architect. Most detailed resource limits proposal with excellent implementation code. 4-layer defense with token bucket in Redis (correct for multi-instance). Testing strategy included. File descriptor check on startup is good operational practice. Parameters are reasonable (10K global, 100 per IP, token bucket 100/min). Supersedes my twitch-tow-mzk which is 99% aligned but less detailed. Adopt tt0 as consensus solution.\nVote: +1 from Maintainability Architect\n\nRATIONALE: 4-layer defense comprehensive. Tunable config correct. 2 dev-weeks fast ROI. Essential DoS protection.\n\nINTEGRATION:\n- ADR-011 update (document 10K connections → ~500 sessions capacity planning)\n- ERROR_HANDLING.md update (rate limit = Tier B3, connection limit = HTTP 429)\n\nANSWERS:\n- Q1 (limits reasonable): YES with caveats\n  - Global 10K: ✅ (1MB/conn × 10K = 10GB, leaves 6GB)\n  - Per-IP 100: ✅ (but whitelist for enterprise NAT/proxy)\n  - Vote rate 100/min: ✅ (add burst=200 for viral streams)\n  - Connection rate 10/sec: ✅ (whitelist health checks)\n- Q2 (all 4 layers or subset): Phased - Week 1: global+IP, Week 2: rate limits\n- Q3 (admin API): YES but Phase 2 (defer until metrics inform tuning)\n\nRECOMMENDATIONS:\n- Connection limit = HTTP 429 (not 503)\n- Vote rate limit = silent drop + metric (not error)\n- Add metrics: connection_limit_rejections_total, ip_limit_rejections_total, votes_rate_limited_total\n- Document error handling patterns in ERROR_HANDLING.md\n- Admin API design: POST /api/admin/limits (auth required, audit log)\n\nEFFORT: 2 dev-weeks + 2 days docs","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:14:14.381359+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:56:56.833668+01:00","closed_at":"2026-02-12T17:56:56.833668+01:00","close_reason":"Phase 2 solution proposals - superseded by final implementation epics","labels":["consensus","dos-protection","phase-2-solution","rate-limiting"]}
{"id":"twitch-tow-u90","title":"EPIC: Write Tier 2 Scaling ADRs (4 ADRs)","description":"Write 4 ADRs that document scaling-related architectural decisions: single bot account architecture, stateless instances, ref counting for coordination, and database vs Redis data separation. These build on Tier 1 foundation.\n\n## User Story\nAs an operator deploying to production, I need to understand how the multi-instance coordination works so I can troubleshoot scaling issues and configure infrastructure correctly.\n\n## Value Proposition\n- Documents multi-tenant scaling model\n- Explains ref counting coordination mechanism\n- Clarifies when to use PostgreSQL vs Redis\n- Prevents common scaling anti-patterns\n\n## Tasks\n\n### 1. Write ADR-004: Single bot account reads all channels\n\n**Context:**\n- Each streamer needs chat message access for sentiment analysis\n- Twitch EventSub subscriptions require user authorization\n- Traditional approach: each streamer authorizes full app access\n- New bot account model: separate bot account with special scopes\n\n**Decision:**\n- Use **single dedicated bot account** to read chat on behalf of all streamers\n- Bot authorizes with `user:read:chat` + `user:bot` scopes (one-time setup)\n- Streamers only grant `channel:bot` scope during OAuth (allows bot into their channel)\n- EventSub subscriptions use `user_id` = bot, `broadcaster_user_id` = streamer\n- Bot account's Twitch user ID configured via `BOT_USER_ID` env var\n\n**Alternatives considered:**\n1. **Per-streamer OAuth** - Each streamer authorizes full chat access\n   - Rejected: More complex OAuth flow (more scopes to approve)\n   - Rejected: Token management per streamer (refresh, expiry)\n   - Rejected: EventSub subscription per streamer complexity\n\n2. **Multiple bot accounts** - Separate bot per streamer or shard\n   - Rejected: Token management overhead (N bots)\n   - Rejected: Rate limits distributed (harder to track)\n   - Rejected: Doesn't simplify anything\n\n3. **IRC fallback** - Use legacy Twitch IRC instead of EventSub\n   - Rejected: Deprecated transport, EventSub preferred\n   - Rejected: Doesn't support bot account model\n\n**Consequences:**\n\n✅ **Positive:**\n- Streamers only grant `channel:bot` (minimal scope, clear purpose)\n- Single token to manage and refresh (bot's token)\n- Simpler EventSub subscription logic (one user_id for all)\n- Clear separation: bot account = read chat, streamer account = config\n\n❌ **Negative:**\n- Single rate limit pool for all streamers (5000 requests/min shared)\n- Bot account is single point of failure (if token expires, all chats go dark)\n- Requires bot account setup (manual one-time OAuth)\n- Bot must be granted access to every streamer's channel\n\n🔄 **Trade-offs:**\n- Chose simplicity over rate limit isolation\n- Accept single point of failure for simpler auth flow\n- Prioritize minimal streamer permissions over distributed rate limits\n\n**Related decisions:**\n- ADR-003: EventSub webhooks (consequence: bot receives all chat messages)\n\n**Files to create:**\n- `docs/adr/004-single-bot-account.md`\n\n**Time estimate:** 60 minutes\n\n### 2. Write ADR-005: No sticky sessions - stateless instances\n\n**Context:**\n- Load balancing across multiple instances\n- Traditional approach: sticky sessions route user to same instance\n- Redis-only architecture enables alternative model\n\n**Decision:**\n- **Any instance can serve any session** - no session affinity\n- Load balancer uses round-robin or least-connections (no stickiness)\n- Session state pulled from Redis on every WebSocket connection\n- Ref counting tracks how many instances serve each session\n\n**Alternatives considered:**\n1. **Sticky sessions via load balancer** - IP hash or cookie-based routing\n   - Rejected: Doesn't solve cleanup when instance crashes (orphaned sessions)\n   - Rejected: Uneven load distribution (popular streamers pin to one instance)\n   - Rejected: Complex failover (need to re-route all sticky sessions)\n   - Rejected: Contradicts Redis-only architecture goal\n\n2. **Session affinity with consistent hashing** - Route by session UUID hash\n   - Rejected: Same problems as sticky sessions\n   - Rejected: Instance removal requires re-hashing (disrupts sessions)\n\n3. **Dedicated instance per streamer** - Manual assignment in config\n   - Rejected: Manual ops burden\n   - Rejected: Doesn't scale (fixed capacity per streamer)\n\n**Consequences:**\n\n✅ **Positive:**\n- Simple load balancing (any algorithm works)\n- Even load distribution (no hot instances)\n- Easy failover (no state to migrate)\n- Instance crashes don't lose sessions (state in Redis)\n\n❌ **Negative:**\n- Requires ref counting for cleanup coordination\n- Eventual consistency for ref count (30s cleanup window)\n- Cold start latency (fetch config from DB on first connect)\n\n🔄 **Trade-offs:**\n- Chose operational simplicity over optimal latency\n- Accept 30s cleanup delay for ref count races\n- Prioritize horizontal scaling over session affinity optimization\n\n**Related decisions:**\n- ADR-001: Redis-only architecture (enables stateless instances)\n- ADR-006: Ref counting strategy (consequence: cleanup coordination)\n\n**Files to create:**\n- `docs/adr/005-stateless-instances.md`\n\n**Time estimate:** 60 minutes\n\n### 3. Write ADR-006: Ref counting for multi-instance coordination\n\n**Context:**\n- Multiple instances may serve same session simultaneously (no sticky sessions)\n- Need to coordinate session cleanup (remove from Redis when no clients)\n- Distributed system, failures inevitable\n\n**Decision:**\n- Use **Redis INCR/DECR** for ref counting (key: `ref_count:{overlayUUID}`)\n- Increment when instance starts serving session (`IncrRefCount`)\n- Decrement when last client disconnects (`OnSessionEmpty`)\n- Cleanup timer scans sessions with `ref_count = 0` + `last_disconnect \u003e 30s ago`\n- **Accept eventual consistency** (race conditions tolerated)\n\n**Race conditions accepted:**\n1. Two instances increment simultaneously → ref count = 2 (correct)\n2. Decrement + delete race → orphaned ref count key (cleaned by 30s timer)\n3. Increment during cleanup scan → session recreated immediately (correct)\n\n**Alternatives considered:**\n1. **Distributed locks (Redlock)** - Acquire lock before cleanup\n   - Rejected: Complex protocol (5 Redis nodes, clock skew handling)\n   - Rejected: Overkill for best-effort cleanup\n   - Rejected: Performance overhead (lock acquisition latency)\n\n2. **Leader election (Raft/etcd)** - Single leader does all cleanup\n   - Rejected: Adds external dependency (etcd)\n   - Rejected: Leader becomes bottleneck for cleanup\n   - Rejected: Complexity not justified for 30s grace period\n\n3. **Sticky sessions** - Only one instance per session, no coordination\n   - Rejected: See ADR-005 (contradicts scaling model)\n\n**Consequences:**\n\n✅ **Positive:**\n- Simple implementation (INCR/DECR atomic operations)\n- Fast operations (\u003c1ms latency)\n- No external dependencies (just Redis)\n- Self-healing (30s timer cleans up race condition artifacts)\n\n❌ **Negative:**\n- 30s cleanup delay (sessions linger in Redis)\n- Race conditions possible (two instances cleanup simultaneously)\n- Orphaned keys from decrement-delete races (cleaned eventually)\n\n🔄 **Trade-offs:**\n- Chose simplicity + performance over strong consistency\n- Accept 30s window for eventual cleanup\n- Prioritize availability (no locks) over perfect cleanup\n\n**Related decisions:**\n- ADR-005: Stateless instances (consequence: need ref counting)\n- ADR-010: Eventual consistency over strong consistency (philosophy)\n\n**Files to create:**\n- `docs/adr/006-ref-counting-coordination.md`\n\n**Time estimate:** 60 minutes\n\n### 4. Write ADR-007: Database vs Redis data separation\n\n**Context:**\n- Two storage systems: PostgreSQL (durable) and Redis (fast but ephemeral)\n- Need to decide what data lives where\n- Cold start scenario: instance restarts, Redis empty\n\n**Decision:**\n- **PostgreSQL = source of truth** for durable data:\n  - Users (Twitch OAuth data, tokens encrypted at rest)\n  - Config (sentiment settings per user)\n  - EventSub subscriptions (for idempotency + crash recovery)\n\n- **Redis = ephemeral cache** for hot session state:\n  - Sessions (current value, config snapshot, broadcaster mapping)\n  - Sentiment (decaying value, last_update timestamp)\n  - Ref counts (instance coordination)\n  - Debounce (1s TTL, auto-expires)\n\n- **Cold start flow:**\n  1. WebSocket connects → lookup user by overlay UUID (PostgreSQL)\n  2. Fetch config from PostgreSQL\n  3. Activate session in Redis (`CreateSession` writes hash)\n  4. Subscribe to Twitch EventSub\n  5. Now session is hot (all reads from Redis)\n\n**Alternatives considered:**\n1. **All data in PostgreSQL** - No Redis, read DB on every tick\n   - Rejected: Too slow (PostgreSQL query latency 5-20ms, need \u003c2ms)\n   - Rejected: Doesn't support 50ms tick loop (N sessions × 50Hz = high load)\n\n2. **All data in Redis** - No PostgreSQL, Redis is source of truth\n   - Rejected: Lose data on Redis failure (no durable storage)\n   - Rejected: Harder to query (no SQL joins)\n   - Rejected: Backups more complex (RDB/AOF vs pg_dump)\n\n3. **Dual-write to both** - Update PostgreSQL + Redis on config save\n   - Rejected: Consistency issues (which wins on mismatch?)\n   - Rejected: More complex (need to sync on every write)\n   - Rejected: PostgreSQL is already source of truth (fetch on activation)\n\n**Consequences:**\n\n✅ **Positive:**\n- Clear separation: PostgreSQL = durable, Redis = fast\n- PostgreSQL is source of truth (config changes persist across restarts)\n- Redis optimized for hot path (50ms tick loop)\n- Lose Redis data? Just reactivate sessions from PostgreSQL (no data loss)\n\n❌ **Negative:**\n- Cold start latency (fetch config from PostgreSQL on first connect)\n- Two storage systems to monitor and backup\n- Config changes require Redis update (stale until next activation)\n\n🔄 **Trade-offs:**\n- Chose performance (Redis hot path) over simplicity (single DB)\n- Accept cold start penalty for optimized steady-state\n- Prioritize durability (PostgreSQL) over real-time config updates (eventual)\n\n**Related decisions:**\n- ADR-001: Redis-only architecture (hot session state)\n- ADR-009: Token encryption at rest (PostgreSQL storage)\n\n**Files to create:**\n- `docs/adr/007-database-redis-separation.md`\n\n**Time estimate:** 60 minutes\n\n### 5. Update ADR index page\n\nAdd Tier 2 ADRs to `docs/adr/README.md` table.\n\n**Time estimate:** 10 minutes\n\n### 6. Review and polish\n\n- Read all 4 ADRs for consistency\n- Cross-reference related decisions\n- Verify trade-offs captured clearly\n\n**Time estimate:** 30 minutes\n\n## Acceptance Criteria\n\n- ✅ All 4 ADRs written\n- ✅ Alternatives explain why rejected\n- ✅ Consequences show trade-offs (not just pros/cons)\n- ✅ Related decisions cross-referenced\n- ✅ Index page updated\n\n## Files Changed\n\n**Created:**\n- `docs/adr/004-single-bot-account.md`\n- `docs/adr/005-stateless-instances.md`\n- `docs/adr/006-ref-counting-coordination.md`\n- `docs/adr/007-database-redis-separation.md`\n\n**Modified:**\n- `docs/adr/README.md` (add Tier 2 links)\n\n## Dependencies\n- Epic 1 complete (Tier 1 ADRs provide foundation)\n\n## Effort Estimate\n**Total: 4 hours** (240 minutes)\n- ADR-004 (60 min): Single bot account\n- ADR-005 (60 min): Stateless instances\n- ADR-006 (60 min): Ref counting\n- ADR-007 (60 min): Database vs Redis\n- Polish (30 min): Review + index update","status":"closed","priority":1,"issue_type":"epic","assignee":"dev-quality","owner":"patrick.scheid@deepl.com","estimated_minutes":240,"created_at":"2026-02-12T17:26:45.267417+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T18:10:33.452072+01:00","closed_at":"2026-02-12T18:10:33.452072+01:00","close_reason":"Implemented all Tier 2 Scaling ADRs. Created ADR-004 (Single bot account), ADR-005 (Stateless instances), ADR-006 (Ref counting coordination), and ADR-007 (Database vs Redis separation). All ADRs follow consistent template with Context/Decision/Alternatives/Consequences sections. Updated README.md index with Tier 2 table. All acceptance criteria met. Committed in 2574710."}
{"id":"twitch-tow-uf7","title":"Discussion: Error handling consistency and sentinel error usage","description":"## Issue\nError handling is inconsistent across layers. Some errors are logged and swallowed, others propagate but lose context. Sentinel errors from domain package are not always checked correctly.\n\n## Current State\n- Domain sentinel errors (ErrUserNotFound, ErrConfigNotFound, ErrSubscriptionNotFound)\n- HTTP handlers check sentinel errors with errors.Is (good)\n- Engine.ProcessVote logs errors but returns (0, false) - no error propagation\n- Webhook handler silently drops votes on error (no alert mechanism)\n- SaveConfig best-effort updates live session, logs but ignores error (app/service.go:167-169)\n\n## Inconsistencies\n1. **ProcessVote error handling**: All errors return same (0, false) - can't distinguish failure types\n2. **Silent failures in webhook**: Votes dropped with only a log entry (no alert, no retry)\n3. **Best-effort patterns**: SaveConfig UpdateConfig failure is silent (line 167-169)\n4. **OnSessionEmpty errors**: Logged but no compensation logic (app/service.go:128-138)\n5. **Broadcaster tick errors**: Log and continue, but no metrics on error rate\n\n## Risks\n- **Silent data loss**: Dropped votes with no visibility to streamers\n- **Debugging difficulty**: Insufficient context in logs to diagnose root cause\n- **Operational gaps**: No alerts for high error rates or specific failure modes\n- **Compensation logic**: Failed operations don't trigger cleanup/retry\n\n## Suggestions\n1. Add error classification (transient vs. permanent, retriable vs. not)\n2. Track error metrics by type and operation (e.g., vote_processing_errors_total)\n3. Add retry queue for webhook vote processing failures (at-least-once delivery)\n4. Bubble up error types from ProcessVote (don't flatten to bool)\n5. Add structured error logging with stack traces (use errors.WithStack)\n6. Define error budget thresholds (alert if error rate \u003e 1% over 5 min)\n7. Add circuit breaker that disables session on consecutive errors (auto-recovery)\n8. Document error handling patterns and escalation policy\n\n## Files\n- internal/sentiment/engine.go:39-68 (ProcessVote flattens errors)\n- internal/twitch/webhook.go:68-72 (votes dropped silently)\n- internal/app/service.go:166-169 (UpdateConfig error ignored)\n- internal/app/service.go:126-138 (OnSessionEmpty error handling)\n- internal/broadcast/broadcaster.go:227-233 (tick error handling)","notes":"RESOLVED: Already addressed by Epic 4 (twitch-tow-apx) - Error Handling Standardization. Covers: 3-tier error classification (Domain/Infrastructure/Programming), logging rules (decision boundary vs origin), structured logging standards, double-logging fixes. All concerns from this bead are addressed.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:05:33.095687+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:48:20.938508+01:00","closed_at":"2026-02-12T17:48:20.938512+01:00"}
{"id":"twitch-tow-usj","title":"Discussion: Single Redis instance is a SPOF and bottleneck","description":"Single Redis instance creates availability and scalability concerns.\n\nCurrent: Single Redis, no replication, no failover\nThroughput: 100K-200K ops/sec limit\nAt 10 instances x 1K sessions: 200K calls/sec (at limit)\n\nAvailability risks:\n- Redis crash = total outage\n- Network partition = immediate degradation\n- No failover mechanism\n\nScalability limits:\n- Vertical scaling only\n- No read scaling\n- Single write point\n\nSolutions:\nA. Redis Sentinel (HA via master-replica + auto-failover)\nB. Redis Cluster (horizontal write scaling)\nC. Read replicas (scale reads 10x+)\nD. Hybrid Sentinel + replicas (recommended)\n\nRecommendation:\nPhase 1: Document SPOF, acceptable for MVP\nPhase 2: Deploy Sentinel + read replicas\n- Route broadcaster reads to replicas\n- Accept 50-200ms staleness (fine for sentiment)\nPhase 3: Consider Cluster only if over 500K ops/sec\n\nPriority: P1 - blocks horizontal scaling beyond ~10 instances\n","notes":"RESOLVED: This discussion is addressed by the consolidated Redis resilience solution.\n\n**Implemented in:**\n- **twitch-tow-h6x** (CONSOLIDATED: Redis Resilience - Circuit Breaker + Config Cache + Sentinel + Replicas)\n  - Phase 1: Circuit breaker (week 1)\n  - Phase 2: Config cache (week 2)\n  - Phase 3: Redis Sentinel for HA (week 3-4) - **directly addresses this SPOF concern**\n  - Phase 4: Read replicas for scale (week 5-6)\n\n**How the solution addresses this:**\n- Eliminates SPOF via Sentinel with 3-node quorum + auto-failover\n- Provides HA with 1 master + 2 replicas\n- Auto-failover in 30-60 seconds\n- Read replicas scale to 500K ops/sec (5x improvement)\n- Replication lag 50-200ms (acceptable for sentiment)\n\n**Also covered by epics:**\n- twitch-tow-6hl (Circuit Breaker implementation details)\n- twitch-tow-4c4 (Config Cache to reduce Redis load by 99%)\n\nThe hybrid Sentinel + read replicas approach recommended in this discussion is exactly what was adopted in the consolidated solution.","status":"closed","priority":1,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:03.742711+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:35:45.00322+01:00","closed_at":"2026-02-12T17:35:45.003223+01:00"}
{"id":"twitch-tow-vgp","title":"Positive: SQL injection protection via sqlc","description":"The codebase correctly uses sqlc for SQL query generation, providing strong SQL injection protection:\n\n**Security strengths:**\n\n1. **Parameterized queries everywhere**: All SQL queries in sqlc/queries/ use $1, $2 placeholders\n2. **No string concatenation**: Zero instances of fmt.Sprintf for SQL building\n3. **Type-safe bindings**: sqlc generates Go code with proper pgx parameter binding\n4. **No raw SQL in application code**: All queries centralized in .sql files\n\n**Example (users.sql):**\n\n\n**Additional protections:**\n- Redis keys use uuid.String() (no user input injection)\n- UUID parsing validates format before use\n- HTTP parameter validation (validateConfig checks length, range)\n\n**Potential gaps (low risk):**\n1. Redis SCAN pattern 'session:*' is hardcoded (good, no injection)\n2. Lua scripts use tonumber() (safe, but no explicit validation)\n3. Template rendering uses html/template (auto-escapes XSS)\n\n**Verdict**: Strong SQL injection protection. sqlc is the right choice for this codebase.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:56.162556+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:02.967962+01:00","closed_at":"2026-02-12T17:57:02.967962+01:00","close_reason":"Positive architectural notes - no action required"}
{"id":"twitch-tow-vwq","title":"Discussion: Main.go manual dependency wiring vs DI framework","description":"The main.go uses manual dependency injection (200+ lines of setup code):\n\n**Current approach - Manual wiring:**\n\n\n**Characteristics:**\n- Explicit: Clear construction order and dependencies\n- 200 lines in main.go (manageable for current size)\n- No reflection/magic\n- Constructor functions take dependencies explicitly\n- Testing uses manual mocks (no DI container needed)\n\n**Trade-offs:**\n\n**Pros (current approach):**\n- Zero runtime overhead\n- IDE navigation works perfectly\n- Compile-time safety on all deps\n- Easy to understand for new developers\n- No framework lock-in\n- Tests construct minimal dependency graphs\n\n**Cons (current approach):**\n- Boilerplate grows linearly with components\n- Easy to wire incorrectly (wrong dependency order)\n- No lifecycle management hooks\n- Changing constructor signatures requires updating main.go\n\n**Alternative - DI framework (wire/dig/fx):**\n- Pros: Auto-wiring, lifecycle hooks, less boilerplate\n- Cons: Magic, reflection overhead, harder debugging, framework lock-in\n\n**Data point**: At ~40 types total (11 packages), manual DI is still manageable. DI frameworks shine at 100+ types.\n\n**Recommendation**: Keep manual DI for now. Document construction order explicitly. Revisit if package count exceeds 15.","notes":"RESOLVED: Already addressed by Epic 5 (twitch-tow-dmu) Task 2 - ADR-012: Manual dependency injection. Documents trade-offs, rationale for keeping manual DI (explicit, zero overhead, IDE-friendly). Current approach appropriate for 40 types.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:43.812169+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:38:14.738856+01:00","closed_at":"2026-02-12T17:38:14.738859+01:00"}
{"id":"twitch-tow-wdy","title":"Discussion: Data consistency across Redis and PostgreSQL","description":"## Issue\nSession state lives in Redis, authoritative data in PostgreSQL. No consistency guarantees across the two datastores. Config updates could leave Redis stale.\n\n## Current State\n- ActivateSession writes to Redis, Subscribe writes to PostgreSQL (separate operations)\n- SaveConfig writes to PostgreSQL, then best-effort updates Redis (app/service.go:154-172)\n- Subscribe failure triggers DeleteSession rollback (eventsub.go:111-114) - good pattern\n- UpdateConfig failure is logged but ignored (app/service.go:167-169) - stale Redis data\n- No distributed transaction or two-phase commit between Redis and PostgreSQL\n\n## Consistency Risks\n1. **Config drift**: SaveConfig succeeds in DB but fails in Redis → users see stale config\n2. **Session orphans**: ActivateSession succeeds, Subscribe fails → rollback deletes Redis but DB updated\n3. **Partial failures**: Network partition during SaveConfig leaves divergent state\n4. **No reconciliation**: No background job to sync Redis from PostgreSQL\n5. **Cache invalidation**: Manual config changes (direct DB update) don't invalidate Redis\n\n## Failure Scenarios\n- **SaveConfig + UpdateConfig failure**: New config in DB, old config in Redis → wrong triggers\n- **Subscribe rollback race**: Concurrent EnsureSessionActive sees deleted session, re-creates\n- **Redis flush**: Operations team flushes Redis → all sessions reset, no DB record of state\n- **PostgreSQL replica lag**: Read from replica with stale config, activate session with wrong config\n\n## Suggestions\n1. Make UpdateConfig failure return error (don't ignore) → force retry or alert\n2. Add config version field in Redis (compare with DB, detect drift)\n3. Implement eventual consistency reconciliation job (compare Redis vs. DB, log discrepancies)\n4. Add explicit cache invalidation on manual DB updates (NOTIFY/LISTEN or Redis pub/sub)\n5. Consider making PostgreSQL the source of truth for live config (read on each tick)\n6. Add circuit breaker for UpdateConfig failures (stop accepting SaveConfig if Redis unavailable)\n7. Document consistency model and failure modes in CLAUDE.md\n8. Add metrics for Redis/DB consistency checks (drift_detected_total)\n\n## Files\n- internal/app/service.go:153-172 (SaveConfig best-effort pattern)\n- internal/app/service.go:75-120 (EnsureSessionActive two-datastore write)\n- internal/twitch/eventsub.go:145-193 (Subscribe with rollback)","notes":"RESOLVED: Converted to Epic 10 (twitch-tow-3ua) - Redis/PostgreSQL Consistency Monitoring. Implements: config version tracking, reconciliation job, drift detection metrics, auto-fix for stale Redis config. 4 hours effort.","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:06:38.901666+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:46:29.566995+01:00","closed_at":"2026-02-12T17:46:29.566998+01:00"}
{"id":"twitch-tow-wfl","title":"Discussion: No instance coordination beyond shared Redis state","description":"Instances have no coordination mechanism beyond Redis. This limits some operational capabilities.\n\nCurrent coordination:\n✓ Session state: Redis (shared)\n✓ Ref counting: Redis (atomic INCR/DECR)\n✗ Instance discovery: None\n✗ Leader election: None\n✗ Config broadcast: None\n✗ Graceful shutdown coordination: None\n\nScenarios requiring coordination:\n\n1. Leader election for singleton tasks\n- Orphan cleanup (currently duplicated)\n- Metrics aggregation\n- Periodic maintenance tasks\n\n2. Instance discovery\n- Know how many instances running\n- Distribute work by instance count\n- Health monitoring across fleet\n\n3. Broadcast config changes\n- User saves config in dashboard\n- Must update live sessions across all instances\n- Currently: each instance independently polls Redis\n\n4. Graceful shutdown\n- Instance wants to shut down\n- Transfer WebSocket clients to other instances\n- Or notify clients to reconnect\n- Currently: abrupt disconnect\n\n5. Distributed debugging\n- Trace request across instances\n- Correlate logs by request ID\n- Know which instance served which session\n\nMissing capabilities:\n\nA. Instance registry\n- No list of active instances\n- Can't enumerate fleet\n- Can't detect instance failures\n\nB. Event bus\n- No way to send messages between instances\n- Config updates require polling\n- No real-time coordination\n\nC. Distributed locks\n- Orphan cleanup races (covered in other bead)\n- Lua function deployment races\n- Session activation races\n\nD. Health aggregation\n- No fleet-wide health dashboard\n- Can't see total connections across instances\n- Can't aggregate metrics\n\nPotential solutions:\n\n1. Redis pub/sub for events\n```go\n// Publish config update\nrdb.Publish(ctx, \"config:update\", userID)\n\n// Subscribe in each instance\npubsub := rdb.Subscribe(ctx, \"config:update\")\nfor msg := range pubsub.Channel() {\n    invalidateConfigCache(msg.Payload)\n}\n```\n\n2. Redis hash for instance registry\n```go\n// On startup\nrdb.HSet(ctx, \"instances\", instanceID, timestamp)\n\n// Heartbeat every 10s\nrdb.HSet(ctx, \"instances\", instanceID, now)\n\n// Enumerate instances\ninstances := rdb.HGetAll(ctx, \"instances\")\n```\n\n3. Redis SETNX for leader election\n```go\n// Try to become leader (TTL 30s)\nisLeader := rdb.SetNX(ctx, \"leader\", instanceID, 30*time.Second)\nif isLeader {\n    runLeaderTasks()\n}\n```\n\n4. Distributed tracing with correlation IDs\n```go\n// Generate request ID on entry\nrequestID := uuid.New()\nctx = context.WithValue(ctx, \"request_id\", requestID)\n\n// Log with request ID\nslog.Info(\"processing\", \"request_id\", requestID)\n```\n\n5. Service mesh (Istio, Linkerd)\n- Handles instance discovery\n- Provides distributed tracing\n- Health checks and traffic management\n- Heavy infrastructure dependency\n\nRecommendation:\n\nPhase 1: Add instance registry (option 2)\n- Simple Redis hash with heartbeat\n- Enables fleet visibility\n- Foundation for leader election\n\nPhase 2: Add pub/sub for config updates (option 1)\n- Invalidate config cache on update\n- Reduces 10s staleness to real-time\n\nPhase 3: Leader election for cleanup (option 3)\n- Single instance runs orphan cleanup\n- Eliminates duplicate Twitch API calls\n\nDefer: Service mesh (option 5)\n- Overkill for current scale\n- Adds operational complexity\n\nPriority: P3 - nice-to-have, current lack of coordination is workable\n","notes":"CONVERTED TO EPIC: twitch-tow-rn7 (Epic: Instance Coordination - Registry, Pub/Sub, and Leader Election). This epic implements 3-phase coordination: (1) Instance registry with heartbeat for fleet visibility, (2) Pub/sub for real-time config invalidation (\u003c1s vs 10s), (3) Leader election for singleton orphan cleanup (eliminates duplicate Twitch API calls). Lightweight Redis-based solution without service mesh complexity.","status":"closed","priority":3,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T17:09:31.55056+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:43:52.887313+01:00","closed_at":"2026-02-12T17:43:52.887316+01:00"}
{"id":"twitch-tow-wsw","title":"Fix missing ParseFloat error handling in SentimentStore","description":"**Critical Priority**\n\nLocation: internal/redis/sentiment_store.go lines 39, 54\n\nIssue: ParseFloat can return parse errors if Redis returns malformed data or the Lua function returns unexpected format. The error is silently lost.\n\nImpact: Callers get a zero value with no indication of failure, leading to incorrect sentiment calculations in overlays.\n\nFix:\n- Check ParseFloat errors explicitly\n- Return wrapped error with context\n- Add logging for debugging\n- Consider validating Lua function output format","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:25:40.131052+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:35:33.296835+01:00","closed_at":"2026-02-12T16:35:33.296835+01:00","close_reason":"Closed"}
{"id":"twitch-tow-x1r","title":"Implement CSRF protection on dashboard endpoints","description":"**Medium Priority (Security)**\n\nLocation: internal/server/handlers_dashboard.go lines 84-117\n\nIssue: The config save endpoint has no CSRF protection. A malicious site can POST to /dashboard/config on behalf of an authenticated user.\n\nImpact: Users can have their overlay config changed without consent.\n\nFix:\n- Add Echo CSRF middleware\n- Generate and include CSRF tokens in dashboard.html form\n- Verify tokens on all POST endpoints\n- Also applies to /api/reset/:uuid and /api/rotate-overlay-uuid","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:25:59.902056+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:37:44.453369+01:00","closed_at":"2026-02-12T16:37:44.453369+01:00","close_reason":"Closed"}
{"id":"twitch-tow-y9l","title":"EPIC: Cleanup Thundering Herd Prevention with Leader Election","description":"Prevent multiple instances from running orphan cleanup simultaneously using Redis-based leader election with jitter.\n\n## User Story\nAs an operator running multiple ChatPulse instances, I want only one instance to perform orphan cleanup at a time, so we don't waste Twitch API quota with duplicate unsubscribe calls.\n\n## Value Proposition\n- Eliminates duplicate Twitch API calls (saves rate limit quota)\n- Reduces Redis SCAN contention (faster cleanup)\n- Prevents goroutine accumulation from duplicate background tasks\n- Graceful leader failover (30s lease renewal)\n\n## Background\n\n**Current behavior (app/service.go):**\nEvery instance runs cleanup timer independently:\n- 30s ticker per instance\n- All instances start at boot (roughly synchronized)\n- All run `ListOrphans` SCAN at same time\n- All call Twitch unsubscribe API for same orphans\n\n**Problems:**\n1. **Duplicate Twitch calls:** 10 instances × 100 orphans = 1000 API calls (should be 100)\n2. **Rate limit waste:** Twitch rate limits apply, duplicates consume quota\n3. **Redis SCAN contention:** All instances scan simultaneously\n4. **Unbounded goroutines:** Each instance spawns background goroutines for Twitch calls\n\n**Scale impact:**\n- At 10 instances: 10× API call waste\n- At 100 instances: 100× API call waste\n- Twitch API rate limits unclear but likely strict\n\n## Tasks\n\n### 1. Add jitter to cleanup interval (Phase 1 - Quick Win)\n\n**Implementation:**\n\n```go\n// internal/app/service.go\n\nfunc NewService(cfg *config.Config, ...) *Service {\n    // ... existing setup\n    \n    // Add random jitter (0-10s) to cleanup interval\n    jitter := time.Duration(rand.Intn(10)) * time.Second\n    cleanupInterval := cfg.CleanupInterval() + jitter\n    \n    s.cleanupTicker = time.NewTicker(cleanupInterval)\n    \n    // ... rest unchanged\n}\n```\n\n**Benefits:**\n- Spreads cleanup load over 10-second window\n- Reduces simultaneous SCAN (not eliminated)\n- Simple change (5 lines)\n- No coordination required\n\n**Limitations:**\n- Still allows duplicates (just less likely to collide)\n- Doesn't eliminate Twitch API waste\n\n**Files to modify:**\n- `internal/app/service.go` (add jitter to ticker creation)\n\n**Time estimate:** 30 minutes\n\n---\n\n### 2. Implement Redis leader election (Phase 2 - Core Solution)\n\n**Pattern: Redis SETNX with TTL**\n\n```go\n// internal/app/leader.go (new file)\n\ntype LeaderElector struct {\n    rdb        *redis.Client\n    instanceID string\n    lockKey    string\n    lockTTL    time.Duration\n}\n\nfunc NewLeaderElector(rdb *redis.Client, instanceID string) *LeaderElector {\n    return \u0026LeaderElector{\n        rdb:        rdb,\n        instanceID: instanceID,\n        lockKey:    \"cleanup:leader\",\n        lockTTL:    30 * time.Second,\n    }\n}\n\n// TryAcquire attempts to become leader\nfunc (l *LeaderElector) TryAcquire(ctx context.Context) (bool, error) {\n    // SETNX: set only if key doesn't exist\n    ok, err := l.rdb.SetNX(ctx, l.lockKey, l.instanceID, l.lockTTL).Result()\n    if err != nil {\n        return false, fmt.Errorf(\"failed to acquire leader lock: %w\", err)\n    }\n    return ok, nil\n}\n\n// Renew extends leader lease (call every 15s)\nfunc (l *LeaderElector) Renew(ctx context.Context) error {\n    // Check current value is our instance ID (don't steal lock)\n    currentLeader, err := l.rdb.Get(ctx, l.lockKey).Result()\n    if err == redis.Nil {\n        return fmt.Errorf(\"leader lock lost\")\n    }\n    if err != nil {\n        return fmt.Errorf(\"failed to check leader: %w\", err)\n    }\n    \n    if currentLeader != l.instanceID {\n        return fmt.Errorf(\"leader lock stolen by %s\", currentLeader)\n    }\n    \n    // Renew TTL\n    ok, err := l.rdb.Expire(ctx, l.lockKey, l.lockTTL).Result()\n    if err != nil {\n        return fmt.Errorf(\"failed to renew leader lock: %w\", err)\n    }\n    if \\!ok {\n        return fmt.Errorf(\"leader lock lost during renewal\")\n    }\n    \n    return nil\n}\n\n// Release voluntarily releases leadership\nfunc (l *LeaderElector) Release(ctx context.Context) error {\n    // Delete only if we're still the leader\n    script := `\n        if redis.call(\"GET\", KEYS[1]) == ARGV[1] then\n            return redis.call(\"DEL\", KEYS[1])\n        else\n            return 0\n        end\n    `\n    \n    _, err := l.rdb.Eval(ctx, script, []string{l.lockKey}, l.instanceID).Result()\n    return err\n}\n```\n\n**Instance ID generation:**\n```go\n// cmd/server/main.go\n\ninstanceID := generateInstanceID()\n\nfunc generateInstanceID() string {\n    hostname, _ := os.Hostname()\n    return fmt.Sprintf(\"%s-%d\", hostname, os.Getpid())\n}\n```\n\n**Files to create:**\n- `internal/app/leader.go` (new file for leader election logic)\n\n**Time estimate:** 2 hours\n\n---\n\n### 3. Integrate leader election into cleanup timer\n\n**Update Service to use leader election:**\n\n```go\n// internal/app/service.go\n\ntype Service struct {\n    // ... existing fields\n    leaderElector *LeaderElector\n    isLeader      bool\n}\n\nfunc NewService(cfg *config.Config, ..., rdb *redis.Client, ...) *Service {\n    instanceID := generateInstanceID()\n    \n    s := \u0026Service{\n        // ... existing initialization\n        leaderElector: NewLeaderElector(rdb, instanceID),\n        isLeader:      false,\n    }\n    \n    // ... cleanup ticker setup (with jitter from Phase 1)\n    \n    return s\n}\n\nfunc (s *Service) runCleanupLoop(ctx context.Context) {\n    renewTicker := time.NewTicker(15 * time.Second) // Renew lease every 15s\n    defer renewTicker.Stop()\n    \n    for {\n        select {\n        case \u003c-s.cleanupTicker.C:\n            // Try to become leader (if not already)\n            if \\!s.isLeader {\n                acquired, err := s.leaderElector.TryAcquire(ctx)\n                if err != nil {\n                    s.logger.Error(\"failed to acquire leader lock\", \"error\", err)\n                    continue\n                }\n                if \\!acquired {\n                    s.logger.Debug(\"skipping cleanup (not leader)\")\n                    continue\n                }\n                s.isLeader = true\n                s.logger.Info(\"became cleanup leader\")\n            }\n            \n            // Run cleanup as leader\n            if err := s.CleanupOrphans(ctx); err != nil {\n                s.logger.Error(\"cleanup failed\", \"error\", err)\n            }\n            \n        case \u003c-renewTicker.C:\n            // Renew leadership lease\n            if s.isLeader {\n                if err := s.leaderElector.Renew(ctx); err != nil {\n                    s.logger.Warn(\"lost leader lock\", \"error\", err)\n                    s.isLeader = false\n                }\n            }\n            \n        case \u003c-ctx.Done():\n            // Graceful shutdown: release leadership\n            if s.isLeader {\n                releaseCtx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n                defer cancel()\n                if err := s.leaderElector.Release(releaseCtx); err != nil {\n                    s.logger.Error(\"failed to release leader lock\", \"error\", err)\n                }\n            }\n            return\n        }\n    }\n}\n```\n\n**Graceful shutdown:**\n```go\nfunc (s *Service) Stop() {\n    // ... existing cleanup ticker stop\n    \n    // Release leadership before exiting\n    if s.isLeader {\n        ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n        defer cancel()\n        s.leaderElector.Release(ctx)\n    }\n}\n```\n\n**Files to modify:**\n- `internal/app/service.go` (integrate leader election into cleanup loop)\n- `cmd/server/main.go` (pass redis client to NewService)\n\n**Time estimate:** 2 hours\n\n---\n\n### 4. Add leader election metrics\n\n**Track leadership events:**\n\n```go\n// internal/app/metrics.go\n\nvar (\n    cleanupLeaderGauge = prometheus.NewGaugeVec(\n        prometheus.GaugeOpts{\n            Name: \"chatpulse_cleanup_leader\",\n            Help: \"1 if this instance is cleanup leader, 0 otherwise\",\n        },\n        []string{\"instance_id\"},\n    )\n    \n    cleanupSkipped = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"chatpulse_cleanup_skipped_total\",\n            Help: \"Total cleanup cycles skipped (not leader)\",\n        },\n        []string{\"reason\"}, // \"not_leader\" vs \"leader_lock_failed\"\n    )\n    \n    leaderElectionFailures = prometheus.NewCounterVec(\n        prometheus.CounterOpts{\n            Name: \"chatpulse_leader_election_failures_total\",\n            Help: \"Total leader election failures\",\n        },\n        []string{\"reason\"}, // \"acquire_failed\", \"renew_failed\", \"lock_stolen\"\n    )\n)\n\n// In runCleanupLoop:\nif \\!acquired {\n    cleanupSkipped.WithLabelValues(\"not_leader\").Inc()\n    continue\n}\n\n// When becoming leader:\ncleanupLeaderGauge.WithLabelValues(instanceID).Set(1)\n\n// When losing leadership:\ncleanupLeaderGauge.WithLabelValues(instanceID).Set(0)\nleaderElectionFailures.WithLabelValues(\"lock_lost\").Inc()\n```\n\n**Benefits:**\n- Track which instance is leader (useful for debugging)\n- Alert if leadership flaps frequently (\u003e10/min = instability)\n- Monitor cleanup skipped events (should be N-1 instances per cycle)\n\n**Files to modify:**\n- `internal/app/metrics.go` (add leader election metrics)\n- `internal/app/service.go` (increment metrics at appropriate points)\n\n**Dependencies:**\n- Requires Observability epic (Prometheus metrics)\n\n**Time estimate:** 1 hour\n\n---\n\n### 5. Test leader election behavior\n\n**Test scenarios:**\n\n**Test 1: Only leader runs cleanup**\n```go\nfunc TestCleanupOnlyRunsByLeader(t *testing.T) {\n    // Setup 3 Service instances with shared Redis\n    rdb := setupTestRedis(t)\n    \n    svc1 := NewService(cfg, ..., rdb, ...)\n    svc2 := NewService(cfg, ..., rdb, ...)\n    svc3 := NewService(cfg, ..., rdb, ...)\n    \n    // All try to become leader\n    acquired1, _ := svc1.leaderElector.TryAcquire(ctx)\n    acquired2, _ := svc2.leaderElector.TryAcquire(ctx)\n    acquired3, _ := svc3.leaderElector.TryAcquire(ctx)\n    \n    // Only one should succeed\n    acquiredCount := 0\n    if acquired1 { acquiredCount++ }\n    if acquired2 { acquiredCount++ }\n    if acquired3 { acquiredCount++ }\n    \n    assert.Equal(t, 1, acquiredCount, \"only one instance should be leader\")\n}\n```\n\n**Test 2: Leader failover after TTL expiry**\n```go\nfunc TestLeaderFailover(t *testing.T) {\n    rdb := setupTestRedis(t)\n    \n    // Instance 1 becomes leader\n    svc1 := NewService(cfg, ..., rdb, ...)\n    acquired, _ := svc1.leaderElector.TryAcquire(ctx)\n    require.True(t, acquired)\n    \n    // Wait for TTL to expire (30s + buffer)\n    time.Sleep(35 * time.Second)\n    \n    // Instance 2 can now become leader\n    svc2 := NewService(cfg, ..., rdb, ...)\n    acquired, _ = svc2.leaderElector.TryAcquire(ctx)\n    assert.True(t, acquired, \"instance 2 should become leader after TTL expiry\")\n}\n```\n\n**Test 3: Leader renewal prevents takeover**\n```go\nfunc TestLeaderRenewalPrevents Takeover(t *testing.T) {\n    rdb := setupTestRedis(t)\n    \n    // Instance 1 becomes leader\n    svc1 := NewService(cfg, ..., rdb, ...)\n    svc1.leaderElector.TryAcquire(ctx)\n    \n    // Renew lease every 15s\n    go func() {\n        ticker := time.NewTicker(15 * time.Second)\n        defer ticker.Stop()\n        for range ticker.C {\n            svc1.leaderElector.Renew(ctx)\n        }\n    }()\n    \n    // Instance 2 tries to become leader after 20s\n    time.Sleep(20 * time.Second)\n    svc2 := NewService(cfg, ..., rdb, ...)\n    acquired, _ := svc2.leaderElector.TryAcquire(ctx)\n    \n    // Should fail (instance 1 renewed lease)\n    assert.False(t, acquired, \"instance 2 should NOT become leader (lease renewed)\")\n}\n```\n\n**Test 4: Graceful leadership release**\n```go\nfunc TestGracefulLeadershipRelease(t *testing.T) {\n    rdb := setupTestRedis(t)\n    \n    // Instance 1 becomes leader\n    svc1 := NewService(cfg, ..., rdb, ...)\n    svc1.leaderElector.TryAcquire(ctx)\n    \n    // Release leadership\n    err := svc1.leaderElector.Release(ctx)\n    require.NoError(t, err)\n    \n    // Instance 2 can immediately become leader (no waiting for TTL)\n    svc2 := NewService(cfg, ..., rdb, ...)\n    acquired, _ := svc2.leaderElector.TryAcquire(ctx)\n    assert.True(t, acquired, \"instance 2 should become leader immediately after release\")\n}\n```\n\n**Files to modify:**\n- `internal/app/leader_test.go` (new file for leader election tests)\n- `internal/app/service_test.go` (add cleanup leader integration tests)\n\n**Time estimate:** 2 hours\n\n---\n\n### 6. Document leader election pattern\n\n**Add to CLAUDE.md:**\n\n```markdown\n## Cleanup Leader Election\n\nOrphan cleanup runs on only one instance at a time using Redis-based leader election.\n\n### Pattern: Redis SETNX with TTL\n\n```go\n// Try to become leader\nok, _ := rdb.SetNX(ctx, \"cleanup:leader\", instanceID, 30*time.Second).Result()\nif \\!ok {\n    // Another instance is leader, skip cleanup\n    return\n}\n\n// We're the leader, run cleanup\nrunCleanup()\n\n// Renew lease every 15s\nrdb.Expire(ctx, \"cleanup:leader\", 30*time.Second)\n```\n\n### Benefits\n- ✅ Eliminates duplicate Twitch API calls (10× instances → 1× API calls)\n- ✅ Reduces Redis SCAN contention\n- ✅ Prevents goroutine accumulation from duplicates\n\n### Failure Modes\n\n**Leader crashes:**\n- Lock TTL expires after 30s\n- Another instance becomes leader on next cycle (≤30s recovery)\n\n**Leader slow to renew:**\n- Renewal happens every 15s\n- If renewal fails, lock expires at 30s\n- New leader elected (graceful failover)\n\n**Network partition:**\n- Leader can't reach Redis → renewal fails → loses leadership\n- Other instances elect new leader\n- No split-brain (Redis is single source of truth)\n\n### Metrics\n\n- `chatpulse_cleanup_leader{instance_id}` - 1 if leader, 0 otherwise\n- `chatpulse_cleanup_skipped_total{reason}` - Cleanup cycles skipped\n- `chatpulse_leader_election_failures_total{reason}` - Election failures\n\n### Operations\n\n**Check current leader:**\n```bash\nredis-cli GET cleanup:leader\n# Returns instance ID of current leader\n```\n\n**Force leader change** (emergency):\n```bash\nredis-cli DEL cleanup:leader\n# Next instance to run cleanup becomes leader\n```\n\n**Lock stolen alert:**\nIf `chatpulse_leader_election_failures_total{reason=\"lock_stolen\"}` fires:\n- Indicates clock skew or multiple instances with same instance ID\n- Check instance ID generation (hostname + PID should be unique)\n```\n\n**Files to modify:**\n- `CLAUDE.md` (add Cleanup Leader Election section)\n\n**Time estimate:** 1 hour\n\n---\n\n## Acceptance Criteria\n\n- ✅ Cleanup timer includes random jitter (0-10s)\n- ✅ Only one instance runs cleanup at a time\n- ✅ Leader election uses Redis SETNX with 30s TTL\n- ✅ Leader lease renewed every 15s\n- ✅ Graceful leadership release on shutdown\n- ✅ Failover within 30s if leader crashes\n- ✅ Tests cover leader election, failover, renewal\n- ✅ Metrics track leader status and skipped cleanups\n- ✅ Documentation explains failure modes + operations\n\n## Files Changed\n\n**Created:**\n- `internal/app/leader.go` (LeaderElector implementation)\n- `internal/app/leader_test.go` (leader election tests)\n\n**Modified:**\n- `internal/app/service.go` (add jitter, integrate leader election, graceful release)\n- `internal/app/service_test.go` (add integration tests)\n- `internal/app/metrics.go` (add leader election metrics)\n- `cmd/server/main.go` (pass redis client to Service, generate instance ID)\n- `CLAUDE.md` (add Cleanup Leader Election section)\n\n## Dependencies\n- Optional: Observability epic for metrics (can implement metrics later)\n\n## Effort Estimate\n**Total: 5 hours** (300 minutes)\n- Jitter (30min): Add random jitter to cleanup interval\n- Leader election (2h): Implement LeaderElector (acquire, renew, release)\n- Integration (2h): Wire leader election into Service cleanup loop\n- Metrics (1h): Add leader election metrics (optional)\n- Testing (2h): 4 test scenarios (leader-only, failover, renewal, release)\n- Documentation (1h): CLAUDE.md section + operations guide\n\n## Success Metrics\n- Twitch API calls reduced to 1× per cleanup cycle (down from N×)\n- Only 1 instance shows `cleanup_leader=1` at any time\n- Failover occurs within 30s of leader crash\n- No `lock_stolen` alerts (clean election)","status":"open","priority":2,"issue_type":"epic","assignee":"Patrick Scheid","owner":"patrick.scheid@deepl.com","estimated_minutes":300,"created_at":"2026-02-12T17:43:00.1678+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T17:57:44.50918+01:00"}
{"id":"twitch-tow-z64","title":"Replace log.Println with structured logging","description":"**Low Priority (Code Quality)**\n\nLocation: Multiple files throughout codebase\n\nIssue: Mix of Println and Printf, no log levels, no structured fields.\n\nImpact: Difficult debugging in production, can't filter by severity.\n\nFix:\n- Choose structured logging library (zerolog, zap, slog)\n- Add log levels (debug, info, warn, error)\n- Add contextual fields (user_id, session_uuid)\n- Make logger configurable via env","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:28.945436+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:48:59.703311+01:00","closed_at":"2026-02-12T16:48:59.703311+01:00","close_reason":"Completed structured logging migration to slog\n\n## Implementation Complete\n\nSuccessfully migrated all 65 log statements from log.Println/Printf to structured logging using Go's stdlib slog package.\n\n### Summary\n- **Library**: log/slog (Go 1.21+ stdlib, zero external dependencies)\n- **Files modified**: 17 files across internal/\n- **Log statements migrated**: 65 total\n- **Build status**: ✅ Passing\n- **Approach**: Incremental migration, all at once\n\n### Key Changes\n\n**1. New Logging Package** (internal/logging/logger.go)\n- Centralized logger initialization\n- Configurable via ENV: LOG_LEVEL (debug/info/warn/error), LOG_FORMAT (text/json)\n- Helper functions for contextual logging: WithSession(), WithUser(), WithBroadcaster(), WithError()\n\n**2. Configuration** (internal/config/config.go)\n- Added LOG_LEVEL (default: info)\n- Added LOG_FORMAT (default: text)\n- Updated .env.example with documentation\n\n**3. Main.go Initialization** (cmd/server/main.go)\n- Logger initialized before any other components\n- First log: application starting with env + port\n\n**4. Migration Pattern**\nBefore: `log.Printf(\"Failed to connect: %v\", err)`\nAfter: `slog.Error(\"Failed to connect\", \"error\", err)`\n\n### Log Level Usage\n- **Error**: Operation failures, database errors, API errors\n- **Warn**: Non-fatal issues, timeouts, slow clients\n- **Info**: Application lifecycle, connections, subscriptions\n- **Debug**: Detailed tracing, client registration (reduced noise)\n\n### Files Migrated\n✓ cmd/server/main.go (13)\n✓ internal/broadcast/broadcaster.go (11)\n✓ internal/app/service.go (11)\n✓ internal/twitch/eventsub.go (11)\n✓ internal/server/handlers_auth.go (9)\n✓ internal/server/handlers_overlay.go (5)\n✓ internal/server/handlers_dashboard.go (4)\n✓ internal/twitch/webhook.go (4)\n✓ internal/server/handlers_api.go (3)\n✓ internal/redis/session_repository.go (2)\n✓ internal/database/postgres.go (2)\n✓ internal/server/server.go (1)\n✓ internal/server/handlers.go (1)\n✓ internal/sentiment/engine.go (1)\n✓ internal/config/config.go (1)\n\n### Benefits\n- **Production debugging**: Structured fields enable log aggregation/filtering\n- **Performance**: slog is faster than fmt-based logging\n- **No dependencies**: Pure stdlib solution\n- **Log levels**: Can adjust verbosity without code changes\n- **JSON output**: Production-ready format for log aggregators\n\n### Example Output\nText format (development):\n```\ntime=2026-02-12T16:45:00.000Z level=INFO msg=\"Application starting\" env=development port=8080\ntime=2026-02-12T16:45:01.123Z level=DEBUG msg=\"Client registered\" session_uuid=abc-123 total_clients=1\ntime=2026-02-12T16:45:05.456Z level=ERROR msg=\"Failed to connect to database\" error=\"connection refused\"\n```\n\nJSON format (production):\n```json\n{\"time\":\"2026-02-12T16:45:00Z\",\"level\":\"INFO\",\"msg\":\"Application starting\",\"env\":\"development\",\"port\":\"8080\"}\n```\n\nAll log statements now use structured logging with proper context!"}
{"id":"twitch-tow-zlm","title":"Address TODO comments in domain models","description":"**Low Priority (Technical Debt)**\n\nLocation: internal/domain/user.go:15, internal/domain/config.go:14\n\nTODOs about token separation and label naming are unresolved design decisions.\n\nImpact: Technical debt, unclear API design.\n\nFix:\n- Review and decide on token separation\n- Review and decide on label naming\n- Either implement changes or remove TODOs with rationale","status":"closed","priority":2,"issue_type":"task","owner":"patrick.scheid@deepl.com","created_at":"2026-02-12T16:26:23.186764+01:00","created_by":"Patrick Scheid","updated_at":"2026-02-12T16:41:12.60766+01:00","closed_at":"2026-02-12T16:41:12.60766+01:00","close_reason":"TODO comments resolved with clear rationale documentation\n\nBoth TODOs have been addressed with comprehensive rationale comments:\n\n**1. User.go (line 15) - Token separation**\nRESOLVED: Tokens kept in User struct with clear rationale:\n- User and tokens have identical lifecycle (created/updated together)\n- No use case for querying users without tokens or vice versa\n- Separation would add complexity (JOIN queries, dual updates) without clear benefit\n- Token encryption handled at repository layer, not domain layer\n\n**2. Config.go (line 14) - Label naming**\nRESOLVED: LeftLabel/RightLabel retained with spatial naming rationale:\n- Overlay UI displays labels on left/right sides of sentiment bar\n- Dashboard UI clarifies semantic meaning: 'Left Label (Against)' / 'Right Label (For)'\n- Spatial naming matches user mental model for overlay positioning\n- ForLabel/AgainstLabel would be redundant with ForTrigger/AgainstTrigger\n- CSS classes use .label-left/.label-right for consistency\n\nBoth decisions are well-documented and follow clean code principles:\n- Simple over complex (YAGNI for token separation)\n- Clear naming that matches domain (spatial for UI elements)\n- Rationale preserved for future maintainers\n\nNo code changes needed - documentation complete.\nFiles verified: internal/domain/user.go, internal/domain/config.go"}
